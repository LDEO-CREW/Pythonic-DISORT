{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c6244a4",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import PyDISORT\n",
    "\n",
    "# Required by PyDISORT\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from inspect import signature\n",
    "from math import pi\n",
    "from scipy import integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d438e85f",
   "metadata": {
    "code_folding": [],
    "format": "row",
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# Only required in this notebook for tests and exposition\n",
    "import disort # TO REMOVE\n",
    "import autograd as ag\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import constants\n",
    "#from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cd6607",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [1. USER INPUT REQUIRED: Choose parameters](#1.-USER-INPUT-REQUIRED:-Choose-parameters)\n",
    "\t* [1.1 Choose optical properties](#1.1-Choose-optical-properties)\n",
    "\t* [1.2 Choose computational parameters](#1.2-Choose-computational-parameters)\n",
    "\t* [1.3 Choose phase function](#1.3-Choose-phase-function)\n",
    "\t\t* [1.3.1 OPTIONAL: Choose delta-M scaling](#1.3.1-OPTIONAL:-Choose-delta-M-scaling)\n",
    "\t* [1.4 Choose incident beam](#1.4-Choose-incident-beam)\n",
    "\t* [1.5 OPTIONAL: Choose Dirichlet BCs](#1.5-OPTIONAL:-Choose-Dirichlet-BCs)\n",
    "\t* [1.6 OPTIONAL: Choose BDRF](#1.6-OPTIONAL:-Choose-BDRF)\n",
    "\t* [1.7 OPTIONAL: Choose other internal sources](#1.7-OPTIONAL:-Choose-other-internal-sources)\n",
    "* [2. PyDISORT modules and outputs](#2.-PyDISORT-modules-and-outputs)\n",
    "* [3. Breakdown of single layer solver](#3.-Breakdown-of-single-layer-solver)\n",
    "\t* [3.1 Quadrature](#3.1-Quadrature)\n",
    "\t\t* [3.1.1 Verification of quadrature](#3.1.1-Verification-of-quadrature)\n",
    "\t\t* [3.1.2 Normalization verification of phase function](#3.1.2-Normalization-verification-of-phase-function)\n",
    "\t* [3.2 Fourier expansion of solution](#3.2-Fourier-expansion-of-solution)\n",
    "\t\t* [3.2.1 Surface reflection](#3.2.1-Surface-reflection)\n",
    "\t\t* [3.2.2 Important terms](#3.2.2-Important-terms)\n",
    "\t* [3.3 Delta-M scaling](#3.3-Delta-M-scaling)\n",
    "\t* [3.4 System of ODEs](#3.4-System-of-ODEs)\n",
    "\t\t* [3.4.1 How to choose computational parameters](#3.4.1-How-to-choose-computational-parameters)\n",
    "\t\t* [3.4.2 Assembly of system](#3.4.2-Assembly-of-system)\n",
    "\t* [3.5 Diagonalization](#3.5-Diagonalization)\n",
    "\t* [3.6 General solution for each Fourier mode](#3.6-General-solution-for-each-Fourier-mode)\n",
    "\t\t* [3.6.1 The particular solutions](#3.6.1-The-particular-solutions)\n",
    "\t\t* [3.6.2 The homogeneous solution for one layer](#3.6.2-The-homogeneous-solution-for-one-layer)\n",
    "\t\t* [3.6.3 Verification of the general solution](#3.6.3-Verification-of-the-general-solution)\n",
    "\t* [3.7 The full solution](#3.7-The-full-solution)\n",
    "\t\t* [3.7.1 Verification of full solution without corrections](#3.7.1-Verification-of-full-solution-without-corrections)\n",
    "\t\t* [3.7.2 Nakajima-Tanaka intensity corrections](#3.7.2-Nakajima-Tanaka-intensity-corrections)\n",
    "\t\t* [3.7.3 Verification of NT corrected full solution](#3.7.3-Verification-of-NT-corrected-full-solution)\n",
    "\t* [3.8 Computation of flux](#3.8-Computation-of-flux)\n",
    "\t\t* [3.8.1 Verification of flux](#3.8.1-Verification-of-flux)\n",
    "\t\t* [3.8.2 Reflectance and transmittance](#3.8.2-Reflectance-and-transmittance)\n",
    "* [4. Timing PyDISORT](#4.-Timing-PyDISORT)\n",
    "* [5. Solve for multiple atmospheric layers](#5.-Solve-for-multiple-atmospheric-layers)\n",
    "* [6. Comparisons with Stamnes' DISORT -- TO REPLACE WITH PYTEST](#6.-Comparisons-with-Stamnes'-DISORT----TO-REPLACE-WITH-PYTEST)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5373074c",
   "metadata": {},
   "source": [
    "Denote the optical depth as $\\tau$, the cosine of the polar angle as $\\mu$ (positive is upward), and the azimuthal angle as $\\phi$ (positive is counterclockwise when looking down on the atmospheric layer). We wish to solve the (one-dimensional) Radiative Transfer Equation\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu \\frac{\\partial u(\\tau, \\mu, \\phi)}{\\partial \\tau} = u(\\tau, \\mu, \\phi) &-\\frac{\\omega}{4 \\pi} \\int_{-1}^{1} \\int_{0}^{2 \\pi} p\\left(\\mu, \\phi ; \\mu', \\phi'\\right) u\\left(\\tau, \\mu', \\phi'\\right) \\mathrm{d} \\phi' \\mathrm{d} \\mu' \\\\\n",
    "&-\\frac{\\omega I_0}{4 \\pi} p\\left(\\mu, \\phi ;-\\mu_{0}, \\phi_{0}\\right) \\exp\\left(-\\mu_{0}^{-1} \\tau\\right) - s(\\tau, \\mu)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "for the **diffuse** (specific) intensity, $u = u_\\text{diffuse}$. This will be done numerically using the Discrete Ordinates Method. Our Discrete Ordinates Method package is called PyDISORT. The total intensity is given by\n",
    "\n",
    "$$u_\\text{total} = u_\\text{diffuse} + u_\\text{direct}$$\n",
    "\n",
    "where for $-\\mu_0 < 0$ (downwards),\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\mu\\frac{\\partial u_\\text{direct}(\\tau, \\mu, \\phi)}{\\partial\\tau} = u_\\text{direct}(\\tau, \\mu, \\phi), \\quad u_\\text{direct}(0, \\mu, \\phi) = I_0 \\delta(\\mu + \\mu_0) \\delta(\\phi - \\phi_0) \\\\\n",
    "&\\implies u_\\text{direct}(\\tau, \\mu, \\phi) = I_0 \\delta(\\mu + \\mu_0) \\delta(\\phi - \\phi_0) \\exp\\left(-\\mu_{0}^{-1} \\tau\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf1dadf",
   "metadata": {},
   "source": [
    "The fundamental idea behind the Discrete Ordinates Method, from Chandrasekhar [[1]](#cite-Cha1960), is to perform the following series expansions\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "u\\left(\\tau, \\mu, \\phi\\right) &\\approx \\sum_{n=0}^\\text{NLoops} u^n\\left(\\tau, \\mu\\right)\\cos\\left(n\\left(\\phi_0 - \\phi\\right)\\right) \\quad \\text{(Fourier cosine series expansion)}\\\\\n",
    "p\\left(\\mu, \\phi ; \\mu', \\phi'\\right) = p\\left(\\cos\\gamma\\right) &\\approx \\sum_{\\ell=0}^\\text{NLeg} (2\\ell + 1) g_\\ell P_\\ell\\left(\\cos\\gamma\\right) \\quad \\text{(Legendre series expansion)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which will deal with the $\\phi'$ integral, and approximate the $\\mu'$ integral using Gauss-Legendre quadrature with $\\text{NQuad} = 2N$ quadrature points, i.e.\n",
    "\n",
    "$$\\int_{-1}^1 [\\dots] \\ \\mathrm{d}\\mu' \\approx \\sum_{|j|=1,\\,j \\neq 0}^\\text{N} [\\dots]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e7be3c",
   "metadata": {},
   "source": [
    "**Multiple atmospheric layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc5f6c5",
   "metadata": {},
   "source": [
    "It is straightforward to generalize to multiple atmospheric layers, each demarcated by $\\tau_l$ for index $l = 0, 1, \\dots, L$, and each with its own single-scattering albedo $\\omega$ and phase function $p$. Each layer is modeled by its own radiative transfer equation, and until section [3.6](#3.6-General-solution-for-each-Fourier-mode) the method is the same for each layer. The only complication from moving to a multi-layered atmosphere is that the boundary conditions for each layer are coupled. Therefore, we would need to implement section [5](#5.-Solve-for-multiple-atmospheric-layers) instead of section [3.6](#3.6-General-solution-for-each-Fourier-mode)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff99ba1a",
   "metadata": {},
   "source": [
    "# 1. USER INPUT REQUIRED: Choose parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27764183",
   "metadata": {},
   "source": [
    "Our notations generally follow those in Stamnes et. al.'s 1988 paper [[2]](#cite-STWJ1988), **with the equivalent variable in their FORTRAN DISORT code [[3]](#cite-Sta1999) in brackets**, though Stamnes' DISORT and our PyDISORT require somewhat different inputs. The default values we provide correspond to *Test Problem 3: Henyey-Greenstein Scattering* of Stamnes' DISORT but we tweak the values of $\\mu_0$, $\\phi_0$, $\\omega$, and decrease $\\text{NQuad}$ and increase $g$ so that $\\delta-M$ scaling, explained in section [1.3.1](#1.3.1-OPTIONAL:-Choose-delta-M-scaling), will have a greater effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d52f3ac",
   "metadata": {},
   "source": [
    "## 1.1 Choose optical properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96889d85",
   "metadata": {},
   "source": [
    "The phase function will be chosen later in section [1.3](#1.3-Choose-phase-function)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f10e74f",
   "metadata": {},
   "source": [
    "* **Optical depth (DTAUC)**\n",
    "\n",
    "We take the top of the atmosphere to have $\\tau = 0$. Consequently, the first (zeroth index) atmospheric layer from the top has optical thickness $\\tau_0$ and each subsequent atmospheric layer $l$ has optical thickness $\\tau_l - \\tau_{l-1}$. The number of atmospheric layers is taken to be `len(tau_arr)`. Thanks to Stamnes-Conklin's substitutions [[5]](#cite-SC1984) we can accomodate large optical depths, though there will still be inaccuracies. We require each $\\tau_l$ to be positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f555ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While we specify two atmospheric layers for illustrative purposes, the two layers \n",
    "# have identical optical properties, and so this model is equivalent to a one layer model\n",
    "\n",
    "#################### SUPPLIED TO PYDISORT ########################\n",
    "\n",
    "tau_arr = np.array([4, 8])\n",
    "\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1b2326",
   "metadata": {},
   "source": [
    "While PyDISORT can address a multi-layer atmosphere, the verification tests in this notebook are for a single-layer (equivalent) atmosphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "539889fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 1\n",
    "\n",
    "# Used internally in PyDISORT\n",
    "taul = tau_arr[l]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d306e",
   "metadata": {},
   "source": [
    "* **Single-scattering albedo (SSALB)**\n",
    "\n",
    "As before, **the zeroth index corresponds to the atmospheric layer at the top of the atmosphere**. Within each layer we assume $\\omega$ to be independent from $\\tau$. We require each $\\omega \\in [0,1)$, and values too close to $1$ will cause instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da1154a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### SUPPLIED TO PYDISORT ########################\n",
    "\n",
    "omega_arr = np.array([0.75, 0.75])\n",
    "\n",
    "##################################################################\n",
    "\n",
    "# Used internally in PyDISORT\n",
    "omegal = omega_arr[l]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855af7bc",
   "metadata": {},
   "source": [
    "## 1.2 Choose computational parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d51344",
   "metadata": {},
   "source": [
    "* **Number of quadrature points for the** $\\mu'$ **integral (NSTR)**\n",
    "\n",
    "This parameter is also known as the number of \"streams\" in the radiation literature and in Stamnes' DISORT [[3]](#cite-Sta1999). We require $\\text{NQuad}$ to be $\\geq 2$ and even. Note that this parameter contributes the most to the computational complexity which scales like $\\mathcal{O}\\left(\\text{NQuad}^3\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70938256",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### SUPPLIED TO PYDISORT ########################\n",
    "\n",
    "NQuad = 16\n",
    "\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c51440",
   "metadata": {},
   "source": [
    "We define $N = \\text{NQuad} \\ / \\ 2$. This variable will be used extensively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7873aa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = NQuad // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b40ab3",
   "metadata": {},
   "source": [
    "* **(OPTIONAL) Number of phase function Legendre coefficients to use (NMOM)**\n",
    "\n",
    "We require $\\text{NLeg} \\leq \\text{NQuad}$. In general we recommend choosing $\\text{NLeg} = \\text{NQuad}$, and this is the default. Stamnes' DISORT [[3]](#cite-Sta1999) only allows $\\text{NLeg} = \\text{NQuad}$. See section [3.4.1](#3.4.1-How-to-choose-computational-parameters) for an explanation of the requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd9dc96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### SUPPLIED TO PYDISORT ########################\n",
    "\n",
    "NLeg = NQuad\n",
    "\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f75ef7f",
   "metadata": {},
   "source": [
    "* **(OPTIONAL) Number of loops, also number of Fourier modes to use in the numerical solution**\n",
    "\n",
    "The number of loops must be positive and cannot exceed the number of phase function Legendre coefficients ($\\text{NLeg}$). The default is $\\text{NLoops} = \\text{NQuad}$. See section [3.4.1](#3.4.1-How-to-choose-computational-parameters) for an explanation of this parameter and how it is adaptive in Stamnes' DISORT [[3]](#cite-Sta1999). Note that `only_flux = True` overwrites this parameter and is equivalent to `NLoops = 1`, except that `only_flux = True` will also cause the intensity function to not be returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d37b5fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### SUPPLIED TO PYDISORT ########################\n",
    "\n",
    "NLoops = NQuad # Maximum NLoops\n",
    "\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ef686f",
   "metadata": {},
   "source": [
    "* **(OPTIONAL) Whether to only compute the flux, or compute both flux and intensity (ONLYFL)**\n",
    "\n",
    "If `True`, PyDISORT will be much faster because we will only need to solve the $0$th Fourier mode integro-differential equation, see section [3.8](#3.8-Computation-of-flux)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1330118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### SUPPLIED TO PYDISORT ########################\n",
    "\n",
    "only_flux = False\n",
    "\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4802d53",
   "metadata": {},
   "source": [
    "* **(OPTIONAL; EXPERIMENTAL) Whether to parallelize the for-loop over Fourier modes**\n",
    "\n",
    "The outermost loop in PyDISORT is relatively easy to parallelize and we will use the *parfor* package to do so. Parallelization comes with massive initialization costs, however, so if $\\text{NLoops}$ is small it may be faster to disable parallelization with the flag `parfor_Fourier=False`, and this is the default. See section [3.4.2](#3.4.2-Assembly-of-system) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e2986b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### SUPPLIED TO PYDISORT ########################\n",
    "\n",
    "parfor_Fourier = False\n",
    "\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f834616",
   "metadata": {},
   "source": [
    "## 1.3 Choose phase function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d5c631",
   "metadata": {},
   "source": [
    "We assume that the phase function is directly dependent on only the scattering angle $\\gamma$, i.e. the scattering medium has spherical symmetry. Then, we can follow the method in [[2]](#cite-STWJ1988), but with slightly different notations and definitions, and expand the phase function in a Legendre series with respect to $\\cos\\gamma$:\n",
    "\n",
    "$$\n",
    "p(\\cos\\gamma) \\approx \\sum_{\\ell=0}^\\text{NLeg} (2\\ell + 1)g_\\ell P_\\ell(\\cos\\gamma), \\quad g_\\ell = \\frac{1}{2}\\int_{-1}^{1} p(\\cos\\gamma) P_\\ell(\\cos\\gamma) \\mathrm{d}\\cos\\gamma\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05020faa",
   "metadata": {},
   "source": [
    "The scattering angle $\\gamma$ is between the incident angular vector $\\left(\\theta', \\phi'\\right)$ and the outgoing angular vector $(\\theta, \\phi)$ such that\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\cos\\gamma &= \\cos\\theta'\\cos\\theta + \\sin\\theta'\\sin\\theta\\cos\\left(\\phi'-\\phi\\right) \\\\\n",
    "\\iff \\nu &= \\mu' \\mu + \\sqrt{1 - \\mu'^2} \\sqrt{1 - \\mu^2} \\cos\\left(\\phi'-\\phi\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where we define\n",
    "\n",
    "$$\n",
    "\\nu = \\cos\\gamma, \\quad \\mu = \\cos\\theta, \\quad\\mu' = \\cos\\theta'\n",
    "$$\n",
    "\n",
    "Therefore, by the addition theorem for spherical harmonics\n",
    "\n",
    "$$\n",
    "P_\\ell(\\nu) = P_\\ell\\left(\\mu'\\right)P_\\ell(\\mu) + 2\\sum_{m=1}^\\ell \\frac{(\\ell-m)!}{(\\ell+m)!}P_\\ell^m\\left(\\mu'\\right)P_\\ell^m(\\mu)\\cos\\left(m\\left(\\phi'-\\phi\\right)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9b2d19",
   "metadata": {},
   "source": [
    "In general, we will use the $\\nu$-argument form in PyDISORT as it is the most straightforward to integrate. In our exposition, however, we will generally express a phase function with arguments $\\mu, \\phi, \\mu', \\phi'$ for consistency with the radiative transfer equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a04bb03",
   "metadata": {},
   "source": [
    "***Henyey-Greenstein phase function***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962d17ec",
   "metadata": {},
   "source": [
    "Our definition excludes $\\omega$ and is normalized to $1$,\n",
    "\n",
    "$$p(\\nu) = \\frac{1-g^2}{\\left(1+g^2-2 g \\nu\\right)^{3 / 2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c55f15d",
   "metadata": {},
   "source": [
    "**Asymmetry factor (GG)**\n",
    "\n",
    "The magnitude of $g$ must be less than $1$. Magnitudes close to $1$ will cause instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d62297c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76584e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_HG_nu = lambda nu: (1 - g**2) / (1 + g**2 - 2 * g * nu) ** (3 / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b14d0e",
   "metadata": {},
   "source": [
    "* **Unweighted phase function Legendre coefficients** $g_\\ell$ **(PMOM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "debaf671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not used in PyDISORT\n",
    "NLeg_all = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d07f3b",
   "metadata": {},
   "source": [
    "We encourage having `Leg_coeffs_all` contain as many phase function Legendre coefficients as are available. For most part PyDISORT will only use $\\text{NLeg}$ coefficients, but the remaining coefficients are important for approximating the true phase function as accurately as possible for NT corrections, see section [3.7.2](#3.7.2-Nakajima-Tanaka-corrections). We require at least $\\text{NLeg}$ coefficients each with magnitude $<= 1$. For a multi-layer atmosphere, `Leg_coeffs_all` must be a matrix which $l$th row index corresponds to the atmospheric layer indexed by $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22713e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### SUPPLIED TO PYDISORT ########################\n",
    "\n",
    "Leg_coeffs_all = np.tile(g ** np.arange(NLeg_all), (2, 1))\n",
    "\n",
    "##################################################################\n",
    "\n",
    "# Used internally in PyDISORT\n",
    "Leg_coeffs = Leg_coeffs_all[:, :NLeg]\n",
    "Leg_coeffs_l = Leg_coeffs[l, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b777425",
   "metadata": {},
   "source": [
    "***Rayleigh phase function***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167955ce",
   "metadata": {},
   "source": [
    "$$p(\\nu) = \\frac{3}{4} (1 + \\nu^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5516bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_R_nu = lambda nu: (3 / 4) * (1 + nu**2)  # Currently unused"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c41735",
   "metadata": {},
   "source": [
    "**Integral derivation of Legendre coefficients for verification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec4f8c5",
   "metadata": {},
   "source": [
    "The following algorithm can be used to derive the Legendre coefficients for a general phase function. The algorithm can also be vectorized, but we would no longer be able to use `scipy.integrate.quad` for integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ad0fddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed all tests\n"
     ]
    }
   ],
   "source": [
    "Leg_coeffs_l_test = np.empty(NLeg)\n",
    "for ell in range(NLeg):\n",
    "    integrand = lambda nu: p_HG_nu(nu) * sc.special.eval_legendre(ell, nu)\n",
    "    Leg_coeffs_l_test[ell] = (1 / 2) * integrate.quad(integrand, -1, 1)[0]\n",
    "\n",
    "assert np.allclose(Leg_coeffs_l, Leg_coeffs_l_test)\n",
    "\n",
    "print(\"Passed all tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4868357a",
   "metadata": {},
   "source": [
    "### 1.3.1 OPTIONAL: Choose delta-M scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a4325a",
   "metadata": {},
   "source": [
    "Standard Legendre series approximation of highly anisotropic phase functions require a large number of terms to accurately capture the strong directional scattering. This is due to the slow decay of their Legendre coefficients. Using the HG phase function as an illustrative example, its Legendre coefficients are given by $\\mathscr{g}_\\ell = g^\\ell$ where $g$ is the asymmetry factor. It is clear therefore that the closer $|g|$ is to $1$, i.e. the more anisotropic the HG phase function is, the slower its Legendre coefficients will decay.\n",
    "\n",
    "We can use fewer Legendre terms if we re-express the phase function as a linear combination of a Dirac $\\delta$-function and a more isotropic remainder. We follow the method in [[6]](#cite-Wis1977) for scaling the first $2M$ Legendre coefficients of a phase function such that they become the coefficients of the remainder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f5735d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "&p(\\nu) \\approx 2 f \\delta(1-\\nu) + (1 - f) \\sum_{\\ell=0}^{2 M-1} (2\\ell + 1) g_\\ell^* P_\\ell(\\nu), \\quad g^*_\\ell = \\frac{g_\\ell - f}{1 - f} \\\\\n",
    "&\\iff p(\\mu, \\phi; \\mu', \\phi') \\approx 4 \\pi f \\delta(\\mu - \\mu')\\delta(\\phi - \\phi') + (1 - f) p^*(\\mu, \\phi; \\mu', \\phi')\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where the fractional scattering into peak $f \\in [0, 1)$ is to be chosen, with $f = 0$ equivalent to no $\\delta-M$ scaling. The method is equivalent to approximating the truncated coefficients as $f$ instead of $0$ as in standard truncation approximation. The first $2M$ Legendre coefficients of this re-expression will agree with those of the phase function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251603bc",
   "metadata": {},
   "source": [
    "We generally choose $f = g_{2M} = g_\\text{NLeg}$ so that the first $2M + 1$, and not just $2M$, Legendre coefficients of the re-expression agree with those of the phase function. This is automatically done in Stamnes' DISORT [[3]](#cite-Sta1999). This choice of $f$ with $M = 1$ is equivalent to the delta-Eddington method, see [[7]](#cite-JWW1976). We will discuss the impact of this phase function re-expression on the radiative transfer equation in section [3.3](#3.3-Delta-M-scaling)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e172a8e",
   "metadata": {},
   "source": [
    "* **(OPTIONAL) Fractional scattering into peak (Automatically chosen in Stamnes' DISORT [[3]](#cite-Sta1999))**\n",
    "\n",
    "We require $f \\in [0, 1)$. Values close to $1$ will cause instability. The zeroth entry of the array corresponds to the zeroth (topmost) atmospheric layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "962f6f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### SUPPLIED TO PYDISORT ########################\n",
    "\n",
    "f_arr = np.array([Leg_coeffs[l - 1, -1], Leg_coeffs_all[l, -1]])\n",
    "\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef87ed2",
   "metadata": {},
   "source": [
    "* **(OPTIONAL) Whether to perform Nakajima-Tanaka intensity corrections, see section [3.7.2](#3.7.2-Nakajima-Tanaka-corrections) (Always true in Stamnes' DISORT [[3]](#cite-Sta1999))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "563cae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### SUPPLIED TO PYDISORT ########################\n",
    "\n",
    "NT_cor = False\n",
    "\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952e789d",
   "metadata": {},
   "source": [
    "## 1.4 Choose incident beam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d2926d",
   "metadata": {},
   "source": [
    "* **Parameters for the incident (direct) collimated (sun) beam**\n",
    "\n",
    "In order to prevent singularities in the term $x$ in section [3.7.2](#3.7.2-Nakajima-Tanaka-corrections), we do not allow $\\mu_0$ to coincide with a quadrature angle. If that happens either $\\text{NQuad}$ or $\\mu_0$ should be tweaked. We also do not allow $\\mu_0 \\leq 0$. PyDISORT is not designed to model a beam shining up from the surface into the bottom of the atmosphere. Small $\\mu_0$ values will cause instability. We require both angles to be principal. Choosing $I_0 = 0$ disables this source and other internal sources can be chosen in section [1.7](#1.7-OPTIONAL:-Choose-other-internal-sources).\n",
    "\n",
    "The flux contribution of this source is $I_0 \\mu_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52dafd3a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#################### SUPPLIED TO PYDISORT ########################\n",
    "\n",
    "# Intensity (FBEAM)\n",
    "I0 = 200\n",
    "# Cosine of polar angle (UMU0)\n",
    "mu0 = 0.5\n",
    "# Azimuthal angle (PHI0)\n",
    "phi0 = 1.2 * pi\n",
    "\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e9b571",
   "metadata": {},
   "source": [
    "## 1.5 OPTIONAL: Choose Dirichlet BCs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265b83a4",
   "metadata": {},
   "source": [
    " * **(OPTIONAL) Dirichlet boundary conditions (No direct equivalent in Stamnes' DISORT [[3]](#cite-Sta1999), but see variable *IBCND*)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6a7466",
   "metadata": {},
   "source": [
    "$$\n",
    "u\\left(\\tau_{\\text{BoA}}, \\mu_i, \\phi \\right) = \\sum_{m = 0}^{\\text{NLoops}}b^+_{im}\\cos(m(\\phi_0 - \\phi)), \\quad u(0, -\\mu_i, \\phi) = \\sum_{m = 0}^{\\text{NLoops}}b^-_{im}\\cos(m(\\phi_0 - \\phi))\n",
    "$$\n",
    "\n",
    "for $i = 1, \\dots, N$, where $b^\\pm$ are matrices to be specified and \"BoA\" stands for \"Bottom of Atmosphere\". We have $i$ and $m$ as the row and column indices respectively. We also allow each of $b^\\pm$ to be a constant. The default is $b^+ = b^- = 0$ (homogeneous BCs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeac418",
   "metadata": {},
   "source": [
    "If $u\\left(\\tau_{\\text{BoA}}, \\mu, \\phi \\right) = \\psi^+(\\mu, \\phi)$ and $u\\left(0, -\\mu, \\phi \\right) = \\psi^-(\\mu, \\phi)$, then $\\psi^+$ and $\\psi^-$ must first be discretized in $\\mu$, following which $b^+_{im}$ and $b^-_{im}$ are the truncated Fourier coefficients of $\\psi^+_i$ and $\\psi^-_i$ respectively. Note that `PyDISORT.subroutines.Gauss_Legendre_quad(N)` can be used to get the $\\mu$ grid points. We require $\\psi^+$ and $\\psi^-$ to be even about some $\\phi$ value (not necessarily $\n",
    "\\phi_0$) for them to be compatible with the pure cosine series expansion of the solution. Each BC input must be a scalar or a matrix of dimension $N \\times \\text{NLeg}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3802abd8",
   "metadata": {},
   "source": [
    "The BoA Dirichlet BC can be used to model longwave radiation from the surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c2bd769",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#################### SUPPLIED TO PYDISORT ########################\n",
    "\n",
    "# At bottom of atmosphere\n",
    "b_pos = 0\n",
    "# At top of atmosphere\n",
    "b_neg = 0\n",
    "\n",
    "##################################################################\n",
    "\n",
    "\n",
    "# Code within pydisort to ensure that the BC inputs are of the correct shape\n",
    "scalar_b_pos, scalar_b_neg = False, False\n",
    "if len(np.atleast_1d(b_pos)) == 1:\n",
    "    scalar_b_pos = True\n",
    "else:\n",
    "    assert np.shape(b_pos) == (N, NLoops)\n",
    "    \n",
    "if len(np.atleast_1d(b_neg)) == 1:\n",
    "    scalar_b_neg = True\n",
    "else:\n",
    "    assert np.shape(b_neg) == (N, NLoops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf0da42",
   "metadata": {},
   "source": [
    "## 1.6 OPTIONAL: Choose BDRF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aa1596",
   "metadata": {},
   "source": [
    "In order to specify a reflective surface with Bi-Directional Reflectance Distribution Function (BDRF) $q$, specify `NLeg` of its Legendre coefficients $h_\\ell$ with respect to the cosine of the scattering angle $\\nu$. For $ 0 < \\mu, \\mu' \\leq 1$, we have\n",
    "\n",
    "$$q(\\mu, \\phi; -\\mu', \\phi') = q(\\nu) \\approx \\sum_{\\ell=0}^\\text{NBDRF} (2\\ell + 1)h_\\ell P_\\ell(\\nu), \\quad h_\\ell = \\frac{1}{2}\\int_{-1}^{1} q(\\nu) P_\\ell(\\nu) \\mathrm{d}\\nu$$\n",
    "\n",
    "This is analogous to our treatment of the phase function $p$, and we similarly assume $q$ to demonstrate spherical symmetry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c2f893",
   "metadata": {},
   "source": [
    "Note that\n",
    "\n",
    "$$\\frac{1}{\\pi}\\int_0^1 \\int_0^{2\\pi} \\mu q(\\mu, \\phi; -\\mu', \\phi') \\mathrm{d} \\phi \\mathrm{d} \\mu = \\omega_s$$\n",
    "\n",
    "where $\\omega_s$ is the albedo of the surface. It is the user's responsibility to ensure that $0 \\leq \\omega_s \\leq 1$ in accordance with physical constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a91966",
   "metadata": {},
   "source": [
    "***Lambertian BDRF***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c91551a",
   "metadata": {},
   "source": [
    "$$q(\\nu) = \\frac{\\omega_s}{\\pi}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9e9315",
   "metadata": {},
   "source": [
    "**Surface albedo (ALBEDO)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cb63ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "omega_s = 0.1 # Ocean albedo is approximately 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b33ce2f",
   "metadata": {},
   "source": [
    " * **(OPTIONAL) Unweighted BDRF Legendre coefficients (No direct equivalent in Stamnes' DISORT [[3]](#cite-Sta1999), but see variables *IBCND* and *LAMBER*)**\n",
    " \n",
    " We require the number of BDRF Legendre coefficients to be non-negative and $\\leq \\text{NLeg}$, the number of phase function Legendre coefficients. PyDISORT defaults to a black surface, i.e. `Leg_coeffs_BDRF = []`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7aabefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### SUPPLIED TO PYDISORT ########################\n",
    "\n",
    "Leg_coeffs_BDRF = np.array([omega_s / pi])\n",
    "\n",
    "##################################################################\n",
    "\n",
    "# Used internally in PyDISORT\n",
    "NBDRF = len(Leg_coeffs_BDRF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6aaa05",
   "metadata": {},
   "source": [
    "## 1.7 OPTIONAL: Choose other internal sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd2014f",
   "metadata": {},
   "source": [
    "PyDISORT is mostly designed to model solar radiation and as such the internal source we care the most about is scattered light from the sunbeam. PyDISORT can accomodate other internal sources, in particular atmospheric blackbody radiation, though our current implementation is crude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8d3a26",
   "metadata": {},
   "source": [
    "* **Particular solution corresponding to the other internal sources**\n",
    "\n",
    "Given source function $s$, define the vector \n",
    "\n",
    "$$\\tilde{\\mathrm{S}}(\\tau) = \\begin{bmatrix} -\\tilde{\\mathrm{S}}^+(\\tau) \\\\ \\tilde{\\mathrm{S}}^-(\\tau) \\end{bmatrix}$$\n",
    "\n",
    "where vectors $\\tilde{\\mathrm{S}}^\\pm(\\tau)$ have $i$th entry $\\mu_i^{-1}s(\\tau, \\pm\\mu_i)$. Grid points $\\mu_i$ can be determined using `PyDISORT.subroutines.Gauss_Legendre_quad(N)[0]`. An optional input into `pydisort` is the function\n",
    "\n",
    "$$\n",
    "\\mathscr{v}_s(\\tau; \\text{NQuad}, G, K, G_\\text{inv}) = \\int_{\\tau_\\text{BoA}}^{\\tau} G \\exp(\\Sigma(\\tau - t)) G_\\text{inv}\\tilde{\\mathrm{S}}(t) \\mathrm{d} t\n",
    "$$\n",
    "\n",
    "where the diagonal of diagonal matrix $\\Sigma$ is $K$. This function has **exactly five arguments**: ***vector*** $\\tau$, integer $\\text{NQuad}$, matrix $G$, vector $K$ and matrix $G_\\text{inv}$. The axes $0, 1$ of the output capture $\\mu, \\tau$ variation respectively. Given the diagonalization $A = G \\Sigma G^{-1}$, equivalent formulations of $\\mathscr{v}_s$ are\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathscr{v}_s &= \\int_{\\tau_\\text{BoA}}^{\\tau} \\exp(A(\\tau - t))\\tilde{\\mathrm{S}}(t) \\mathrm{d} t \\\\\n",
    "\\frac{\\mathrm{d}\\mathscr{v}_s}{\\mathrm{d}\\tau} &= A\\mathscr{v}_s + \\tilde{\\mathrm{S}}(\\tau), \\quad \\mathscr{v}_s\\left(\\tau_\\text{BoA}\\right) = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "See section [3.5](#3.5-Diagonalization-of-the-coefficient-matrix-for-each-Fourier-mode) and onwards for explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7448bfa",
   "metadata": {},
   "source": [
    "It may be challenging to correctly, not to mention efficiently, code the above expression even if the integration is tractable; *a better implementation is required*. The default argument is `mathscr_vs = None` which we will keep for every verification test in this notebook except for the one in section [3.6.1](#3.6.1-The-particular-solutions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2037ce84",
   "metadata": {},
   "source": [
    "**Example with parameters: blackbody radiation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1978c83",
   "metadata": {},
   "source": [
    "Let $B$ denote the Planck function with SI units. For each atmospheric layer $l$ with $\\tau \\in [\\tau_{l-1}, \\tau_l]$, we define\n",
    "\n",
    "$$s(\\tau) = \\epsilon_l B_\\lambda(T(\\tau))$$\n",
    "\n",
    "where the wavelength $\\lambda$ is to be chosen and $\\epsilon_l = 1 - \\omega_l$ by Kirchoff's law. Following [[4, section 2.5]](#cite-STL2000), we make the assumption that\n",
    "\n",
    "$$B_\\lambda(T(\\tau)) = m\\tau + C$$\n",
    "\n",
    "and the constants $m, C$ are determined by interpolating the points $\\big(T(\\tau_{l-1}), B(T(\\tau_{l-1}))\\big)$ and $\\big(T(\\tau_{l}), B(\\tau_{l})\\big)$ where $T(\\tau_{l-1}), T(\\tau_{l})$ are to be chosen. We get\n",
    " \n",
    "$$m = \\frac{B(T(\\tau_{l})) - B(T(\\tau_{l-1}))}{\\tau_{l} - \\tau_{l-1}}, \\quad C = B(T(\\tau_{l})) - m\\tau_{l}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bc8b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Parameters to choose ########################\n",
    "\n",
    "wavelength = 10e-6\n",
    "T_l = 220  # Temperature at Top of Atmosphere (we assume a single-layer atmosphere)\n",
    "T_lm1 = 300  # Temperature at Bottom of Atmosphere\n",
    "\n",
    "##################################################################\n",
    "\n",
    "Planck = lambda T: (2 * sc.constants.h * sc.constants.c**2 / wavelength**5) / (\n",
    "    np.exp(sc.constants.h * sc.constants.c / (wavelength * sc.constants.k * T)) - 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5888258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_coeff = 1 - omegal  # By Kirchoff's law\n",
    "mu_arr = np.concatenate(\n",
    "    (\n",
    "        PyDISORT.subroutines.Gauss_Legendre_quad(N)[0],\n",
    "        -PyDISORT.subroutines.Gauss_Legendre_quad(N)[0],\n",
    "    )\n",
    ")\n",
    "m = (Planck(T_l) - Planck(T_lm1)) / tau_arr[-1]  # We assume a single-layer atmosphere\n",
    "C_Plank = Planck(T_l) - m * tau_arr[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6c5819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_tilde = lambda tau: (m * tau + C_Plank)[None, :] / -mu_arr[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c460b69d",
   "metadata": {},
   "source": [
    "Consequently, we have that\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathscr{v}_s(\\tau; \\text{NQuad}, G, K, G_\\text{inv}) &= \\int_{\\tau_\\text{BoA}}^{\\tau} G \\exp(\\Sigma(\\tau - t)) G_\\text{inv}\\tilde{\\mathrm{S}}(t) \\mathrm{d} t \\\\\n",
    "&= \\int_{\\tau_\\text{BoA}}^{\\tau} G \\exp(\\Sigma(\\tau - t))(m\\tau + C) G_\\text{inv}\\mu_i^{-1} \\mathrm{d} t \\\\\n",
    "&= G \\big[\\exp\\left(\\Sigma(\\tau - \\tau_\\text{BoA})\\right)\\left(\\left(m\\tau_\\text{BoA} + C\\right)\\Sigma^{-1} + m\\Sigma^{-2} \\right) \\\\\n",
    "&\\quad- C\\Sigma^{-1} - m\\tau\\Sigma^{-1} - m \\Sigma^{-2} \\big] G_\\text{inv}\\left(-\\mu_i^{-1}\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "822e3680",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### SUPPLIED TO PYDISORT #########################################\n",
    "\n",
    "\n",
    "def mathscr_vs(tau, NQuad, G, K, G_inv):\n",
    "    len_tau = len(tau)\n",
    "    arange = np.arange(NQuad)\n",
    "\n",
    "    indices0_and_1 = np.repeat(arange, len_tau)\n",
    "    indices2 = np.tile(np.arange(len_tau), NQuad)\n",
    "\n",
    "    K_AlNo = K[:, None]\n",
    "    tau_NoAl = tau[None, :]\n",
    "    tau_BoA = tau_arr[-1]\n",
    "\n",
    "    tensordiag = np.zeros((NQuad, NQuad, len_tau))\n",
    "    tensordiag[indices0_and_1, indices0_and_1, indices2] = (\n",
    "        np.exp(K_AlNo * (tau_NoAl - tau_BoA))\n",
    "        * ((m * tau_BoA + C_Plank) / K_AlNo + m / K_AlNo**2)\n",
    "        - C_Plank / K_AlNo\n",
    "        - m * tau_NoAl / K_AlNo\n",
    "        - m / K_AlNo**2\n",
    "    ).flatten()\n",
    "\n",
    "    return np.einsum(\n",
    "        \"ij, jkt, k -> it\",\n",
    "        G,\n",
    "        tensordiag,\n",
    "        # All mu dependent terms in \\tilde{S} must be multiplied to the left of G^{-1}.\n",
    "        G_inv @ (1 / -mu_arr),\n",
    "    )\n",
    "\n",
    "\n",
    "#####################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c834594",
   "metadata": {},
   "source": [
    "By definition $\\mathscr{v}_s(\\tau_\\text{BoA}) = 0$ and this will be verified in PyDISORT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2882ef4f",
   "metadata": {},
   "source": [
    "# 2. PyDISORT modules and outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae28b30",
   "metadata": {},
   "source": [
    "PyDISORT comes with three modules: `subroutines`, `_basic_solver` and `pydisort`. The `_basic_solver` and `pydisort` modules each contain a single eponymous function. The `_basic_solver` function should not be called directly; it is called in `pydisort`.\n",
    "\n",
    "The `subroutines` module contains miscellaneous functions that are called in `_basic_solver` and `pydisort`. These functions can also be called directly, as will be done in section [3](#3.-Breakdown-and-verification-of-single-layer-solver). Users may want to directly call three functions in particular: `Clenshaw_Curtis_quad` and `Gauss_Legendre_quad` generate the quadrature nodes and weights for $\\mu$ integration from $-1$ to $1$, and $\\phi$ integration from $0$ to $2\\pi$ respectively. The third function `generate_FD_mat` generates a sparse first derivative matrix with second-order accuracy. Both `Clenshaw_Curtis_quad` and `generate_FD_mat` are called in neither `_basic_solver` nor `pydisort` but are important for verification tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78f8524",
   "metadata": {},
   "source": [
    "**The outputs of the `pydisort` function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a83658",
   "metadata": {},
   "source": [
    "The primary function `pydisort` will always return `mu_arr` (array), `flux_up` (function) and `flux_down` (function). The first output `mu_arr` is the array of $\\mu_i$ values, i.e. $\\mu$ quadrature nodes. We actually constructed `mu_arr` in the blackbody radiation example we provided for section [1.7](#1.7-OPTIONAL:-Choose-other-internal-sources). Both `flux_up` $(F^+)$ and `flux_down` $(F^-)$ accept only one argument, $\\tau$, and return the diffuse fluxes at the specified optical depths in the upward and downward directions respectively. The latter also returns the direct flux as its second output. More details on the flux functions can be found in section [3.8](#3.8-Computation-of-flux). The distinction between \"direct\" and \"diffuse\" is explained in the preamble, above section [1](#1.-USER-INPUT-REQUIRED:-Choose-parameters). \n",
    "\n",
    "When `only_flux = False` (default), `pydisort` will also output the (diffuse specific) intensity function `u` $(u)$ which is continuous and variable in $\\tau$ and $\\phi$ (its arguments) but discrete in $\\mu$ and fixed to the quadrature points $\\mu_i$. The function output is 3D and axes $0, 1, 2$ capture $\\mu, \\tau, \\phi$ variation respectively. The first half of the $\\mu$ indices correspond to $u^+$ (upward) and the second half to $u^-$ (downward) in alignment with `mu_arr`. Finally, note the optional flag `return_Fourier_error` which defaults to `False`, see section [3.7](#3.7-The-full-solution) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e75765",
   "metadata": {},
   "source": [
    "We import our PyDISORT package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc4db2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyDISORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a32f4b7",
   "metadata": {},
   "source": [
    "# 3. Breakdown of single layer solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a4f605",
   "metadata": {},
   "source": [
    "In this section we break down and explain the code for solving the radiative transfer equation for a single atmospheric layer. Unless otherwise stated, we will disable $\\delta-M$ scaling (`f = 0`), NT corrections (`NT_cor = False`) and have no other internal sources (`mathscr_vs = None`). We will explain $\\delta-M$ scaling and NT corrections in their independent sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c48098e",
   "metadata": {},
   "source": [
    "**Tensor product coding philosophy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75344836",
   "metadata": {},
   "source": [
    "First, a foreword on our coding philosophy for tensor products. We will use the following methods in order of preference:\n",
    "\n",
    "* Broadcasting: e.g. outer product of `array1` and `array2` is coded as `array1[:, None] * array2[None, :]`.\n",
    "* NumPy function `einsum` with `optimize = True`: `np.einsum` uses Einstein's summation convention. The documentation can be found at https://numpy.org/doc/stable/reference/generated/numpy.einsum.html.\n",
    "\n",
    "We believe that this is the best balance between code readability and speed. Other common NumPy functions for tensor products are `np.outer` and `np.tensordot` but we do not use them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df50562e",
   "metadata": {},
   "source": [
    "## 3.1 Quadrature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29043208",
   "metadata": {},
   "source": [
    "Generation of Double Gauss-Legendre quadrature weights and points to numerically integrate over $\\mu$ from $-1$ to $1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "180909f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For positive mu values (the weights are identical for both domains)\n",
    "mu_arr_pos, weights_mu = PyDISORT.subroutines.Gauss_Legendre_quad(N) # mu_arr_neg = -mu_arr_pos\n",
    "mu_arr = np.concatenate((mu_arr_pos, -mu_arr_pos))\n",
    "full_weights_mu = np.concatenate((weights_mu, weights_mu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ee345",
   "metadata": {},
   "source": [
    "### 3.1.1 Verification of quadrature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157d2874",
   "metadata": {},
   "source": [
    "$$\\int_{a}^{b} e^x \\ \\mathrm{d}x = e^b - e^a$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a8926fa",
   "metadata": {
    "code_folding": [],
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gauss-Legendre quadrature error ratio = 0.0\n"
     ]
    }
   ],
   "source": [
    "# Double Gauss-Legendre quadrature; integrate from -1 to 1\n",
    "true_sol = np.exp(1) - np.exp(-1)\n",
    "\n",
    "print(\n",
    "    \"Gauss-Legendre quadrature error ratio =\",\n",
    "    np.abs((true_sol - np.sum(np.exp(mu_arr) * full_weights_mu)) / true_sol),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c5f8be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of phi grid points\n",
    "# This selection should ensure that the phi quadrature is at least as accurate as the mu quadrature\n",
    "Nphi = int((NQuad * pi) // 2) * 2 + 1   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ceca22",
   "metadata": {},
   "source": [
    "Generation of Clenshaw-Curtis quadrature weights and points to numerically integrate over $\\phi$ from $0$ to $2\\pi$; these will only be used in tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f449ffd3",
   "metadata": {
    "code_folding": [],
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clenshaw-Curtis quadrature error ratio = 2.1270086547936496e-16\n"
     ]
    }
   ],
   "source": [
    "# Clenshaw-Curtis quadrature; integrate from -1 to 1\n",
    "phi_arr, full_weights_phi = PyDISORT.subroutines.Clenshaw_Curtis_quad(Nphi)\n",
    "true_sol = np.exp(2 * pi) - 1\n",
    "\n",
    "print(\n",
    "    \"Clenshaw-Curtis quadrature error ratio =\",\n",
    "    np.abs((true_sol - np.sum(np.exp(phi_arr) * full_weights_phi)) / true_sol),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb0df06",
   "metadata": {},
   "source": [
    "### 3.1.2 Normalization verification of phase function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441203c1",
   "metadata": {},
   "source": [
    "We expect that\n",
    "\n",
    "$$\n",
    "\\frac{1}{4 \\pi} \\int_{-1}^1 \\int_0^{2 \\pi} p\\left(\\mu, \\phi ; \\mu', \\phi'\\right) d \\phi d \\mu = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b9f7a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_HG_muphi(mu, phi, mu_p, phi_p):\n",
    "    nu = PyDISORT.subroutines.calculate_nu(mu, phi, mu_p, phi_p)\n",
    "    return p_HG_nu(nu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7c23464",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max pointwise error ratio = 0.18221102484512897\n"
     ]
    }
   ],
   "source": [
    "phi_arr, full_weights_phi = PyDISORT.subroutines.Clenshaw_Curtis_quad(Nphi)\n",
    "\n",
    "normalize_pHG = np.einsum(\n",
    "    \"ijkl, i, j -> kl\",\n",
    "    p_HG_muphi(mu_arr, phi_arr, mu_arr, phi_arr),\n",
    "    full_weights_mu,\n",
    "    full_weights_phi,\n",
    "    optimize=True,\n",
    ") / (4 * pi)\n",
    "\n",
    "# Evidently the phase function is relatively difficult to integrate by quadrature\n",
    "print(\"Max pointwise error ratio =\", np.max(np.abs(normalize_pHG - 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b135f22",
   "metadata": {},
   "source": [
    "## 3.2 Fourier expansion of solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be81279f",
   "metadata": {},
   "source": [
    "*We will re-derive equations (6a) to (6d) of [[2]](#cite-STWJ1988) in this subsection.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fae7cb1",
   "metadata": {},
   "source": [
    "We wish to turn the problem of solving the radiative transfer equation\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu \\frac{\\partial u(\\tau, \\mu, \\phi)}{\\partial \\tau} = u(\\tau, \\mu, \\phi) &-\\frac{\\omega}{4 \\pi} \\int_{-1}^{1} \\int_{0}^{2 \\pi} p\\left(\\mu, \\phi ; \\mu', \\phi'\\right) u\\left(\\tau, \\mu', \\phi'\\right) \\mathrm{d} \\phi' \\mathrm{d} \\mu' \\\\\n",
    "&-\\frac{\\omega I_0}{4 \\pi} p\\left(\\mu, \\phi ;-\\mu_{0}, \\phi_{0}\\right) \\exp\\left(-\\mu_{0}^{-1} \\tau\\right) - s(\\tau, \\mu)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "into the problem of solving\n",
    "\n",
    "$$\n",
    "\\mu \\frac{d u^m(\\tau, \\mu)}{d \\tau}=u^m(\\tau, \\mu)-\\int_{-1}^1 D^m\\left(\\tau, \\mu, \\mu'\\right) u^m\\left(\\tau, \\mu'\\right) d \\mu' - Q^m(\\tau, \\mu) - \\delta_{0m}s(\\tau, \\mu)\n",
    "$$\n",
    "\n",
    "for each Fourier mode $m$, where $\\delta_{0m}$ is the Kronecker delta for indices $0$ and $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96955f2",
   "metadata": {},
   "source": [
    "**Definitions and expansions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a99d371",
   "metadata": {},
   "source": [
    "We have the definitions and expansions\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "u\\left(\\tau, \\mu, \\phi\\right) &\\approx \\sum_{n=0}^\\text{NLoops} u^n\\left(\\tau, \\mu\\right)\\cos\\left(n\\left(\\phi_0 - \\phi\\right)\\right) \\quad \\text{(Fourier cosine series expansion)}\\\\\n",
    "p\\left(\\nu\\right) &\\approx \\sum_{\\ell=0}^\\text{NLeg} \\mathscr{g}_\\ell P_\\ell\\left(\\nu\\right) \\quad \\text{(Legendre series expansion)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathscr{g}_\\ell &= \\frac{2\\ell + 1}{2}\\int_{-1}^{1} p\\left(\\nu\\right) P_\\ell\\left(\\nu\\right) \\mathrm{d}\\nu, \\quad &&\\mathscr{g}_\\ell^m = \\frac{\\left(\\ell-m\\right)!}{\\left(\\ell+m\\right)!} \\mathscr{g}_\\ell \\\\\n",
    "\\mu &= \\cos\\theta, \\quad &&\\,\\nu = \\cos\\gamma\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\mathscr{g}_\\ell$ are the *weighted* Legendre coefficients, $\\mathscr{g}_\\ell = (2 \\ell + 1) g_\\ell$. As before, we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\cos\\gamma &= \\cos\\theta'\\cos\\theta + \\sin\\theta'\\sin\\theta\\cos\\left(\\phi'-\\phi\\right) \\\\\n",
    "P_\\ell\\left(\\nu\\right) &= P_\\ell\\left(\\mu'\\right)P_\\ell\\left(\\mu\\right) + 2\\sum_{m=1}^\\ell \\frac{\\left(\\ell-m\\right)!}{\\left(\\ell+m\\right)!}P_\\ell^m(\\mu')P_\\ell^m\\left(\\mu\\right)\\cos\\left(m\\left(\\phi'-\\phi\\right)\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Consequently, we can expand the $\\mu, \\phi, \\mu', \\phi'$ form of the phase function as\n",
    "\n",
    "$$\n",
    "p\\left(\\mu, \\phi; \\mu', \\phi'\\right) \\approx \\sum_{\\ell=0}^\\text{NLeg} \\left[ \\mathscr{g}_\\ell P_\\ell\\left(\\mu'\\right)P_\\ell\\left(\\mu\\right) + 2\\sum_{m=1}^\\ell \\mathscr{g}_\\ell^m P_\\ell^m(\\mu')P_\\ell^m\\left(\\mu\\right)\\cos\\left(m\\left(\\phi'-\\phi\\right)\\right) \\right]\n",
    "$$\n",
    "\n",
    "Going forward we will omit the upper limit of a sum when it is irrelevant to reduce the clutter of our expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b5f6c3",
   "metadata": {},
   "source": [
    "**Multiple scattering term**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d8d22",
   "metadata": {},
   "source": [
    "We first focus on the multiple scattering term of the radiative transfer equation. We substitute the expansions of $p(\\mu, \\phi; \\mu', \\phi')$ and $u(\\tau, \\mu, \\phi)$ to get\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\frac{\\omega}{4 \\pi} \\int_{-1}^{1} \\int_{0}^{2 \\pi} p\\left(\\mu, \\phi ; \\mu', \\phi'\\right) u\\left(\\tau, \\mu', \\phi'\\right) \\mathrm{d} \\phi' \\mathrm{d} \\mu' \\\\\n",
    "&\\approx \\frac{\\omega}{4 \\pi} \\int_{-1}^{1} \\Bigg[ \\int_{0}^{2 \\pi} \\sum_{n=0} \\sum_{\\ell=0} u^n(\\tau) \\mathscr{g}_\\ell P_\\ell\\left(\\mu'\\right)P_\\ell(\\mu) \\cos\\left(n\\left(\\phi_0 - \\phi'\\right)\\right) \\\\\n",
    "&\\quad+ 2\\sum_{n=0} \\sum_{\\ell=0} \\sum_{m=1}^\\ell u^n(\\tau) \\mathscr{g}_\\ell^m P_\\ell^m(\\mu')P_\\ell^m(\\mu)\\cos\\left(m\\left(\\phi'-\\phi\\right)\\right) \\cos\\left(n\\left(\\phi_0 - \\phi'\\right)\\right) \\mathrm{d} \\phi' \\Bigg] \\mathrm{d} \\mu' \\\\\n",
    "&= \\frac{\\omega}{4 \\pi} \\int_{-1}^{1} \\Bigg[ \\int_{0}^{2 \\pi} \\sum_{\\ell=0} u^0(\\tau) \\mathscr{g}_\\ell P_\\ell\\left(\\mu'\\right)P_\\ell(\\mu) \\\\\n",
    "&\\quad+ 2\\sum_{n=1} \\sum_{\\ell=n} \\sum_{m=1}^\\ell u^n(\\tau) \\mathscr{g}_\\ell^m P_\\ell^m(\\mu')P_\\ell^m(\\mu)\\cos\\left(m\\left(\\phi'-\\phi\\right)\\right) \\cos\\left(n\\left(\\phi_0 - \\phi'\\right)\\right) \\mathrm{d} \\phi' \\Bigg] \\mathrm{d} \\mu' \\\\\n",
    "&= \\frac{\\omega}{4 \\pi} \\int_{-1}^{1} \\left[ 2\\pi \\sum_{\\ell=0} u^0(\\tau) \\mathscr{g}_\\ell P_\\ell\\left(\\mu'\\right)P_\\ell(\\mu) + 2\\pi\\sum_{n=1} \\sum_{\\ell=n} u^n(\\tau) \\mathscr{g}_\\ell^n P_\\ell^n(\\mu')P_\\ell^n(\\mu)\\cos\\left(n\\left(\\phi_0 - \\phi\\right)\\right) \\right] \\mathrm{d} \\mu' \\\\\n",
    "&= \\sum_{m=0} \\left\\{ \\int_{-1}^{1} \\frac{\\omega}{2} \\sum_{\\ell=m} u^m \\mathscr{g}_\\ell^m P_\\ell^m(\\mu')P_\\ell^m(\\mu) \\mathrm{d} \\mu'\\right\\} \\cos\\left(m\\left(\\phi_0 - \\phi\\right)\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The term in the curly brackets of the last line is the contribution of the multiple scttering term to the $m$th Fourier mode of the radiative transfer equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdd37d3",
   "metadata": {},
   "source": [
    "**Sunbeam source term**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4332606f",
   "metadata": {},
   "source": [
    "Next, we focus on the sunbeam source term; the other internal sources will only contribute to the $0$th Fourier mode as they have no $\\phi$ dependence. Once again, we substitute the expansion of $p(\\mu, \\phi; \\mu', \\phi')$ to get\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\frac{\\omega I_0}{4 \\pi} p\\left(\\mu, \\phi ;-\\mu_{0}, \\phi_{0}\\right) \\exp\\left(-\\mu_{0}^{-1} \\tau\\right) \\approx \\\\\n",
    "&\\frac{\\omega I_0}{4 \\pi} \\exp\\left(-\\mu_{0}^{-1} \\tau\\right) \\left[ \\sum_{\\ell=0} \\mathscr{g}_\\ell P_\\ell\\left(-\\mu_0\\right)P_\\ell(\\mu) + 2\\sum_{\\ell=0}\\sum_{m=1}^\\ell \\mathscr{g}_\\ell^m P_\\ell^m\\left(-\\mu_0\\right)P_\\ell^m(\\mu)\\cos\\left(m\\left(\\phi_0-\\phi\\right)\\right) \\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "It is immediately apparent that the contribution to the $0$th moment is\n",
    "\n",
    "$$\n",
    "\\frac{\\omega I_0}{4 \\pi} \\exp\\left(-\\mu_{0}^{-1} \\tau\\right)\\sum_{\\ell=0} \\mathscr{g}_\\ell P_\\ell\\left(-\\mu_0\\right)P_\\ell(\\mu)\n",
    "$$\n",
    "\n",
    "For $n \\geq 1$, to determine the contribution to the $n$th moment, we multiply by $\\pi^{-1}\\cos\\left(n\\left(\\phi_0-\\phi\\right)\\right)$ and integrate over $\\phi$ from $0$ to $2\\pi$ to get\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\frac{\\omega I_0}{4 \\pi} \\exp\\left(-\\mu_{0}^{-1} \\tau\\right) \\int_{0}^{2\\pi} \\frac{2}{\\pi}\\sum_{\\ell=0}\\sum_{m=1}^\\ell \\mathscr{g}_\\ell^m P_\\ell^m\\left(-\\mu_0\\right)P_\\ell^m(\\mu)\\cos\\left(m\\left(\\phi_0-\\phi\\right)\\right)\\cos\\left(n\\left(\\phi_0-\\phi\\right)\\right) \\mathrm{d}\\phi \\\\\n",
    "&= \\frac{\\omega I_0}{4 \\pi} \\exp\\left(-\\mu_{0}^{-1} \\tau\\right) \\int_{0}^{2\\pi} \\frac{2}{\\pi}\\sum_{\\ell=n}\\sum_{m=1}^\\ell \\mathscr{g}_\\ell^m P_\\ell^m\\left(-\\mu_0\\right)P_\\ell^m(\\mu)\\cos\\left(m\\left(\\phi_0-\\phi\\right)\\right)\\cos\\left(n\\left(\\phi_0-\\phi\\right)\\right) \\mathrm{d}\\phi \\\\\n",
    "&= \\frac{\\omega I_0}{2 \\pi} \\exp\\left(-\\mu_{0}^{-1} \\tau\\right) \\sum_{\\ell=n} \\mathscr{g}_\\ell^n P_\\ell^n\\left(-\\mu_0\\right)P_\\ell^n(\\mu)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore, the contribution of the source term to the $m$th Fourier mode of the radiative transfer equation (we perform the change of variables $m = n$) is\n",
    "\n",
    "$$\n",
    "\\frac{\\omega I_0 (2 - \\delta_{0m})}{4 \\pi}\\exp\\left(-\\mu_{0}^{-1} \\tau\\right)\\sum_{\\ell=0} \\mathscr{g}_\\ell^m P_\\ell^m\\left(-\\mu_0\\right)P_\\ell^m(\\mu)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0346b18",
   "metadata": {},
   "source": [
    "### 3.2.1 Surface reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf6fe7b",
   "metadata": {},
   "source": [
    "We also address the surface reflection which is given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\frac{1}{\\pi}\\int_0^1 \\int_0^{2\\pi} \\mu' \\left(u(\\tau, -\\mu', \\phi') + u_\\text{direct}(\\tau, -\\mu', \\phi')\\right) q(\\mu, \\phi; -\\mu', \\phi')  \\mathrm{d}\\phi' \\mathrm{d}\\mu' \\\\\n",
    "&= \\frac{1}{\\pi}\\int_0^1 \\int_0^{2\\pi} \\mu' \\left(u(\\tau, -\\mu', \\phi') + I_0 \\delta(\\mu_0 - \\mu') \\delta(\\phi - \\phi_0) \\exp\\left(-\\mu_{0}^{-1} \\tau\\right)\\right) q(\\mu, \\phi; -\\mu', \\phi')  \\mathrm{d}\\phi' \\mathrm{d}\\mu' \\\\\n",
    "&= \\frac{1}{\\pi}\\int_0^1 \\int_0^{2\\pi} \\mu' u(\\tau, -\\mu', \\phi') q(\\mu, \\phi; -\\mu', \\phi') \\mathrm{d}\\phi' \\mathrm{d}\\mu' + \\frac{I_0 \\mu_0}{\\pi} \\exp\\left(-\\mu_{0}^{-1} \\tau\\right) q(\\mu, \\phi; -\\mu_0, \\phi_0)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since we have the Legendre expansion of the BDRF,\n",
    "\n",
    "$$q(\\mu, \\phi; -\\mu', \\phi') = q(\\nu) \\approx \\sum_{\\ell=0}^\\text{NLeg} (2\\ell + 1)h_\\ell P_\\ell(\\nu), \\quad h_\\ell = \\frac{1}{2}\\int_{-1}^{1} q(\\nu) P_\\ell(\\nu) \\mathrm{d}\\nu$$\n",
    "\n",
    "we can apply the same techniques we used on the multiple scattering and source terms to get\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\sum_{m=0} \\Bigg\\{\\int_{0}^{1} 2\\sum_{\\ell=m} u^m(\\tau) \\mathscr{h}_\\ell^m \\mu'P_\\ell^m(\\mu')P_\\ell^m(\\mu)  \\mathrm{d} \\mu' \\\\ \n",
    "&+ \\frac{ I_0 \\mu_0 (2 - \\delta_{0m})}{ \\pi}\\exp\\left(-\\mu_{0}^{-1} \\tau\\right)\\sum_{\\ell=0} \\mathscr{h}_\\ell^m P_\\ell^m\\left(-\\mu_0\\right)P_\\ell^m(\\mu) \\Bigg\\} \\cos\\left(m\\left(\\phi_0 - \\phi\\right)\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\mathscr{h}_\\ell^m = \\frac{\\left(\\ell-m\\right)!}{\\left(\\ell+m\\right)!} (2\\ell + 1) h$$\n",
    "\n",
    "The term in the curly brackets of the last line is the contribution of the surface reflection to the $m$th Fourier mode of the radiative transfer equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735cac63",
   "metadata": {},
   "source": [
    "### 3.2.2 Important terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe996aec",
   "metadata": {},
   "source": [
    "For each Fourier mode, $m \\geq 0$, we have derived the integro-differential equation\n",
    "\n",
    "$$\n",
    "\\mu \\frac{d u^m(\\tau, \\mu)}{d \\tau}=u^m(\\tau, \\mu)-\\int_{-1}^1 D^m\\left(\\mu, \\mu'\\right) u^m\\left(\\tau, \\mu'\\right) d \\mu' - Q^m(\\tau, \\mu) - \\delta_{0m}s(\\tau, \\mu)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "D^m\\left(\\mu, \\mu' \\right) &= \\frac{\\omega}{2} \\sum_{\\ell=m}^\\text{NLeg} \\mathscr{g}_\\ell^m P_\\ell^m(\\mu')P_\\ell^m(\\mu) \\\\\n",
    "Q^m(\\tau, \\mu) &= X^m(\\mu) \\exp\\left(-\\mu_{0}^{-1} \\tau\\right) \\\\\n",
    "X^m(\\mu) &= \\frac{\\omega I_0 (2 - \\delta_{0m})}{4 \\pi}\\sum_{\\ell=0}^\\text{NLeg} \\mathscr{g}_\\ell^m P_\\ell^m\\left(-\\mu_0\\right)P_\\ell^m(\\mu)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and our approach will be to solve for each $u^m$ then construct the full solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60ba3b6",
   "metadata": {},
   "source": [
    "**Surface reflection terms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7895350",
   "metadata": {},
   "source": [
    "Similarly, for each Fourier mode, $m \\geq 0$, the surface reflection terms are\n",
    "\n",
    "$$\\int_0^1 \\mathscr{D}^m(\\mu, -\\mu') u^m(\\tau_\\text{BoA}, -\\mu') \\mathrm{d}\\mu' + \\mathscr{X}^m(\\mu)\\exp\\left(-\\mu_{0}^{-1} \\tau_\\text{BoA}\\right)$$\n",
    "\n",
    "with the analogous definitions\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathscr{D}^m(\\mu, -\\mu') &= 2\\sum_{\\ell=m}^\\text{NLeg} \\mathscr{h}_\\ell^m \\mu'P_\\ell^m(-\\mu')P_\\ell^m(\\mu) \\\\\n",
    "\\mathscr{X}^m(\\mu) &= \\frac{ I_0 \\mu_0 (2 - \\delta_{0m})}{ \\pi}\\sum_{\\ell=0}^\\text{NLeg} \\mathscr{h}_\\ell^m P_\\ell^m\\left(-\\mu_0\\right)P_\\ell^m(\\mu)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d6e98b",
   "metadata": {},
   "source": [
    "## 3.3 Delta-M scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a99d60e",
   "metadata": {},
   "source": [
    "Substitute the truncated, $\\delta-M$ approximated phase function\n",
    "\n",
    "$$\n",
    "p(\\mu, \\phi; \\mu', \\phi') = 4 \\pi f \\delta(\\mu - \\mu')\\delta(\\phi - \\phi') + (1 - f) p^*(\\mu, \\phi; \\mu', \\phi')\n",
    "$$\n",
    "\n",
    "into the radiative transfer equation to get\n",
    "\n",
    "\\begin{aligned}\n",
    "\\mu \\frac{\\partial u(\\tau, \\mu, \\phi)}{\\partial \\tau} = \\ &(1 - \\omega f) u(\\tau, \\mu, \\phi) -\\frac{(1 - f)\\omega}{4 \\pi} \\int_{-1}^{1} \\int_{0}^{2 \\pi} p^*\\left(\\mu, \\phi ; \\mu', \\phi'\\right) u\\left(\\tau, \\mu', \\phi'\\right) \\mathrm{d} \\phi' \\mathrm{d} \\mu' \\\\\n",
    "&-\\omega I_0 \\left(f \\delta(\\mu - \\mu_0)\\delta(\\phi - \\phi_0) + \\frac{1 - f}{4 \\pi} p^*\\left(\\mu, \\phi ;-\\mu_{0}, \\phi_{0}\\right)\\right) \\exp\\left(-\\mu_{0}^{-1} \\tau\\right) - s(\\tau, \\mu)\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d7d05b",
   "metadata": {},
   "source": [
    "Shift the $\\delta$-function term to the incident beam to get\n",
    "\n",
    "$$\\mu\\frac{\\partial u_\\text{direct}(\\tau, \\mu, \\phi)}{\\partial\\tau} = (1 - \\omega f)u_\\text{direct}(\\tau, \\mu, \\phi), \\quad u_\\text{direct}(0, \\mu, \\phi) = I_0 \\delta(\\mu + \\mu_0) \\delta(\\phi - \\phi_0)$$\n",
    "\n",
    "We have added $\\omega f$ of the incident beam that would originally be considered scattered back into the incident beam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff01466",
   "metadata": {},
   "source": [
    "Denote the remaining diffuse intensity as $u^*$, and the augmented direct intensity as $u^*_\\text{direct}$. Perform the change of variables\n",
    "\n",
    "$$\\tau^* = (1 - \\omega f) \\tau \\iff \\frac{\\mathrm{d}\\tau}{\\mathrm{d}\\tau^*} = \\frac{1}{1 - \\omega f}, \n",
    "\\quad \\omega^* = \\frac{1-f}{1 - \\omega f} \\omega, \\quad s^* = \\frac{1}{1 - \\omega f} s$$\n",
    "\n",
    "to get\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu \\frac{\\partial u^*(\\tau^*, \\mu, \\phi)}{\\partial \\tau^*} = u^*(\\tau^*, \\mu, \\phi) &-\\frac{\\omega^*}{4 \\pi} \\int_{-1}^{1} \\int_{0}^{2 \\pi} p^*\\left(\\mu, \\phi ; \\mu', \\phi'\\right) u^*\\left(\\tau^*, \\mu', \\phi'\\right) \\mathrm{d} \\phi' \\mathrm{d} \\mu' \\\\\n",
    "&-\\frac{\\omega^* I_0}{4 \\pi} p^*\\left(\\mu, \\phi ;-\\mu_{0}, \\phi_{0}\\right) \\exp\\left(-\\mu_{0}^{-1} \\tau^*\\right) - s^*\\left(\\tau^*, \\mu\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\mu\\frac{\\partial u^*_\\text{direct}(\\tau^*, \\mu, \\phi)}{\\partial\\tau^*} = u^*_\\text{direct}(\\tau^*, \\mu, \\phi), \\quad u^*_\\text{direct}(0, \\mu, \\phi) = I_0 \\delta(\\mu + \\mu_0) \\delta(\\phi - \\phi_0) \\\\\n",
    "&\\implies u^*_\\text{direct}(\\tau^*, \\mu, \\phi) = I_0 \\delta(\\mu + \\mu_0) \\delta(\\phi - \\phi_0) \\exp\\left(-\\mu_{0}^{-1} \\tau^*\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This is exactly our starting problem, but with every $\\tau$, $\\omega$, $p$, $s$ swapped for $\\tau^*$, $\\omega^*$, $p^*$, $s^*$ respectively. In the literature, $\\tau$, $\\omega$, $p$ are described as having been $\\delta-M$ scaled (or just \"delta-scaled\") to $\\tau^*$, $\\omega^*$, $p^*$. See [[6]](#cite-Wis1977) for more details on $\\delta-M$ scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e4ec46",
   "metadata": {},
   "source": [
    "## 3.4 System of ODEs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73b8abf",
   "metadata": {},
   "source": [
    "*We will prove equations (7a) and (7b) of [[2]](#cite-STWJ1988) in this subsection.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191c4f6b",
   "metadata": {},
   "source": [
    "In order to solve the integro-differential equation\n",
    "\n",
    "$$\n",
    "\\mu \\frac{d u^m(\\tau, \\mu)}{d \\tau}=u^m(\\tau, \\mu)-\\int_{-1}^1 D^m\\left(\\mu, \\mu'\\right) u^m\\left(\\tau, \\mu'\\right) d \\mu' - Q^m(\\tau, \\mu) - \\delta_{0m}s(\\tau, \\mu)\n",
    "$$\n",
    "\n",
    "for each Fourier mode $m$, we need to discretize the $\\mu$ integral. We split the $\\mu$ integral into two integrals: from $-1$ to $0$ and from $0$ to $1$ since there is a singularity at $\\mu = 0$. We approximate each integral by Gauss-Legendre quadrature. This is the *double-Gauss method* from [[8]](#cite-Syk1951)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54087f37",
   "metadata": {},
   "source": [
    "By double-Gauss quadrature, we can approximate the integro-differential equation as\n",
    "\n",
    "\n",
    "$$\n",
    "\\mu_i \\frac{d u^m(\\tau, \\mu_i)}{d \\tau}=u^m(\\tau, \\mu_i)-\\sum_{|j| = 1,\\,j \\neq 0}^N w_j D^m\\left(\\mu_i, \\mu_j\\right) u^m\\left(\\tau, \\mu_j\\right) - Q^m(\\tau, \\mu_i) - \\delta_{0m}s(\\tau, \\mu_i)\n",
    "$$\n",
    "\n",
    "for $i,j = 1, \\dots, N$, where $2N$ is the number of quadrature points. We define\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\alpha = M^{-1}\\left(D^{+} W - I\\right) &&\\beta = M^{-1} D^{-} W \\\\\n",
    "&D^{+}[i,j] = D^m\\left(\\mu_i, \\mu_j\\right) = D^m\\left(-\\mu_i,-\\mu_j\\right) &&D^{-}[i,j] = D^m\\left(-\\mu_i, \\mu_j\\right) = D^m\\left(\\mu_i,-\\mu_j\\right) \\\\\n",
    "&W[i,j] = w_i\\delta_{ij} &&M[i,j] = \\mu_i\\delta_{ij} \\\\ \n",
    "&\\mathrm{S}^{\\pm}(\\tau)[i] = \\mathrm{S}^m\\left(\\tau, \\pm \\mu_i\\right) &&Q^{\\pm}(\\tau)[i] = Q^m\\left(\\tau, \\pm \\mu_i\\right) \\\\\n",
    "&u^\\pm[i] = u^m(\\pm \\mu_i)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and, omitting the $\\tau$ argument, $\\tilde{Q}^\\pm = M^{-1} Q^\\pm$ and $\\tilde{\\mathrm{S}}^\\pm = M^{-1} \\mathrm{S}^\\pm$. This definition of $\\tilde{\\mathrm{S}}^\\pm$ is equivalent to the definition in section [1.7](#1.7-OPTIONAL:-Choose-other-internal-sources). We **claim** that the discretized equation for each Fourier mode can be re-expressed as the system of ODEs\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \\frac{\\mathrm{d}u^+}{\\mathrm{d}\\tau} \\\\ \\frac{\\mathrm{d}u^-}{\\mathrm{d}\\tau} \\end{bmatrix} = \\begin{bmatrix} -\\alpha & -\\beta \\\\ \\beta & \\alpha \\end{bmatrix} \\begin{bmatrix} u^+ \\\\ u^- \\end{bmatrix} + \\begin{bmatrix} -\\tilde{Q}^+ \\\\ \\tilde{Q}^- \\end{bmatrix} + \\delta_{0m}\\begin{bmatrix} -\\tilde{\\mathrm{S}}^+ \\\\ \\tilde{\\mathrm{S}}^- \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7a3b01",
   "metadata": {},
   "source": [
    "**Proof**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cb7bab",
   "metadata": {},
   "source": [
    "On the RHS, substitute $\\alpha, \\beta, \\tilde{Q}^\\pm, \\tilde{\\mathrm{S}}^\\pm$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\begin{bmatrix} -\\alpha & -\\beta \\\\ \\beta & \\alpha \\end{bmatrix} \\begin{bmatrix} u^+ \\\\ u^- \\end{bmatrix} + \\begin{bmatrix} -\\tilde{Q}^+ \\\\ \\tilde{Q}^- \\end{bmatrix} + \\delta_{0m}\\begin{bmatrix} - \\tilde{\\mathrm{S}}^+ \\\\ \\tilde{\\mathrm{S}}^- \\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix} -M^{-1}\\left(D^{+} W - I\\right) & -M^{-1} D^{-} W \\\\ M^{-1} D^{-} W & M^{-1}\\left(D^{+} W - I\\right) \\end{bmatrix} \\begin{bmatrix} u^+ \\\\ u^- \\end{bmatrix} + \\begin{bmatrix} -M^{-1} Q^+ \\\\ M^{-1} Q^- \\end{bmatrix} + \\delta_{0m}\\begin{bmatrix} -M^{-1} \\mathrm{S}^+ \\\\ M^{-1} \\mathrm{S}^- \\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix} -M^{-1} & \\\\ & M^{-1} \\end{bmatrix} \\left( \\begin{bmatrix} D^{+} W - I & D^{-} W \\\\ D^{-} W & D^{+} W - I \\end{bmatrix} \\begin{bmatrix} u^+ \\\\ u^- \\end{bmatrix} + \\begin{bmatrix} Q^+ \\\\ Q^- \\end{bmatrix} + \\delta_{0m}\\begin{bmatrix} \\mathrm{S}^+ \\\\ \\mathrm{S}^- \\end{bmatrix} \\right) \\\\\n",
    "&= \\begin{bmatrix} -\\mu_0^{-1} & & & & & \\\\ & \\ddots & & & & \\\\ & & -\\mu_N^{-1} & & & \\\\ & & & \\mu_0^{-1} & & \\\\ & & & & \\ddots & \\\\ & & & & & \\mu_N^{-1} \\end{bmatrix} \\left( \\left( \\begin{bmatrix} D^{+} W & D^{-} W \\\\ D^{-} W & D^{+} W \\end{bmatrix} - I \\right) \\begin{bmatrix} u^+ \\\\ u^- \\end{bmatrix} + \\begin{bmatrix} Q^+ \\\\ Q^- \\end{bmatrix} + \\delta_{0m}\\begin{bmatrix} \\mathrm{S}^+ \\\\ \\mathrm{S}^- \\end{bmatrix} \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and so\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \\frac{\\mathrm{d}u^+}{\\mathrm{d}\\tau} \\\\ \\frac{\\mathrm{d}u^-}{\\mathrm{d}\\tau} \\end{bmatrix} = \\begin{bmatrix} \\mu_0^{-1} & & & & & \\\\ & \\ddots & & & & \\\\ & & \\mu_N^{-1} & & & \\\\ & & & -\\mu_0^{-1} & & \\\\ & & & & \\ddots & \\\\ & & & & & -\\mu_N^{-1} \\end{bmatrix} \\left(\\begin{bmatrix} u^+ \\\\ u^- \\end{bmatrix} - \\begin{bmatrix} D^{+} W & D^{-} W \\\\ D^{-} W & D^{+} W \\end{bmatrix} \\begin{bmatrix} u^+ \\\\ u^- \\end{bmatrix} - \\begin{bmatrix} Q^+ \\\\ Q^- \\end{bmatrix} - \\delta_{0m}\\begin{bmatrix} \\mathrm{S}^+ \\\\ \\mathrm{S}^- \\end{bmatrix} \\right)\n",
    "$$\n",
    "\n",
    "Multiply across by the $\\mu_i$ values to see that the system is consistent with the discretized equation for each Fourier mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e0124b",
   "metadata": {},
   "source": [
    "**Discretization of surface reflection terms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed4d3a3",
   "metadata": {},
   "source": [
    "We also use Gauss-Legendre quadrature to perform the discretization\n",
    "\n",
    "$$\\int_0^1 \\mathscr{D}^m(\\mu_i, -\\mu') u^m(\\tau_\\text{BoA}, -\\mu') \\mathrm{d}\\mu' \\approx \\sum_{j=1}^\\text{N} w_j\\mathscr{D}^m(\\mu_i, -\\mu_j) u^m(\\tau_\\text{BoA}, -\\mu_j)$$\n",
    "\n",
    "for $i = 1, \\dots, N$ and $0 < \\mu_i, \\mu_j \\leq 1$. We define\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathscr{D}[i, j] &=  \\mathscr{D}^m\\left(\\mu_i,-\\mu_j\\right) \\\\\n",
    "\\mathscr{X}[i] &= \\mathscr{X}^m\\left(\\mu_i\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "so that the discretized surface reflection can be re-expressed as the system \n",
    "\n",
    "$$\n",
    "\\mathscr{D}Wu^- + \\mathscr{X}\\exp\\left(-\\mu_{0}^{-1} \\tau\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7252846",
   "metadata": {},
   "source": [
    "### 3.4.1 How to choose computational parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f009ab80",
   "metadata": {},
   "source": [
    "Consider the integration\n",
    "\n",
    "$$\n",
    "\\int_{-1}^1 D^m\\left(\\mu, \\mu'\\right) u^m\\left(\\tau, \\mu'\\right) d \\mu'\n",
    "$$\n",
    "\n",
    "where $D^m$ is a polynomial in $\\mu'$ of order $\\text{NQuad} - 1$. If we assume that $u^m$ is well-approximated by a polynomial in $\\mu'$, then Gauss-Legendre quadrature should be optimal, though Clenshaw-Curtis quadrature is a worthy consideration. The main consideration when choosing the number of streams is the mostly empirical observation that $\\text{NQuad} < \\text{NLeg}$ results in large errors, see [[1]](#cite-Cha1960) and [[4]](#cite-STWLE2000). The reasons for this are not well understood, though we note that Gauss-Legendre quadrature with $N$ nodes is able to integrate polynomials of up to order $2N-1$ exactly, and that is the order of $D^m$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4119da",
   "metadata": {},
   "source": [
    "Choosing $\\text{NQuad} < \\text{NLeg}$ incurs large errors. What if a user chooses $\\text{NQuad} > \\text{NLeg}$ instead? Increasing $\\text{NQuad}$ does generally result in better accuracy, though monotone convergence is not guaranteed. It, however, also incurs large computational costs. Therefore $\\text{NLeg} = \\text{NQuad}$ is the best compromise in most cases, it is also the only option in Stamnes' DISORT [[3]](#cite-Sta1999)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df1c23c",
   "metadata": {},
   "source": [
    "An added complication is that realistic phase functions are often so complicated that they require many Legendre coefficients to be accurately represented. If $\\text{NLeg}$ is large, having even $\\text{NQuad} = \\text{NLeg}$ will be expensive. An increase in $\\text{NQuad}$ increases computational costs in two ways. First, the size of the systems we need to solve will be larger. Second, there are more Fourier modes $u^m$ which we can solve for to construct the full solution $u$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfd487e",
   "metadata": {},
   "source": [
    "When $\\text{NLeg}$ is large, large systems are needed for accuracy. We, however, do not need to solve for all $\\text{NLeg}$ Fourier modes to accurately construct the full solution. Therefore, we introduced the parameter $\\text{NLoops}$ for the user to choose the number of Fourier modes to use in constructing the full solution. It is so named because our solver is looped $\\text{NLoops}$ times for each atmospheric layer. In PyDISORT $\\text{NLoops}$ must be guessed or tuned. In Stamnes' DISORT, $\\text{NLoops}$ is chosen adaptively, though a little crudely, using the Cauchy convergence criterion, see [[4, section 3.7]](#cite-STWLE2000). In PyDISORT, that same Cauchy / Fourier convergence evaluation will be printed for the last $(m = \\text{NLoops})$ Fourier term if the optional flag `return_Fourier_error = True` is passed to $u$, see section [3.7](#3.7-The-full-solution) in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1acd6e5",
   "metadata": {},
   "source": [
    "### 3.4.2 Assembly of system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924c8cc3",
   "metadata": {},
   "source": [
    "**Until section [3.7](#3.7-The-full-solution), we will need to loop the following for Fourier mode index** $m = 0$ **to** $m = \\text{NLoops}$.\n",
    "\n",
    "This loop is relatively easy to parallelize and we will use the *parfor* package to do so. We eventually want to switch to the *Ray* package but as of release it is not compatible with the latest version of Python. Parallelization comes with massive initialization costs, however, so if $\\text{NLoops}$ is small it may be faster to disable parallelization with the flag `parfor_Fourier=False`, and this is the default. There is great room for improvement in the way we parallelized the for-loops. Parallelization should be considered an experiment feature at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52416f32",
   "metadata": {},
   "source": [
    "We will only demonstrate one loop (one value of $m$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d81156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d78f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We transform the Legendre coefficients into WEIGHTED Legendre coefficients\n",
    "weighted_Leg_coeffs = (2 * np.arange(NLeg) + 1) * Leg_coeffs_l\n",
    "weighted_Leg_coeffs_BDRF = (2 * np.arange(NLeg) + 1) * Leg_coeffs_BDRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e0e6a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from parfor import parfor\\n@parfor(range(NLeg), bar=False)\\ndef fun(m):\\n    ells = np.arange(m, NLeg)\\n    degree_tile = np.tile(ells, (N, 1)).T\\n    fac = np.prod(ells[:, None] + np.arange(-m + 1, m + 1)[None, :], axis=-1)\\n    asso_weights = np.divide(1, fac, out=np.zeros(NLeg - m), where=(fac != 0))\\n    signs = np.empty(NLeg - m)\\n    signs[::2] = 1\\n    signs[1::2] = -1\\n    if m == 0:\\n        prefactor = omegal * I0 / (4 * pi)\\n    else:\\n        prefactor = omegal * I0 / (2 * pi)\\n\\n    asso_leg_term_pos = sc.special.lpmv(m, degree_tile, mu_arr_pos)\\n    asso_leg_term_neg = asso_leg_term_pos * signs[:, None]\\n    asso_leg_term_mu0 = sc.special.lpmv(m, ells, -mu0)\\n    weighted_asso_Leg_coeffs = weighted_Leg_coeffs[ells] * asso_weights\\n\\n    D_temp = weighted_asso_Leg_coeffs[None, :] * asso_leg_term_pos.T\\n    D_pos = (omegal / 2) * D_temp @ asso_leg_term_pos\\n    D_neg = (omegal / 2) * D_temp @ asso_leg_term_neg\\n\\n    X_temp = prefactor * weighted_asso_Leg_coeffs * asso_leg_term_mu0\\n    X_pos = X_temp @ asso_leg_term_pos\\n    X_neg = X_temp @ asso_leg_term_neg\\n    return D_pos, D_neg, X_pos, X_neg'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from parfor import parfor\n",
    "@parfor(range(NLeg), bar=False)\n",
    "def fun(m):\n",
    "    ells = np.arange(m, NLeg)\n",
    "    degree_tile = np.tile(ells, (N, 1)).T\n",
    "    fac = np.prod(ells[:, None] + np.arange(-m + 1, m + 1)[None, :], axis=-1)\n",
    "    asso_weights = np.divide(1, fac, out=np.zeros(NLeg - m), where=(fac != 0))\n",
    "    signs = np.empty(NLeg - m)\n",
    "    signs[::2] = 1\n",
    "    signs[1::2] = -1\n",
    "    if m == 0:\n",
    "        prefactor = omegal * I0 / (4 * pi)\n",
    "    else:\n",
    "        prefactor = omegal * I0 / (2 * pi)\n",
    "\n",
    "    asso_leg_term_pos = sc.special.lpmv(m, degree_tile, mu_arr_pos)\n",
    "    asso_leg_term_neg = asso_leg_term_pos * signs[:, None]\n",
    "    asso_leg_term_mu0 = sc.special.lpmv(m, ells, -mu0)\n",
    "    weighted_asso_Leg_coeffs = weighted_Leg_coeffs[ells] * asso_weights\n",
    "\n",
    "    D_temp = weighted_asso_Leg_coeffs[None, :] * asso_leg_term_pos.T\n",
    "    D_pos = (omegal / 2) * D_temp @ asso_leg_term_pos\n",
    "    D_neg = (omegal / 2) * D_temp @ asso_leg_term_neg\n",
    "\n",
    "    X_temp = prefactor * weighted_asso_Leg_coeffs * asso_leg_term_mu0\n",
    "    X_pos = X_temp @ asso_leg_term_pos\n",
    "    X_neg = X_temp @ asso_leg_term_neg\n",
    "    return D_pos, D_neg, X_pos, X_neg'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e414a5a",
   "metadata": {},
   "source": [
    "Generate $D$ and $X$ (phase function terms):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d12c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ells = np.arange(m, NLeg)\n",
    "degree_tile = np.tile(ells, (N, 1)).T\n",
    "fac = np.prod(ells[:, None] + np.arange(-m + 1, m + 1)[None, :], axis=-1)\n",
    "asso_weights = np.divide(1, fac, out=np.zeros(NLeg - m), where=(fac != 0))\n",
    "signs = np.empty(NLeg - m)\n",
    "signs[::2] = 1\n",
    "signs[1::2] = -1\n",
    "if m == 0:\n",
    "    prefactor = omegal * I0 / (4 * pi)\n",
    "else:\n",
    "    prefactor = omegal * I0 / (2 * pi)\n",
    "\n",
    "asso_leg_term_pos = sc.special.lpmv(m, degree_tile, mu_arr_pos)\n",
    "asso_leg_term_neg = asso_leg_term_pos * signs[:, None]\n",
    "asso_leg_term_mu0 = sc.special.lpmv(m, ells, -mu0)\n",
    "weighted_asso_Leg_coeffs = weighted_Leg_coeffs[ells] * asso_weights\n",
    "\n",
    "D_temp = weighted_asso_Leg_coeffs[None, :] * asso_leg_term_pos.T\n",
    "D_pos = (omegal / 2) * D_temp @ asso_leg_term_pos\n",
    "D_neg = (omegal / 2) * D_temp @ asso_leg_term_neg\n",
    "\n",
    "X_temp = prefactor * weighted_asso_Leg_coeffs * asso_leg_term_mu0\n",
    "X_pos = X_temp @ asso_leg_term_pos\n",
    "X_neg = X_temp @ asso_leg_term_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b46c7e",
   "metadata": {},
   "source": [
    "Generate $\\mathscr{D}$ and $\\mathscr{X}$ (BDRF terms):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f6822d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if m < NBDRF:\n",
    "    if m == 0:\n",
    "        prefactor = mu0 * I0 / pi\n",
    "    else:\n",
    "        prefactor = mu0 * I0 * 2 / pi\n",
    "\n",
    "    asso_leg_term_pos = asso_leg_term_pos[:NBDRF, :]\n",
    "    asso_leg_term_neg = asso_leg_term_neg[:NBDRF, :]\n",
    "    asso_leg_term_mu0 = asso_leg_term_mu0[:NBDRF]\n",
    "    weighted_asso_Leg_coeffs = (\n",
    "        weighted_Leg_coeffs_BDRF[ells[: (NBDRF - m)]] * asso_weights[: (NBDRF - m)]\n",
    "    )\n",
    "\n",
    "    mathscr_D_temp = (\n",
    "        weighted_asso_Leg_coeffs[None, :] * asso_leg_term_pos.T * mu_arr_pos[:, None]\n",
    "    )\n",
    "    mathscr_D_neg = 2 * mathscr_D_temp @ asso_leg_term_neg\n",
    "\n",
    "    mathscr_X_temp = prefactor * weighted_asso_Leg_coeffs * asso_leg_term_mu0\n",
    "    mathscr_X_pos = mathscr_X_temp @ asso_leg_term_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e92901b",
   "metadata": {},
   "source": [
    "Assemble the coefficient matrix and additional terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88ec4674",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_inv = 1 / mu_arr_pos\n",
    "W = weights_mu[None, :]\n",
    "alpha = M_inv[:, None] * (D_pos * W - np.eye(N))\n",
    "beta = M_inv[:, None] * D_neg * W\n",
    "A = np.vstack((np.hstack((-alpha, -beta)), np.hstack((beta, alpha))))\n",
    "\n",
    "weighted_mathscr_D_neg = mathscr_D_neg * W\n",
    "X_tilde = np.concatenate((-M_inv * X_pos, M_inv * X_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc6e0a3",
   "metadata": {},
   "source": [
    "## 3.5 Diagonalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bcb1df",
   "metadata": {},
   "source": [
    "We will for most part omit the Fourier mode index $m$ in this section and in section [3.6](#3.6-General-solution-for-each-Fourier-mode). The system of ODEs for each Fourier mode is\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \\frac{\\mathrm{d}u^+}{\\mathrm{d}\\tau} \\\\ \\frac{\\mathrm{d}u^-}{\\mathrm{d}\\tau} \\end{bmatrix} = \\begin{bmatrix} -\\alpha & -\\beta \\\\ \\beta & \\alpha \\end{bmatrix} \\begin{bmatrix} u^+ \\\\ u^- \\end{bmatrix} + \\begin{bmatrix} -\\tilde{Q}^+ \\\\ \\tilde{Q}^- \\end{bmatrix} + \\delta_{0m} \\begin{bmatrix} -\\tilde{\\mathrm{S}}^+ \\\\ \\tilde{\\mathrm{S}}^- \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e9f00b",
   "metadata": {},
   "source": [
    "We first want to diagonalize the coefficient matrix, which we denote $A$, such that $A = G\\Sigma G^{-1}$. We will use the reduction of order method in [[2]](#cite-STWJ1988), but with minor sign differences, to solve for the eigenpairs of the coefficient matrix from the eigenequation\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} -\\alpha & -\\beta \\\\ \\beta & \\alpha \\end{bmatrix} \\begin{bmatrix} G^+ \\\\ G^- \\end{bmatrix} = k \\begin{bmatrix} G^+ \\\\ G^- \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a3ccdb",
   "metadata": {},
   "source": [
    "We perform the multiplication on the LHS to get the equations\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\alpha G^{+} + \\beta G^{-} & = -k G^{+} \\\\\n",
    "\\beta G^{+} + \\alpha G^{-} & = k G^{-}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Next, add then subtract the equations to get\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& (\\alpha + \\beta)\\left(G^{-} + G^{+}\\right) = k\\left(G^{-} - G^{+}\\right) \\\\\n",
    "& (\\alpha - \\beta)\\left(G^{-} - G^{+}\\right) = k\\left(G^{-} + G^{+}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Finally, we combine the equations to get\n",
    "\n",
    "$$\n",
    "(\\alpha - \\beta) (\\alpha + \\beta) \\left(G^- + G^+\\right) = k^2 \\left(G^- + G^+\\right)\n",
    "$$\n",
    "\n",
    "from which we can solve for the eigenvalues $k$ which are always real, and, using the previous equations, for $G^+$ and $G^-$. Consequently, we can construct the eigenvector matrix\n",
    "\n",
    "$$G = \\begin{bmatrix} G^+ \\\\ G^- \\end{bmatrix}$$\n",
    "\n",
    "We denote the vector of eigenvalues by $K$ with entries arranged negative then positive, from from largest to smallest magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30299143",
   "metadata": {},
   "source": [
    "**Computation of** $G^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1fb8b2",
   "metadata": {},
   "source": [
    "In order to complete the diagonalization, we need to determine $G^{-1}$. We can directly compute the inverse, but it is faster to use the reduction of order method on the transpose of the coefficient matrix. Define \n",
    "\n",
    "$$H = \\begin{bmatrix} H^+ \\\\ H^- \\end{bmatrix}$$ \n",
    "\n",
    "such that $H^T G = \\text{Diag}$ for some diagonal matrix that is the result of the eigenvectors being unnormalized. We have the eigenequation\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} -\\alpha^T & \\beta^T \\\\ -\\beta^T & \\alpha^T \\end{bmatrix} \\begin{bmatrix} H^+ \\\\ H^- \\end{bmatrix} = k \\begin{bmatrix} H^+ \\\\ H^- \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "for the same eigenvalues $k$ as above, up to a constant factor. We again perform the multiplication on the LHS to get the equations\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "-\\alpha^T H^{+} +\\beta^T H^{-} & = k H^{+} \\\\\n",
    "-\\beta^T H^{+} +\\alpha^T H^{-} & = k H^{-}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Next, add then subtract the equations to get\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& (\\alpha + \\beta)^T \\left(H^{-} - H^{+}\\right) = k\\left(H^{-} + H^{+}\\right) \\\\\n",
    "& (\\alpha - \\beta)^T \\left(H^{-} + H^{+}\\right) = k\\left(H^{-} - H^{+}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Finally, we combine the equations to get\n",
    "\n",
    "$$\n",
    "(\\alpha + \\beta)^T (\\alpha - \\beta)^T \\left(H^- + H^+\\right) = k^2 \\left(H^- + H^+\\right)\n",
    "$$\n",
    "\n",
    "from which we can solve for $H^+$ and $H^-$, trivially solve for $\\left(H^T G\\right)^{-1}$ which is diagonal, then solve for $G^{-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "79a3aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_squared, eigenvecs_GpG = np.linalg.eig((alpha - beta) @ (alpha + beta))\n",
    "# Eigenvalues arranged negative then positive, from largest to smallest magnitude\n",
    "K = np.concatenate((-np.sqrt(K_squared), np.sqrt(K_squared)))\n",
    "eigenvecs_GpG = np.hstack((eigenvecs_GpG, eigenvecs_GpG))\n",
    "eigenvecs_GmG = (alpha + beta) @ eigenvecs_GpG / K\n",
    "\n",
    "# Eigenvectors\n",
    "G_pos = (eigenvecs_GpG - eigenvecs_GmG) / 2\n",
    "G_neg = (eigenvecs_GpG + eigenvecs_GmG) / 2\n",
    "G = np.vstack((G_pos, G_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a1f8a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_squared, eigenvecs_HpH = np.linalg.eig(((alpha - beta) @ (alpha + beta)).T)\n",
    "eigenvecs_HpH = np.hstack((eigenvecs_HpH, eigenvecs_HpH))\n",
    "eigenvecs_HmH = (alpha - beta).T @ eigenvecs_HpH / K\n",
    "\n",
    "# Eigenvectors\n",
    "H_pos = (eigenvecs_HpH - eigenvecs_HmH) / 2\n",
    "H_neg = (eigenvecs_HpH + eigenvecs_HmH) / 2\n",
    "G_inv = np.vstack((H_pos, H_neg)).T\n",
    "G_inv = G_inv / np.diag(G_inv @ G)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2ffd0",
   "metadata": {},
   "source": [
    "**Verification of diagonalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32f6cd1e",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed all tests\n"
     ]
    }
   ],
   "source": [
    "assert np.allclose(G * K[None, :] @ G_inv, A)\n",
    "\n",
    "print(\"Passed all tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dbd535",
   "metadata": {},
   "source": [
    "## 3.6 General solution for each Fourier mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e92478e",
   "metadata": {},
   "source": [
    "For each Fourier mode $m$, the general solution is\n",
    "\n",
    "$$\n",
    "u^m = v^m + \\mathscr{v}^m + \\delta_{0m} \\mathscr{v}_s^m\n",
    "$$\n",
    "\n",
    "where $v$ denotes the homogeneous solution and $\\mathscr{v}, \\mathscr{v}_s$ denote particular solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39932af3",
   "metadata": {},
   "source": [
    "Define \n",
    "\n",
    "$$\n",
    "\\tilde{Q} = \\begin{bmatrix} -\\tilde{Q}^+ \\\\ \\tilde{Q}^- \\end{bmatrix}, \\quad \\tilde{\\mathrm{S}} = \\begin{bmatrix} -\\tilde{\\mathrm{S}}^+ \\\\ \\tilde{\\mathrm{S}}^- \\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "and $\\tilde{X}$ to be such that $\\tilde{X}\\exp\\left(-\\mu_0^{-1} \\tau\\right) = \\tilde{Q}$. Recall that $K$ is the vector of eigenvalues and $G$ is the matrix of eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da9edc1",
   "metadata": {},
   "source": [
    "### 3.6.1 The particular solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7277dd5a",
   "metadata": {},
   "source": [
    "**Sunbeam source**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09957a31",
   "metadata": {},
   "source": [
    "The particular solution $\\mathscr{v}$ for the sunbeam source satisfies\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d}\\mathscr{v}}{\\mathrm{d}\\tau} = A \\mathscr{v} + \\tilde{Q}(\\tau)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} -\\alpha & -\\beta \\\\ \\beta & \\alpha \\end{bmatrix}, \\quad \\tilde{Q}(\\tau) = \\tilde{X} \\exp\\left(-\\mu_0^{-1} \\tau\\right)\n",
    "$$\n",
    "\n",
    "Assume the ansatz\n",
    "\n",
    "$$\n",
    "\\mathscr{v}(\\tau) = B\\exp\\left(-\\mu_0^{-1} \\tau\\right)\n",
    "$$\n",
    "\n",
    "for $B$ to be determined. Substitution into the full equation gives\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&-\\mu_0^{-1} B\\exp\\left(-\\mu_0^{-1} \\tau\\right) = AB\\exp\\left(-\\mu_0^{-1} \\tau\\right) + \\tilde{X}\\exp\\left(-\\mu_0^{-1} \\tau\\right) \\\\\n",
    "&\\implies -\\mu_0^{-1} B = AB + \\tilde{X} \\\\\n",
    "&\\implies -\\left(\\mu_0^{-1} I + A\\right)B = \\tilde{X} \\\\\n",
    "&\\implies B = -G\\left(\\mu_0^{-1} I + \\Sigma\\right)^{-1}G^{-1} \\tilde{X}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where we used the diagonalization $A = G\\Sigma G^{-1}$ in the last step and the inverse of the diagonal matrix $\\mu_0^{-1} I + \\Sigma$ is trivial to compute. Note the numerical instability if $\\mu_0^{-1}$ happens to be close to an eigenvalue. One solution to this is dithering, see [[9]](#cite-LSJLTWS2015), but we would want the dithering to be transparent to the user. PyDISORT currently does not have a way to deal with this unlikely possibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d1e398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = -G / (1 / mu0 + K)[None, :] @ G_inv @ X_tilde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f617f40d",
   "metadata": {},
   "source": [
    "**The other internal sources**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a88c08",
   "metadata": {},
   "source": [
    "The solution to the ODE\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d}\\mathscr{v}_s}{\\mathrm{d}\\tau} = A\\mathscr{v}_s + \\tilde{\\mathrm{S}}(\\tau)\n",
    "$$\n",
    "\n",
    "for $\\tau \\in [0, \\tau_\\text{BoA}]$ is \n",
    "\n",
    "$$ \n",
    "\\mathscr{v}_s(\\tau) = \\int_{\\tau_\\text{BoA}}^{\\tau} \\exp(A(\\tau - t)) \\tilde{\\mathrm{S}}(t) \\mathrm{d} t\n",
    "$$\n",
    "\n",
    "disregarding boundary conditions. It is expensive to compute the exponential of non-diagonal matrices, and so we use the diagonalization $A = G\\Sigma G^{-1}$ to get\n",
    "\n",
    "$$ \n",
    "\\mathscr{v}_s(\\tau) = \\int_{\\tau_\\text{BoA}}^{\\tau} G \\exp(\\Sigma(\\tau - t)) G^{-1}\\tilde{\\mathrm{S}}(t) \\mathrm{d} t\n",
    "$$\n",
    "\n",
    "which is the function inputed into PyDISORT, see section [1.7](#1.7-OPTIONAL:-Choose-other-internal-sources). In theory, internal source $s$ with $\\phi$ variation can be accomodated through Fourier cosine expansion but the extra flexibility does not seem worth the overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db54cc",
   "metadata": {},
   "source": [
    "**Verification of particular solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c333cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntau = 100\n",
    "\n",
    "tau_test_arr = np.random.random(Ntau) * taul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a24990",
   "metadata": {},
   "source": [
    "We use the *autograd* package to perform the differentiation with respect to $\\tau$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "db1069c3",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max pointwise error ratio =  1.9179856274563387e-13\n"
     ]
    }
   ],
   "source": [
    "# The particular solution to the sunbeam source\n",
    "mathscr_v = lambda tau: B[:, None] * np.exp(-tau[None, :] / mu0)\n",
    "LHS = np.sum(ag.jacobian(mathscr_v)(tau_test_arr), axis=-1)\n",
    "RHS = A @ mathscr_v(tau_test_arr) + X_tilde[:, None] * np.exp(\n",
    "    -tau_test_arr[None, :] / mu0\n",
    ")\n",
    "\n",
    "print(\"Max pointwise error ratio = \", np.max(np.abs((RHS - LHS) / LHS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c3ae9085",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "Ntau = int(1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "773eea1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max pointwise error ratio =  0.008000170054711514\n"
     ]
    }
   ],
   "source": [
    "tau_test_arr, first_deriv = PyDISORT.subroutines.generate_FD_mat(Ntau, 0, taul)\n",
    "\n",
    "# The particular solution to the other internal sources\n",
    "mathscr_vs_tau = lambda tau: mathscr_vs(tau, NQuad, G, K, G_inv)\n",
    "\n",
    "LHS = mathscr_vs_tau(tau_test_arr) @ first_deriv.T\n",
    "RHS = A @ mathscr_vs_tau(tau_test_arr) + S_tilde(tau_test_arr)\n",
    "\n",
    "print(\"Max pointwise error ratio = \", np.max(np.abs((RHS - LHS) / LHS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3245b434",
   "metadata": {},
   "source": [
    "### 3.6.2 The homogeneous solution for one layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6071b2",
   "metadata": {},
   "source": [
    "The homogeneous solution $v$ can be split into\n",
    "\n",
    "$$\n",
    "v = \\begin{bmatrix} v^+ \\\\ v^- \\end{bmatrix}, \\quad  v^\\pm(\\tau) = G^\\pm E(\\tau) \\xi\n",
    "$$\n",
    "\n",
    "where the $i$th diagonal entry of diagonal matrix $E(\\tau)$ is $\\exp(k_i\\tau)$ and the coefficient vector $\\xi$ is to be determined from the boundary conditions. We assume, for now, that there is only one atmospheric layer with $\\tau \\in [\\tau_{l-1}, \\tau_{l}]$. We have the BCs \n",
    "\n",
    "$$\n",
    "u^-(\\tau_{l-1}) = b^-_m, \\quad u^+\\left( \\tau_{l} \\right) = b^+_m + \\text{surface reflection}\n",
    "$$\n",
    "\n",
    "where $b^\\pm_m$ is the $m$th column of matrix $b^\\pm$. The surface reflection is given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\int_0^1 \\mathscr{D}^m(\\mu, -\\mu') u^m(\\tau_\\text{BoA}, -\\mu') \\mathrm{d}\\mu' + \\mathscr{X}^m(\\mu)\\exp\\left(-\\mu_{0}^{-1} \\tau_\\text{BoA}\\right) \\\\\n",
    "&\\approx \\mathscr{D}Wu^- + \\mathscr{X}\\exp\\left(-\\mu_{0}^{-1} \\tau\\right) \\\\\n",
    "&= \\mathscr{D}W\\left(v^-(\\tau_l) + B^-\\exp\\left(-\\mu_0^{-1} \\tau_l\\right) + \\delta_{0m} \\mathscr{v}_s(\\tau_l)\\right) + \\mathscr{X}\\exp\\left(-\\mu_{0}^{-1} \\tau_l\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8007121",
   "metadata": {},
   "source": [
    "By the principle of superposition, and adding the atmospheric layer index $l$, we have that\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v^-(\\tau_{l-1}) &= b^-_m - B_l^-\\exp\\left(-\\mu_0^{-1} \\tau_{l-1}\\right) - \\delta_{0m}\\mathscr{v}_s(\\tau_{l-1}) \\\\ \n",
    "v^+\\left( \\tau_l \\right) - \\mathscr{D}Wv^-\\left( \\tau_l \\right) &= b^+_m + \\left(\\mathscr{X} + \\mathscr{D}WB_l^- - B_l^+\\right)\\exp\\left(-\\mu_0^{-1} \\tau_{l}\\right) + \\delta_{0m}(\\mathscr{D}W - I)\\mathscr{v}_s(\\tau_{l})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This produces the system\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} G_l^- E(\\tau_{l-1}) \\\\ \\left(G_l^+ - \\mathscr{D}WG_l^-\\right) E(\\tau_l) \\end{bmatrix} \\xi = \\begin{bmatrix} b^-_m - B_l^-\\exp\\left(-\\mu_0^{-1} \\tau_{l-1}\\right) - \\delta_{0m}\\mathscr{v}_s(\\tau_{l-1}) \\\\ b^+_m + \\left(\\mathscr{X} + \\mathscr{D}WB_l^- - B_l^+\\right)\\exp\\left(-\\mu_0^{-1} \\tau_{l}\\right) + \\delta_{0m}(\\mathscr{D}W - I)\\mathscr{v}_s(\\tau_{l})\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "which we can solve to determine $\\xi$. When there is only one atmospheric layer, $\\tau_{l-1} = 0$ and $\\tau_{l} = \\tau_0 = \\tau_\\text{BoA}$. In addition, $\\mathscr{v}_s(\\tau_\\text{BoA}) = 0$ by definition. Consequently, we once again omit index $l$ and simplify the system to\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} G^- \\\\ \\left(G^+ - \\mathscr{D}WG^-\\right) E(\\tau_0)  \\end{bmatrix} \\xi = \\begin{bmatrix} b^-_m - B^- - \\delta_{0m}\\mathscr{v}_s(0) \\\\ b^+_m + \\left(\\mathscr{X} + \\mathscr{D}WB^- - B^+\\right)\\exp\\left(-\\mu_0^{-1} \\tau_0\\right) \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee875d4",
   "metadata": {},
   "source": [
    "**Stamnes-Conklin's substitutions to address ill-conditioning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e8cc79",
   "metadata": {},
   "source": [
    "The problem with the above formulation is that for large $\\tau_0$, the terms in $E(\\tau_0)$ that correspond to positive eigenvalues will be large. This may result in overflow or at least an ill-conditioned system. A solution to this problem is given by Stamnes and Conklin in [[5]](#cite-SC1984). First, re-express the system as\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} G^{--} & G^{-+} \\\\ \\left(G^+ - \\mathscr{D}WG^-\\right)^- E^-(\\tau_0) & \\left(G^+ - \\mathscr{D}WG^-\\right)^+ E^+(\\tau_0) \\end{bmatrix} \\begin{bmatrix} \\xi^- \\\\ \\xi^+ \\end{bmatrix} = \\text{RHS}\n",
    "$$\n",
    "\n",
    "where the extra superscripts $+$ and $-$ correspond to only positive or only negative eigenvalues respectively. Recall that the eigenvalues are arranged negative then positive, e.g. $G^- = \\begin{bmatrix} G^{--} & G^{-+} \\end{bmatrix}$. Next, substitute $\\xi^- = C^-$ and $\\xi^+ = C^+ E^-(\\tau_0)$ to get\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} G^{--} & G^{-+} E^-(\\tau_0) \\\\ \\left(G^+ - \\mathscr{D}WG^-\\right)^- E^-(\\tau_0) &  \\left(G^+ - \\mathscr{D}WG^-\\right)^+ \\end{bmatrix} \\begin{bmatrix} C^- \\\\ C^+ \\end{bmatrix} = \\text{RHS}\n",
    "$$\n",
    "\n",
    "This reformulated system has no positive exponents so there is no risk of overflow. Furthermore, when $\\tau_0$ is large the matrix becomes approximately block diagonal and so it remains well-conditioned. We solve for $C$ and do not construct $\\xi$ because we will use the factor of $E^-(\\tau_0)$ to avoid positive exponents when we construct the general solution. This method is easily generalizable to multiple atmospheric layers, see section [5](#5.-Solving-for-multiple-atmospheric-layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c1399d5b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "El = np.exp(K * taul)\n",
    "B_pos, B_neg = B[:N], B[N:]\n",
    "if scalar_b_pos:\n",
    "    b_pos_m = np.full(N, b_pos)\n",
    "else:\n",
    "    b_pos_m = b_neg[:, m]\n",
    "if scalar_b_neg:\n",
    "    b_neg_m = np.full(N, b_neg)\n",
    "else:\n",
    "    b_neg_m = b_neg[:, m]\n",
    "if m < NBDRF:\n",
    "    BDRF_RHS_contribution = (\n",
    "        mathscr_X_pos + weighted_mathscr_D_neg @ B_neg - B_pos\n",
    "    ) * np.exp(-taul / mu0)\n",
    "else:\n",
    "    BDRF_RHS_contribution = 0\n",
    "\n",
    "RHS = np.concatenate(\n",
    "    (\n",
    "        b_neg_m - B_neg,\n",
    "        b_pos_m + BDRF_RHS_contribution,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4479af5",
   "metadata": {},
   "source": [
    "**Solving the system naively**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "acad8093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition number = 2.430322699402251e+157\n"
     ]
    }
   ],
   "source": [
    "if m < NBDRF:\n",
    "    BDRF_LHS_contribution = weighted_mathscr_D_neg @ G_neg\n",
    "else:\n",
    "    BDRF_RHS_contribution = 0\n",
    "\n",
    "LHS = np.vstack((G_neg, (G_pos - BDRF_LHS_contribution) * El))\n",
    "\n",
    "print(\"Condition number =\", np.linalg.cond(LHS))\n",
    "# The system is too ill-conditioned to be solved if the optical thickness is large\n",
    "# xi = np.linalg.solve(LHS, RHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d9db1b",
   "metadata": {},
   "source": [
    "**Solving the system with Stamnes-Conklin's substitutions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b4596ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition number = 1.6547492218737911\n"
     ]
    }
   ],
   "source": [
    "B_pos, B_neg = B[:N], B[N:]\n",
    "El_neg = np.exp(K[:N] * taul)\n",
    "\n",
    "# LHS is a re-arranged COPY of matrix G\n",
    "LHS = np.vstack((G_neg, G_pos - weighted_mathscr_D_neg @ G_neg))\n",
    "LHS[:N, N:] *= El_neg[None, :]\n",
    "LHS[N:, :N] *= El_neg[None, :]\n",
    "\n",
    "print(\"Condition number =\", np.linalg.cond(LHS))\n",
    "C = np.linalg.solve(LHS, RHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cd5cc8",
   "metadata": {},
   "source": [
    "Finally, we have the general solution for one Fourier mode $m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e28b122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The general solution for one Fourier mode\n",
    "\n",
    "# Note that we use the factor of E^-(\\tau_0) attached to the coefficients (see Section 3.6.2)\n",
    "# to avoid positive exponents when we construct the general solution for one Fourier mode\n",
    "def um(tau):\n",
    "    tau = np.atleast_1d(tau)\n",
    "    exponent = np.vstack(\n",
    "        (\n",
    "            K[:N, None] * tau[None, :],\n",
    "            K[N:, None] * (tau - taul)[None, :],\n",
    "        )\n",
    "    )\n",
    "    return np.squeeze(\n",
    "        (G * C[None, :]) @ np.exp(exponent) + B[:, None] * np.exp(-tau[None, :] / mu0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6093190e",
   "metadata": {},
   "source": [
    "### 3.6.3 Verification of the general solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f9a98b",
   "metadata": {},
   "source": [
    "**Does the general solution satisfy the BCs?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5db8eb24",
   "metadata": {
    "code_folding": [],
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed all tests\n"
     ]
    }
   ],
   "source": [
    "# At top of atmosphere\n",
    "assert np.allclose(um(0)[N:], b_neg_m)\n",
    "\n",
    "# At bottom of atmosphere\n",
    "if m < NBDRF:\n",
    "    surface_reflection = weighted_mathscr_D_neg @ um(taul)[N:] + mathscr_X_pos * np.exp(\n",
    "        -taul / mu0\n",
    "    )\n",
    "else:\n",
    "    surface_reflection = 0\n",
    "assert np.allclose(um(taul)[:N], b_pos_m + surface_reflection)\n",
    "\n",
    "print(\"Passed all tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4376cf8",
   "metadata": {},
   "source": [
    "**Does the general solution satisfy the Fourier mode integro-differential equation?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69dd124",
   "metadata": {},
   "source": [
    "Since we rely on quadrature to compute the integral in this verification, it will not reflect quadrature error. In other words, this verification checks whether the general solution satisfies the approximate integro-differential equation\n",
    "\n",
    "$$\n",
    "\\mu_i \\frac{d u^m(\\tau, \\mu_i)}{d \\tau}=u^m(\\tau, \\mu_i)-\\sum_{|j| = 1,\\,j \\neq 0}^N w_j D^m\\left(\\mu_i, \\mu_j\\right) u^m\\left(\\tau, \\mu_j\\right) - Q^m(\\tau, \\mu_i) - \\delta_{0m}s(\\tau, \\mu_i)\n",
    "$$\n",
    "\n",
    "rather than the true equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "28be111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntau = 100\n",
    "\n",
    "tau_test_arr = np.random.random(Ntau) * taul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ddbcdb01",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max pointwise error ratio =  1.1925877585600359e-11\n"
     ]
    }
   ],
   "source": [
    "D = np.hstack((np.vstack((D_pos, D_neg)), np.vstack((D_neg, D_pos))))\n",
    "LHS = mu_arr[:, None] * np.sum(ag.jacobian(um)(tau_test_arr), axis=-1)\n",
    "RHS = (\n",
    "    um(tau_test_arr)\n",
    "    - np.einsum(\"ij, jt, j -> it\", D, um(tau_test_arr), full_weights_mu, optimize=True)\n",
    "    - np.concatenate((X_pos, X_neg))[:, None] * np.exp(-tau_test_arr[None, :] / mu0)\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Max pointwise error ratio = \",\n",
    "    np.max(np.abs((RHS - LHS) / LHS)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7394658b",
   "metadata": {},
   "source": [
    "## 3.7 The full solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a49a51",
   "metadata": {},
   "source": [
    "The above must be repeated for $\\text{NLoops}$ Fourier modes. The full numerical solution given by PyDISORT is\n",
    "\n",
    "$$\n",
    "u(\\mu, \\tau, \\phi) \\approx \\sum_{m=0}^\\text{NLoops} u^m(\\mu_i,\\tau)\\cos\\left(m\\left(\\phi_0 - \\phi\\right)\\right)\n",
    "$$\n",
    "\n",
    "This solution is continuous and variable in $\\tau$ and $\\phi$ but discrete in $\\mu$ and fixed to the quadrature points $\\mu_i$. The function output is 3D and axes $0, 1, 2$ capture $\\mu, \\tau, \\phi$ variation respectively. The first half of the $\\mu$ indices correspond to $u^+$ (upward) and the second half to $u^-$ (downward).\n",
    "\n",
    "If we have a multi-layer atmosphere then we will have a \"full\" solution for each atmospheric layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee15761a",
   "metadata": {},
   "source": [
    "**IMPORTANT:** Reminder that PyDISORT will output the **diffuse intensity**. Add the intensity of the incident beam to get the total intensity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4f974b",
   "metadata": {},
   "source": [
    "**Interpolation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f00a70",
   "metadata": {},
   "source": [
    "If values of $u$ are desired at non-quadrature points, i.e. at $\\mu \\neq \\mu_i$, then interpolation is required. The simplest method is to use `np.interp` on the upward then downward $\\mu$ points to form two splines which together form the full interpolation. Performing the interpolation this way, using Legendre points, convergence is guaranteed as $N \\rightarrow \\infty$, see [[10, chapter 8]](#cite-Tre1996). \n",
    "\n",
    "There are, however, caveats. First, the true solution is in general discontinuous at $\\mu = 0$ so there is no accurate way to interpolate that point. Second, intensity values very near the sunbeam remain inaccurate despite NT corrections, so one may be concerned about including them in the interpolation. Finally, using a high-order polynomial for interpolation is expensive, unstable and sensitive to errors in the interpolation points. Thus, even though Runge phenomenon is not a concern, one may want to consider other methods of interpolation, or to not interpolate at all and just use the values at $\\mu_i$ especially when $\\text{NQuad}$ is large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d0b0d8",
   "metadata": {},
   "source": [
    "**Assessment of Fourier convergence**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f0a0ef",
   "metadata": {},
   "source": [
    "In Stamnes' DISORT, $\\text{NLoops}$ is chosen adaptively using the Cauchy convergence criterion, see [[4, section 3.7]](#cite-STWLE2000). In PyDISORT, that same Cauchy / Fourier convergence evaluation \n",
    "\n",
    "$$\n",
    "\\max _{\\mu_i, \\tau, \\phi} \\frac{\\left|u^\\text{NLoops}(\\tau, \\phi) \\cos \\left(\\text{NLoops}\\left(\\phi_0-\\phi\\right)\\right)\\right|}{\\left|u(\\tau, \\phi)\\right|}\n",
    "$$\n",
    "\n",
    "will be printed for the last Fourier term, for the $\\tau, \\phi$ values provided, if the optional flag `return_Fourier_error = True` is passed to $u$. The default is `return_Fourier_error = False`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada4fda8",
   "metadata": {},
   "source": [
    "### 3.7.1 Verification of full solution without corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "01be0afd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tauL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Call PyDISORT from package\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mPyDISORT\u001b[39;00m\n\u001b[0;32m      4\u001b[0m mu_arr, u \u001b[38;5;241m=\u001b[39m PyDISORT\u001b[38;5;241m.\u001b[39mpydisort(\n\u001b[0;32m      5\u001b[0m     b_pos, b_neg, \n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[0;32m      7\u001b[0m     NQuad, NLeg, NLoops, \n\u001b[0;32m      8\u001b[0m     Leg_coeffs_all, \n\u001b[1;32m----> 9\u001b[0m     \u001b[43mtauL\u001b[49m, omega, \n\u001b[0;32m     10\u001b[0m     mu0, phi0, I0\n\u001b[0;32m     11\u001b[0m )[:\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tauL' is not defined"
     ]
    }
   ],
   "source": [
    "# Call PyDISORT from package\n",
    "import PyDISORT\n",
    "\n",
    "mu_arr, u = PyDISORT.pydisort(\n",
    "    b_pos, b_neg, \n",
    "    False, \n",
    "    NQuad, NLeg, NLoops, \n",
    "    Leg_coeffs_all, \n",
    "    tauL, omega, \n",
    "    mu0, phi0, I0\n",
    ")[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320e873d",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# Number of phi grid points\n",
    "# This selection should ensure that the phi quadrature is at least as accurate as the mu quadrature\n",
    "Nphi = int((NQuad * pi) // 2) * 2 + 1\n",
    "phi_arr, full_weights_phi = PyDISORT.subroutines.Clenshaw_Curtis_quad(Nphi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a90a8c",
   "metadata": {},
   "source": [
    "**Does the full solution satisfy the BCs?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d6b2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At top of atmosphere\n",
    "assert np.allclose(\n",
    "    u(0, phi_arr)[N:, :], b_neg @ np.cos(np.arange(NLoops)[:, None] * (phi0 - phi_arr))\n",
    ")\n",
    "\n",
    "# At bottom of atmosphere\n",
    "assert np.allclose(\n",
    "    u(tauL[0], phi_arr)[:N, :],\n",
    "    b_pos @ np.cos(np.arange(NLoops)[:, None] * (phi0 - phi_arr)),\n",
    ")\n",
    "print(\"Passed all tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70540fcc",
   "metadata": {},
   "source": [
    "**Does the full solution satisfy the radiative transfer equation?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0d37b0",
   "metadata": {},
   "source": [
    "Similar to the verification in section [3.6.3](#3.6.3-Verification-of-the-general-solution), this verification will not reflect quadrature error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e9767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu_arr is arranged as it is for code efficiency and readability\n",
    "# For presentation purposes we re-arrange mu_arr from smallest to largest\n",
    "reorder_mu = np.argsort(mu_arr)\n",
    "\n",
    "full_weights_mu_RO = full_weights_mu[reorder_mu]\n",
    "mu_arr_RO = mu_arr[reorder_mu]\n",
    "\n",
    "MU_ARR, PHI_ARR = np.meshgrid(phi_arr, mu_arr_RO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce2835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "tau_pt = float(1e-2)  # Must be a float for auto-differentiation to work\n",
    "\n",
    "plot = u(tau_pt, phi_arr)[reorder_mu]\n",
    "\n",
    "fig = plt.figure(figsize=(9, 6))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.contourf(MU_ARR, PHI_ARR, plot, 200)\n",
    "ax.scatter(\n",
    "    phi0,\n",
    "    -mu0,\n",
    "    np.linspace(0, np.max(plot) * 1.1, 200),\n",
    "    marker=\".\",\n",
    "    color=\"red\",\n",
    "    label=\"Incident beam at $\\mu$ = \"\n",
    "    + str(-mu0)\n",
    "    + \", $\\phi$ = \"\n",
    "    + str(np.around(phi0, 3)),\n",
    ")\n",
    "ax.set_xlabel(r\"$\\phi$\")\n",
    "ax.set_ylabel(r\"$\\mu$\")\n",
    "ax.set_zlabel(\"Intensity\")\n",
    "plt.title(r\"Uncorrected intensities at $\\tau =$\" + str(np.around(tau_pt, 3)))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae559251",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "LHS = (\n",
    "    mu_arr_RO[:, None] * ag.jacobian(lambda tau: u(tau, phi_arr))(tau_pt)[reorder_mu, :]\n",
    ")\n",
    "RHS = (\n",
    "    u(tau_pt, phi_arr)[reorder_mu]\n",
    "    - (omega[0] / (4 * pi))\n",
    "    * np.einsum(\n",
    "        \"ijkl, kl, k, l -> ij\",\n",
    "        p_HG_muphi(mu_arr_RO, phi_arr, mu_arr_RO, phi_arr, g),\n",
    "        u(tau_pt, phi_arr)[reorder_mu],\n",
    "        full_weights_mu_RO,\n",
    "        full_weights_phi,\n",
    "        optimize=True,\n",
    "    )\n",
    "    - (omega[0] * I0 / (4 * pi))\n",
    "    * p_HG_muphi(mu_arr_RO, phi_arr, -mu0, phi0, g)\n",
    "    * np.exp(-tau_pt / mu0)\n",
    ")\n",
    "error = np.abs(RHS - LHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70023a70",
   "metadata": {
    "scrolled": true,
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"At tau = \" + str(tau_pt))\n",
    "print(\"Max pointwise error = \", np.max(error))\n",
    "# We use clip so as to not divide by quantities too close to 0\n",
    "print(\n",
    "    \"Max pointwise error ratio =\",\n",
    "    np.max(error / np.clip(np.abs(LHS), a_min=1e-3, a_max=None)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976d4f63",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "plot = np.log10(error)\n",
    "\n",
    "fig = plt.figure(figsize=(9, 6))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.contourf(MU_ARR, PHI_ARR, plot, 200)\n",
    "ax.scatter(\n",
    "    phi0,\n",
    "    -mu0,\n",
    "    np.linspace(np.min(plot), np.max(plot), 200) * 1.1,\n",
    "    marker=\".\",\n",
    "    color=\"red\",\n",
    "    label=\"Incident beam at $\\mu$ = \"\n",
    "    + str(-mu0)\n",
    "    + \", $\\phi$ = \"\n",
    "    + str(np.around(phi0, 3)),\n",
    ")\n",
    "ax.set_xlabel(r\"$\\phi$\")\n",
    "ax.set_ylabel(r\"$\\mu$\")\n",
    "ax.set_zlabel(\"Log10 of pointwise error\")\n",
    "plt.title(r\"Uncorrected pointwise errors at $\\tau =$\" + str(tau_pt))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0340bf",
   "metadata": {},
   "source": [
    "The following is the code for using finite difference instead of auto-differentiation to perform the analysis. Auto-differentiation does not work on the results from Stamnes' DISORT. Be careful of the fact that the finite difference method will itself introduce substantial errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21a6a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Use finite difference instead of auto-differentiation \n",
    "# FD is necessary to analyze the error for Stamnes' DISORT,\n",
    "# but the FD will itself incur large inaccuracies\n",
    "Ntau_deriv = int(tauL[0] * 1000) + 1  # Number of tau grid points for finite difference\n",
    "\n",
    "tau_arr_deriv = np.linspace(0, tauL[0], Ntau_deriv)\n",
    "h = tau_arr_deriv[1] - tau_arr_deriv[0]  # grid spacing\n",
    "\n",
    "# Construct 1st derivative matrix with 2nd order accuracy\n",
    "first_deriv = np.zeros((Ntau_deriv, Ntau_deriv))\n",
    "diagonal = np.ones(Ntau_deriv) / (2 * h)\n",
    "first_deriv += np.diag(diagonal[:-1], 1)\n",
    "first_deriv += np.diag(-diagonal[:-1], -1)\n",
    "first_deriv = first_deriv[1:-1, :].T # This is due to tau being indexed by columns instead of rows'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e6ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''LHS_Stamnes = mu_arr_RO[:, None, None] * np.einsum(\"itp, tj -> ijp\", uu, first_deriv)\n",
    "RHS_Stamnes = (\n",
    "    uu\n",
    "    - (omega[0] / (4 * pi))\n",
    "    * np.einsum(\n",
    "        \"ijkl, ktl, k, l -> itj\",\n",
    "        p_HG_muphi(mu_arr_RO, phi_arr, mu_arr_RO, phi_arr, g),\n",
    "        uu,\n",
    "        full_weights_mu_RO,\n",
    "        full_weights_phi,\n",
    "        optimize=True,\n",
    "    )\n",
    "    - (omega[0] * I0 / (4 * pi))\n",
    "    * p_HG_muphi(mu_arr_RO, phi_arr, -mu0, phi0, g)[:, None, :]\n",
    "    * np.exp(-tau_arr_deriv[None, :, None] / mu0)\n",
    ")[:, 1:-1, :]\n",
    "\n",
    "error_Stamnes = np.abs(RHS_Stamnes - LHS_Stamnes)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a772a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''max_error_index = np.unravel_index(error_Stamnes.argmax(), LHS_Stamnes.shape)\n",
    "\n",
    "print(\n",
    "    \"Global max pointwise error at\",\n",
    "    np.around(\n",
    "        (\n",
    "            mu_arr[max_error_index[0]],\n",
    "            tau_arr_deriv[max_error_index[1]],\n",
    "            phi_arr[max_error_index[2]],\n",
    "        ),\n",
    "        3,\n",
    "    ),\n",
    ")\n",
    "print(\"Max pointwise error = \", np.max(error_Stamnes))\n",
    "print()\n",
    "# We use clip so as to not divide by quantities too close to 0\n",
    "print(\n",
    "    \"Max pointwise error ratio = \",\n",
    "    np.max(error_Stamnes / np.clip(np.abs(LHS_Stamnes), a_min=1e-3, a_max=None)),\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c680d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''%matplotlib widget\n",
    "\n",
    "index = 10\n",
    "\n",
    "plot = uu[:, index, :]\n",
    "\n",
    "fig = plt.figure(figsize=(9, 6))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.contourf(MU_ARR, PHI_ARR, plot, 200)\n",
    "ax.scatter(\n",
    "    phi0,\n",
    "    -mu0,\n",
    "    np.linspace(0, np.max(plot), 200) * 1.1,\n",
    "    marker=\".\",\n",
    "    color=\"red\",\n",
    "    label=\"Incident beam at $\\mu$ = \"\n",
    "    + str(-mu0)\n",
    "    + \", $\\phi$ = \"\n",
    "    + str(np.around(phi0, 3)),\n",
    ")\n",
    "ax.set_xlabel(r\"$\\phi$\")\n",
    "ax.set_ylabel(r\"$\\mu$\")\n",
    "ax.set_zlabel(\"Intensity\")\n",
    "plt.title(\n",
    "    r\"Stamnes' DISORT intensities at $\\tau$ = \"\n",
    "    + str(np.around(tau_arr_deriv[index], 3))\n",
    ")\n",
    "plt.legend()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20f74d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''print(\"At tau = \" + str(tau_arr_deriv[index]))\n",
    "print(\"Max pointwise error = \", np.max(error_Stamnes[:, index, :]))\n",
    "# We use clip so as to not divide by quantities too close to 0\n",
    "print(\n",
    "    \"Max pointwise error ratio =\",\n",
    "    np.max(\n",
    "        error_Stamnes[:, 8, :]\n",
    "        / np.clip(np.abs(LHS_Stamnes[:, 8, :]), a_min=1e-3, a_max=None)\n",
    "    ),\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af39963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''%matplotlib widget\n",
    "\n",
    "plot = np.log10(error_Stamnes[:, index, :])\n",
    "\n",
    "fig = plt.figure(figsize=(9, 6))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.contourf(MU_ARR, PHI_ARR, plot, 200)\n",
    "ax.scatter(\n",
    "    phi0,\n",
    "    -mu0,\n",
    "    np.linspace(np.min(plot), np.max(plot), 200) * 1.1,\n",
    "    marker=\".\",\n",
    "    color=\"red\",\n",
    "    label=\"Incident beam at $\\mu$ = \"\n",
    "    + str(-mu0)\n",
    "    + \", $\\phi$ = \"\n",
    "    + str(np.around(phi0, 3)),\n",
    ")\n",
    "ax.set_xlabel(r\"$\\phi$\")\n",
    "ax.set_ylabel(r\"$\\mu$\")\n",
    "ax.set_zlabel(\"Pointwise error\")\n",
    "plt.title(\n",
    "    r\"Stamnes' DISORT Pointwise error at $\\tau$ = \"\n",
    "    + str(np.around(tau_arr_deriv[index], 3))\n",
    ")\n",
    "plt.legend()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86ecace",
   "metadata": {},
   "source": [
    "### 3.7.2 Nakajima-Tanaka intensity corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64918ae",
   "metadata": {},
   "source": [
    "This subsection summarizes the main points from [[11]](#cite-NT1988) and from *Appendix A* of [[4]](#cite-STWLE2000) but omits most of the mathematical explanation. Recall that $\\tau^*, \\omega^*, p^*$ denote $\\delta-M$ scaled parameters and $f$ is the scattering fraction into peak. Nakajima-Tanaka (NT) corrections are disabled by default, enable them using the flag `NT_cor = True`. Note that if $I_0 = 0$, $f = 0$ or only $\\text{NLeg}$ coefficients are supplied in `Leg_coeffs_all`, then NT corrections will also be disabled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11b6904",
   "metadata": {},
   "source": [
    "The $\\delta-M$ method allows for accurate flux computation, but intensity values remain inaccurate particularly at backscattering angles and around the incident beam. These inaccuracies are largely caused by truncation of the Legendre series of the phase function. Nakajima and Tanaka proposed intensity corrections to reduce these accuracies. This correction is applied to the intensity but not to the flux as the latter is already accurate. This unfortunately means that flux values calculated by integrating the corrected intensity will differ slightly from values given by the flux functions.\n",
    "\n",
    "Note that NT corrections ignore, or rather have not been derived for, the other internal sources $s$, see [[4, section 3.6.3]](#cite-STWLE2000)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06207f6c",
   "metadata": {},
   "source": [
    "**TMS correction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b1b162",
   "metadata": {},
   "source": [
    "For the first of two NT corrections, named *TMS*, we approximate $u_\\text{true} \\approx u_\\text{TMS} = u^* + \\left(\\tilde{u}_1^* - u_1^*\\right)$. We have that $u_1^*$ and $\\tilde{u}_1^*$ are true solutions to the single-scattering equations for\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu \\frac{\\partial \\tilde{u}_1^*(\\tau, \\mu, \\phi)}{\\partial \\tau} &= \\tilde{u}_1^*(\\tau, \\mu, \\phi) -\\frac{\\omega^* I_0}{(1 - f) 4 \\pi} p\\left(\\mu, \\phi ;-\\mu_{0}, \\phi_{0}\\right) \\exp\\left(-\\mu_{0}^{-1} \\tau^*\\right) \\\\\n",
    "\\mu \\frac{\\partial u_1^*(\\tau, \\mu, \\phi)}{\\partial \\tau} &= u_1^*(\\tau, \\mu, \\phi) -\\frac{\\omega^* I_0}{4 \\pi} p^*\\left(\\mu, \\phi ;-\\mu_{0}, \\phi_{0}\\right) \\exp\\left(-\\mu_{0}^{-1} \\tau^*\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "respectively. Note that by construction (section [1.3.1](#1.3.1-OPTIONAL:-Choose-delta-M-scaling)), $p^*$ has a finite number of (non-zero) Legendre coefficients. In contrast, $p$ is the original phase function and in general has infinite Legendre coefficients. Unlike the Henyey-Greenstein phase function, however, realistic phase functions derived from Mie equations generally only have series representations. This lack of a closed-form means that it is impossible to input the true $p$ into PyDISORT. Instead, we use a large (`NLeg_all`) number of Legendre coefficients to very accurately approximate $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4912b19f",
   "metadata": {},
   "source": [
    "By superposition, every correction term, i.e. every term but $u^*$, must satisfy homogeneous BCs. We can solve for $\\left(\\tilde{u}_1^* - u_1^*\\right)$ analytically. Let $\\tau \\in [\\tau_{l-1}, \\tau_l]$; we will need to apply the TMS correction to the solution for each atmospheric layer. We have at the quadrature nodes\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left(\\tilde{u}_1^* - u_1^*\\right)(\\tau^*, \\mu_i, \\phi) &= \\mathscr{B}_l(\\tau^*, \\mu_i, \\phi)\\left[\\exp\\left(-\\frac{\\tau^*}{\\mu_0}\\right) - \\exp\\left(\\frac{\\tau^* - \\tau^*_{l}}{\\mu_i} - \\frac{\\tau^*_{l}}{\\mu_0}\\right)\\right] \\\\\n",
    "\\left(\\tilde{u}_1^* - u_1^*\\right)(\\tau^*, -\\mu_i, \\phi) &= \\mathscr{B}_l(\\tau^*, \\mu_i, \\phi)\\left[\\exp\\left(-\\frac{\\tau^*}{\\mu_0}\\right) - \\exp\\left(\\frac{\\tau^*_{l-1} - \\tau^*}{\\mu_i} - \\frac{\\tau^*_{l-1}}{\\mu_0}\\right)\\right] \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The correction source terms give\n",
    "\n",
    "$$\n",
    "\\mathscr{B}_l(\\tau^*, \\mu, \\phi) = \\left(\\frac{\\mu_0}{\\mu_0 + \\mu}\\right) \\frac{\\omega^*_l I_0}{4 \\pi} \\left(\\frac{p_l}{1-f} - p^*_l\\right)\\left(\\mu, \\phi ;-\\mu_{0}, \\phi_{0}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc57bc49",
   "metadata": {},
   "source": [
    "**IMS correction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f68b9a",
   "metadata": {},
   "source": [
    "The TMS correction substantially reduces the truncation error everywhere except around the incident beam. We introduce a second NT correction named *IMS* to address the remaining errors of significant magnitude. We incorporate the IMS with the approximation $u_\\text{true} \\approx u_\\text{TMS} + u_\\text{IMS}$. It is impractical to determine $u_\\text{IMS}$ exactly and both [[4]](#cite-STWLE2000) and [[11]](#cite-NT1988) make many approximations, albeit different approximations, to determine $u_\\text{IMS}$. We follow [[4]](#cite-STWLE2000) and approximate the IMS correction as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "u_\\text{IMS}(\\tau, -\\mu, \\phi) \\approx \\frac{I_0}{4 \\pi} \\frac{\\left(\\bar{\\omega} \\bar{f}\\right)^2}{1-\\bar{\\omega} \\bar{f}}\\left[2 p_r\\left(-\\mu, \\phi ;-\\mu_0, \\phi_0\\right)-p_r^2\\left(-\\mu, \\phi ;-\\mu_0, \\phi_0\\right)\\right]\\chi\\left(\\tau,-\\mu,-\\mu_0',-\\mu_0'\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "for $\\mu > 0$, and $u_\\text{IMS}(\\tau, -\\mu, \\phi) \\approx 0$ for $\\mu \\leq 0$, i.e. the correction only applies to the downward intensities. We have that $p_r = p - p^*$ is the residual phase function from the $\\delta-M$ approximation which we previously approximated as a delta function (section [1.3.1](#1.3.1-OPTIONAL:-Choose-delta-M-scaling)) and\n",
    "\n",
    "$$\n",
    "p_r^2\\left(-\\mu, \\phi ;-\\mu_0, \\phi_0\\right) = \\frac{1}{4 \\pi} \\int_0^{2 \\pi} \\int_{-1}^1 p_r\\left(-\\mu, \\phi ; \\mu', \\phi'\\right) p_r\\left(\\mu', \\phi' ;-\\mu_0, \\phi_0\\right) \\mathrm{d} \\phi' \\mathrm{d} \\mu'\n",
    "$$\n",
    "\n",
    "Using the series expansion for the residual phase function and optical properties that are averaged over all atmospheric layers (explicit formulas given below), we get\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "u_\\text{IMS}\\approx \\frac{I_0}{4 \\pi} \\frac{\\left(\\bar{\\omega} \\bar{f}\\right)^2}{1-\\bar{\\omega} \\bar{f}}\\left[\\sum_{\\ell=0}^\\infty (2\\ell + 1) \\left(2\\bar{g}_\\ell - \\bar{g}_\\ell^2\\right)P_\\ell(\\cos\\gamma)\\right]\\chi\\left(\\tau, -\\mu, -\\mu_0', -\\mu_0'\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "As with $p$, we truncate the series at `NLeg_all` and assume the remaining terms are negligible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eb9bc7",
   "metadata": {},
   "source": [
    "We also have the function\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\chi\\left(\\tau, -\\mu, -\\mu', -\\mu^{\\prime \\prime}\\right) = \\frac{\\exp\\left({-\\tau \\big/ \\mu}\\right)}{\\mu \\mu'} \\int_0^\\tau \\exp\\left(\\frac{t}{\\mu} - \\frac{t}{\\mu'}\\right) \\left(\\int_0^t \\exp\\left(\\frac{t'}{\\mu'} - \\frac{t'}{\\mu^{\\prime \\prime}}\\right) \\mathrm{d} t' \\right)\\mathrm{d} t \\\\\n",
    "&\\implies \\chi\\left(\\tau, -\\mu, -\\mu_0', -\\mu_0'\\right) = \\frac{1}{\\mu\\mu'_0x}\\left[\\left(\\tau-\\frac{1}{x}\\right) \\exp\\left(-\\frac{\\tau}{\\mu_0'}\\right)+\\frac{\\exp\\left(-\\tau\\big/\\mu\\right)}{x}\\right], \\quad x = \\frac{1}{\\mu}-\\frac{1}{\\mu_0'}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which is further detailed in [[4]](#cite-STWLE2000). In order to prevent singularities in the term $x$, we do not allow $\\mu_0$ to coincide with a quadrature angle, see section [1.4](#1.4-Choose-incident-beam-parameters). Finally, we have from [[4]](#cite-STWLE2000) the optical values that are averaged over all atmospheric layers, as well as a scaled $\\mu_0$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\bar{\\omega}&=\\sum_{l=1}^L \\omega_l \\tau_l \\bigg/ \\sum_{l=1}^L \\tau_l \\\\\n",
    "\\bar{f}&=\\sum_{l=1}^L f_l \\omega_l \\tau_l \\bigg/ \\sum_{l=1}^L \\omega_l \\tau_l \\\\\n",
    "\\bar{g}_{\\ell}&=\\sum_{l=1}^L g_{l, \\ell}^r \\omega_l \\tau_l \\bigg/ \\sum_{l=1}^L f_l \\omega_l \\tau_l \\\\\n",
    "g_{l, \\ell}^r &= \n",
    "\\begin{cases} f_l, &\\ell \\leq \\text{NLeg} \\\\\n",
    "g_{l, \\ell}, &\\ell > \\text{NLeg} \\end{cases} \\\\\n",
    "\\mu_0' &= \\frac{\\mu_0} {1-\\bar{\\omega} \\bar{f}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9401284",
   "metadata": {},
   "source": [
    "**Further NT corrections**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b51af",
   "metadata": {},
   "source": [
    "If the BDRF is strongly peaked then we would face the same problems as described in section [3.3](#3.3-Delta-M-scaling). It is possible to adapt $\\delta-M$ scaling and NT corrections to the BDRF and these are implemented in version 3 and newer of Stamnes' DISORT, see [[9]](#cite-LSJLTWS2015). These are currently not implemented in PyDISORT though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429280f5",
   "metadata": {},
   "source": [
    "### 3.7.3 Verification of NT corrected full solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e6217d",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "u_NT = PyDISORT.pydisort(\n",
    "    b_pos, b_neg, \n",
    "    False, \n",
    "    NQuad, NLeg, NLoops, \n",
    "    Leg_coeffs_all, \n",
    "    tauL, omega, \n",
    "    mu0, phi0, I0, \n",
    "    f=f[0], NT_cor=True\n",
    ")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658700db",
   "metadata": {},
   "source": [
    "**Does the full solution satisfy the BCs?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dcddf8",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# At top of atmosphere\n",
    "assert np.allclose(\n",
    "    u_NT(0, phi_arr)[N:, :],\n",
    "    b_neg @ np.cos(np.arange(NLoops)[:, None] * (phi0 - phi_arr)[None, :]),\n",
    ")\n",
    "\n",
    "# At bottom of atmosphere\n",
    "assert np.allclose(\n",
    "    u_NT(tauL, phi_arr)[:N, :],\n",
    "    b_pos @ np.cos(np.arange(NLoops)[:, None] * (phi0 - phi_arr)[None, :]),\n",
    ")\n",
    "print(\"Passed all tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c57007",
   "metadata": {},
   "source": [
    "**Does the full solution satisfy the radiative transfer equation?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a721695",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "tau_pt = float(1e-2)  # Must be a float for auto-differentiation to work\n",
    "\n",
    "plot = u_NT(tau_pt, phi_arr)[reorder_mu]\n",
    "\n",
    "fig = plt.figure(figsize=(9, 6))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.contourf(MU_ARR, PHI_ARR, plot, 200)\n",
    "ax.scatter(\n",
    "    phi0,\n",
    "    -mu0,\n",
    "    np.linspace(0, np.max(plot), 200) * 1.1,\n",
    "    marker=\".\",\n",
    "    color=\"red\",\n",
    "    label=\"Incident beam at $\\mu$ = \"\n",
    "    + str(-mu0)\n",
    "    + \", $\\phi$ = \"\n",
    "    + str(np.around(phi0, 3)),\n",
    ")\n",
    "ax.set_xlabel(r\"$\\phi$\")\n",
    "ax.set_ylabel(r\"$\\mu$\")\n",
    "ax.set_zlabel(\"Intensity\")\n",
    "plt.title(r\"TMS corrected intensities at $\\tau =$\" + str(tau_pt))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7ae1f2",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "LHS_NT = (\n",
    "    mu_arr_RO[:, None]\n",
    "    * ag.jacobian(lambda tau: u_NT(tau, phi_arr))(tau_pt)[reorder_mu, :]\n",
    ")\n",
    "RHS_NT = (\n",
    "    u_NT(tau_pt, phi_arr)[reorder_mu]\n",
    "    - (omega[0] / (4 * pi))\n",
    "    * np.einsum(\n",
    "        \"ijkl, kl, k, l -> ij\",\n",
    "        p_HG_muphi(mu_arr_RO, phi_arr, mu_arr_RO, phi_arr, g),\n",
    "        u_NT(tau_pt, phi_arr)[reorder_mu],\n",
    "        full_weights_mu_RO,\n",
    "        full_weights_phi,\n",
    "        optimize=True,\n",
    "    )\n",
    "    - (omega[0] * I0 / (4 * pi))\n",
    "    * p_HG_muphi(mu_arr_RO, phi_arr, -mu0, phi0, g)\n",
    "    * np.exp(-tau_pt / mu0)\n",
    ")\n",
    "error_NT = np.abs(RHS_NT - LHS_NT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b991f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"At tau = \" + str(tau_pt))\n",
    "print(\"Max pointwise error = \", np.max(error_NT))\n",
    "print(\"Max pointwise error ratio =\", np.max(error_NT / np.abs(LHS_NT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b275c3",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "plot = np.log10(error_NT)\n",
    "\n",
    "fig = plt.figure(figsize=(9, 6))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.contourf(MU_ARR, PHI_ARR, plot, 200)\n",
    "ax.scatter(\n",
    "    phi0,\n",
    "    -mu0,\n",
    "    np.linspace(np.min(plot), np.max(plot), 200) * 1.1,\n",
    "    marker=\".\",\n",
    "    color=\"red\",\n",
    "    label=\"Incident beam at $\\mu$ = \"\n",
    "    + str(-mu0)\n",
    "    + \", $\\phi$ = \"\n",
    "    + str(np.around(phi0, 3)),\n",
    ")\n",
    "ax.set_xlabel(r\"$\\phi$\")\n",
    "ax.set_ylabel(r\"$\\mu$\")\n",
    "ax.set_zlabel(\"Log10 of pointwise error\")\n",
    "plt.title(r\"Corrected pointwise errors at $\\tau =$\" + str(tau_pt))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82919f7",
   "metadata": {},
   "source": [
    "## 3.8 Computation of flux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe27fd8",
   "metadata": {},
   "source": [
    "PyDISORT also returns the positive and negative (hemispheric) flux functions. We have that\n",
    "\n",
    "$$\n",
    "F_\\text{total}^\\pm(\\tau) = F_\\text{diffuse}^\\pm(\\tau) + F_\\text{direct}^\\pm(\\tau)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfea9f9",
   "metadata": {},
   "source": [
    "**Direct (beam) flux**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d21962c",
   "metadata": {},
   "source": [
    "Since the incident beam is\n",
    "\n",
    "$$u_\\text{direct}(\\tau, \\mu, \\phi) = I_0 \\delta(\\mu + \\mu_0) \\delta(\\phi - \\phi_0) \\exp\\left(-\\mu_{0}^{-1} \\tau\\right)$$\n",
    "\n",
    "and $\\mu_0 > 0$, we have \n",
    "\n",
    "$$F_\\text{direct}^+(\\tau) \\equiv 0, \\quad F_\\text{direct}^-(\\tau) = I_0 \\mu_0 \\exp\\left(-\\mu_{0}^{-1} \\tau\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfc204e",
   "metadata": {},
   "source": [
    "**Diffuse flux**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85f1c8c",
   "metadata": {},
   "source": [
    "The diffuse flux equals\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "F_\\text{diffuse}^\\pm(\\tau) &= \\int_{0}^{1} \\int_{0}^{2 \\pi} \\mu u\\left(\\tau, \\pm\\mu, \\phi\\right) \\mathrm{d} \\phi \\mathrm{d} \\mu \\\\\n",
    "&= \\sum_{m=0}^\\infty \\left(\\int_{0}^{1} \\mu u^m(\\tau, \\pm\\mu) \\mathrm{d} \\mu \\int_{0}^{2 \\pi} \\cos\\left(m\\left(\\phi_0 - \\phi\\right)\\right) \\mathrm{d} \\phi \\right) \\\\\n",
    "&= 2\\pi \\int_{0}^{1} \\mu u^0\\left(\\tau, \\pm\\mu\\right) \\mathrm{d} \\mu \\\\\n",
    "&\\approx 2\\pi \\sum_{i = 1} w_i\\mu_i u^0\\left(\\tau, \\pm\\mu_i\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where we used the cosine expansion of $u$ from section [3.2](#3.2-Fourier-expansion-of-solution). In the last line we approximated the $\\mu$ integral by some quadrature rule. Only the $0$th moment matters for the flux. The upwelling and downwelling are respectively \n",
    "\n",
    "$$F_\\text{total}^+(0), \\quad F_\\text{total}^-(\\tau_\\text{BoA})$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26665498",
   "metadata": {},
   "source": [
    "**Impact of** $\\delta-M$ **scaling on flux calculations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e093ebd1",
   "metadata": {},
   "source": [
    "When we perform $\\delta-M$ scaling we artificially augment the incident beam at the expense of the diffuse radiation. If we only cared about the total upward and downward flux, then our calculations are identical to above. If we wish to distinguish between direct and diffuse fluxes like in PyDISORT and Stamnes' DISORT [[2]](#cite-STWJ1988), however, then we need to exercise more caution. The direct fluxes must be\n",
    "\n",
    "$$F_\\text{direct}^+(\\tau) \\equiv 0, \\quad F_\\text{direct}^-(\\tau) = I_0 \\mu_0 \\exp\\left(-\\mu_{0}^{-1} \\tau\\right)$$\n",
    "\n",
    "With $\\delta-M$ scaling, however, we get a larger value for the downward direct flux since the incident beam was augmented,\n",
    "\n",
    "$$F_\\text{direct}^-(\\tau^*) = I_0 \\mu_0 \\exp\\left(-\\mu_{0}^{-1} \\tau^*\\right) > I_0 \\mu_0 \\exp\\left(-\\mu_{0}^{-1} \\tau\\right)$$\n",
    "\n",
    "The solution is simply to re-classify the additional downward flux from direct to diffuse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d8c134",
   "metadata": {},
   "source": [
    "### 3.8.1 Verification of flux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bad9364",
   "metadata": {},
   "source": [
    "**Does integrating the intensity functions produce the flux functions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2790cdd2",
   "metadata": {},
   "source": [
    "This test will fail if NT corrections are enabled, i.e. `NT_cor == True`, because the corrections are applied only to the intensity and not the fluxes. This test can be tweaked to pass if $\\delta-M$ scaling is activated, i.e. `f > 0`, but as it is coded now it will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae37fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntau = 1000 # Number of tau test points\n",
    "tau_arr = np.random.random(Ntau) * tauL[0]\n",
    "\n",
    "# Number of phi grid points\n",
    "# This selection should ensure that the phi quadrature is at least as accurate as the mu quadrature\n",
    "Nphi = int((NQuad * pi) // 2) * 2 + 1  \n",
    "phi_arr, full_weights_phi = PyDISORT.subroutines.Clenshaw_Curtis_quad(Nphi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88090b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No delta-scaling\n",
    "u, flux_up, flux_down = PyDISORT.pydisort(\n",
    "    b_pos, b_neg, False, NQuad, NLeg, NLoops, Leg_coeffs_all, tauL, omega, mu0, phi0, I0,\n",
    ")[1:]\n",
    "u_cache = u(tau_arr, phi_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d345c6d3",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "flux_up_test = np.einsum(\n",
    "    \"itp, i, p -> t\",\n",
    "    mu_arr_pos[:, None, None] * u_cache[:N, :],\n",
    "    weights_mu,\n",
    "    full_weights_phi,\n",
    "    optimize=True,\n",
    ")\n",
    "flux_down_test = np.einsum(\n",
    "    \"itp, i, p -> t\",\n",
    "    mu_arr_pos[:, None, None] * u_cache[N:, :],\n",
    "    weights_mu,\n",
    "    full_weights_phi,\n",
    "    optimize=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96307082",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Flux up\")\n",
    "print(\n",
    "    \"Max pointwise error = \",\n",
    "     np.linalg.norm(flux_up(tau_arr) - flux_up_test, ord=np.infty),\n",
    ")\n",
    "print()\n",
    "print(\"Flux down (diffuse only)\")\n",
    "print(\n",
    "    \"Max pointwise error = \",\n",
    "    np.linalg.norm(flux_down(tau_arr)[0] - flux_down_test, ord=np.infty),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7790e4cb",
   "metadata": {},
   "source": [
    "**Does** $\\delta-M$ **scaling result in more accurate fluxes?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd9552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fluxes without delta-scaling\n",
    "flux_up, flux_down = PyDISORT.pydisort(\n",
    "    0, 0, True, 128, NLeg, NLoops, Leg_coeffs_all, tauL, omega, mu0, phi0, I0, f=0\n",
    ")[1:]\n",
    "\n",
    "# Fluxes with delta-scaling\n",
    "flux_up_dS, flux_down_dS = PyDISORT.pydisort(\n",
    "    0, 0, True, 128, NLeg, NLoops, Leg_coeffs_all, tauL, omega, mu0, phi0, I0, f=f[0]\n",
    ")[1:]\n",
    "\n",
    "# Fluxes calculated using a large number of phase function Legendre coefficients\n",
    "flux_up_true, flux_down_true = PyDISORT.pydisort(\n",
    "    0, 0, True, 128, 128, NLoops, Leg_coeffs_all, tauL, omega, mu0, phi0, I0\n",
    ")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c942f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without delta-scaling, i.e. f =\", 0)\n",
    "print()\n",
    "print(\"Flux up: Max pointwise error =\", np.abs(flux_up(0) - flux_up_true(0)))\n",
    "print(\n",
    "    \"Flux down: Max pointwise error =\",\n",
    "    np.abs(\n",
    "        np.sum(flux_down(tauL[0]), axis=0) - np.sum(flux_down_true(tauL[0]), axis=0)\n",
    "    ),\n",
    ")\n",
    "print()\n",
    "\n",
    "print(\"With delta-scaling, f =\", f)\n",
    "print()\n",
    "print(\"Flux up: Max pointwise error =\", np.abs(flux_up_dS(0) - flux_up_true(0)))\n",
    "print(\n",
    "    \"Flux down: Max pointwise error =\",\n",
    "    np.abs(\n",
    "        np.sum(flux_down_dS(tauL[0]), axis=0) - np.sum(flux_down_true(tauL[0]), axis=0)\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62131840",
   "metadata": {},
   "source": [
    "### 3.8.2 Reflectance and transmittance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed237332",
   "metadata": {},
   "source": [
    "**Incident flux**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699d10ef",
   "metadata": {},
   "source": [
    "In order to compute the reflectance and transmittance, we first need to determine the incident flux. The incident flux from the incident beam is $I_0 \\mu_0$. With $\\delta-M$ scaling, the apparent direct flux becomes $(1 + \\omega f) I_0 \\mu_0$. Of course the true direct flux remains $I_0 \\mu_0$. This discrepancy is explained in [[7]](#cite-JWW1976): under $\\delta-M$ scaling, the apparent direct flux \"includes scattered radiation traveling in very nearly the same direction as the incident beam\".\n",
    "\n",
    "Recall that the boundary conditions are\n",
    "\n",
    "$$\n",
    "u\\left(\\tau_\\text{BoA}, \\mu_i, \\phi \\right) = \\sum_{m = 0}^{\\text{NLoops}}b^+_{im}\\cos(m(\\phi_0 - \\phi)), \\quad u(0, -\\mu_i, \\phi) = \\sum_{m = 0}^{\\text{NLoops}}b^-_{im}\\cos(m(\\phi_0 - \\phi)) \\quad i = 1, \\dots, N\n",
    "$$\n",
    "\n",
    "The incident flux from the BCs, which we denote $F_{b^\\pm}$, are\n",
    "\n",
    "$$F_{b^\\pm} = 2 \\pi \\sum_{i = 0}^N w_i \\mu_i b^\\pm_{i0}$$\n",
    "\n",
    "respectively, where $w_i$ are quadrature weights.  Once again, only the $0$th moment matters for the flux. If the BCs are constant over $\\mu$, we will instead have \n",
    "\n",
    "$$F_{b^\\pm} = \\pi b^\\pm$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882a21c4",
   "metadata": {},
   "source": [
    "**Computation and interpretation of reflectance and transmittance values**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720a5dd7",
   "metadata": {},
   "source": [
    "Reflectance, $R$, and transmittance, $T$, values only make sense if the incident radiation comes entirely from one side of the atmosphere, usually downward onto the top layer. Moreover, we generally want to calculate reflectance and transmittance with respect to a specific source. As an example, we calculate the reflectance and transmittance with respect to the incident beam:\n",
    "\n",
    "$$R = \\frac{F_\\text{Total}^+(0)}{I_0 \\mu_0}, \\quad T = \\frac{F_\\text{Total}^-(\\tau_0)}{I_0 \\mu_0}$$\n",
    "\n",
    "which requires us to set the BCs $b^\\pm = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458a4fd4",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "flux_up, flux_down = PyDISORT.pydisort(\n",
    "    0, 0, True, NQuad, NLeg, NLoops, Leg_coeffs_all, tauL[0], omega[0], mu0, phi0, I0\n",
    ")[1:3]\n",
    "\n",
    "\n",
    "print(\"Reflectance, R =\", flux_up(0) / (I0 * mu0))\n",
    "print(\"Transmittance, T =\", np.sum(flux_down(tauL[0]), axis=0) / (I0 * mu0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b544964",
   "metadata": {},
   "source": [
    "**Verification of reflectance and transmittance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2945321",
   "metadata": {},
   "source": [
    "**Are** $R$ **and** $T$ **independent of the incident flux and** $\\phi_0$**?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a48f1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_tests = 100\n",
    "min_I0, max_I0 = 0, 1e10\n",
    "\n",
    "\n",
    "def test():\n",
    "    for i in range(num_of_tests):\n",
    "        I0_test = np.random.uniform(min_I0, max_I0)\n",
    "        phi0_test = np.random.uniform() * 2 * pi\n",
    "        flux_up_test, flux_down_test = PyDISORT.pydisort(\n",
    "            0, 0, True, NQuad, NLeg, NLoops, Leg_coeffs_all, tauL, omega, mu0, phi0_test, I0_test, f=f[0]\n",
    "        )[1:3]\n",
    "        R = flux_up_test(0) / (I0_test * mu0)\n",
    "        T = np.sum(flux_down_test(tauL[0]), axis=0) / (I0_test * mu0)\n",
    "\n",
    "        I0_test = np.random.uniform(min_I0, max_I0)\n",
    "        phi0_test = np.random.uniform() * 2 * pi\n",
    "        flux_up_test, flux_down_test = PyDISORT.pydisort(\n",
    "            0, 0, True, NQuad, NLeg, NLoops, Leg_coeffs_all, tauL, omega, mu0, phi0_test, I0_test, f=f[0]\n",
    "        )[1:3]\n",
    "        assert np.isclose(R, flux_up_test(0) / (I0_test * mu0))\n",
    "        assert np.isclose(T, np.sum(flux_down_test(tauL[0]), axis=0) / (I0_test * mu0))\n",
    "test()\n",
    "print(\"Passed all tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bd1639",
   "metadata": {},
   "source": [
    "# 4. Timing PyDISORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078e607d",
   "metadata": {},
   "source": [
    "The time taken will of course be parameter-dependent, but this should give a sense of the speed of PyDISORT. The parameters that affect the speed of PyDISORT the most are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146bc9cf",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"NQuad, NLoops, NLeg =\", NQuad, NLoops, NLeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416dc04a",
   "metadata": {},
   "source": [
    "**Time taken to solve the radiative transfer equation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22d2337",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"Intensity\")\n",
    "%timeit PyDISORT.pydisort(b_pos, b_neg, False, NQuad, NLeg, NLoops, Leg_coeffs_all, tauL, omega, mu0, phi0, I0)\n",
    "print()\n",
    "\n",
    "print(\"Intensity with corrections\")\n",
    "%timeit PyDISORT.pydisort(b_pos, b_neg, False, NQuad, NLeg, NLoops, Leg_coeffs_all, tauL, omega, mu0, phi0, I0, f=f[0], NT_cor=True)\n",
    "print()\n",
    "\n",
    "print(\"Only fluxes\")\n",
    "%timeit PyDISORT.pydisort(b_pos, b_neg, True, NQuad, NLeg, NLoops, Leg_coeffs_all, tauL, omega, mu0, phi0, I0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41446198",
   "metadata": {},
   "source": [
    "**Time taken to evaluate the solution at a point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558d759b",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"Intensity\")\n",
    "%timeit u(tau_arr[Ntau//2], phi_arr[Nphi//2])\n",
    "print()\n",
    "\n",
    "print(\"Intensity with corrections\")\n",
    "%timeit u_NT(tau_arr[Ntau//2], phi_arr[Nphi//2])\n",
    "print()\n",
    "\n",
    "print(\"Fluxes\")\n",
    "%timeit flux_up(0)\n",
    "%timeit flux_down(tauL[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3ce759",
   "metadata": {},
   "source": [
    "# 5. Solve for multiple atmospheric layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8ef541",
   "metadata": {},
   "source": [
    "If we have multiple atmospheric layers, they will be coupled through their BCs. Notice that we only use the BCs to solve for the coefficients of the homogeneous solution. Hence, we first solve for the general solution of each layer up to unknown coefficients before solving for all the coefficients simultaneously through a generalization of section [3.6.2](#3.6.2-The-homogeneous-solution-for-one-layer). Next, we construct the \"full solution\" for each layer as per section [3.7](#3.7-The-full-solution). The full solution for the entire multi-layer atmosphere simply branches to the \"full solution\" of individual layers depending on the $\\tau$ input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38db6f05",
   "metadata": {},
   "source": [
    "**Multi-layer generalization of section [3.6.2](#3.6.2-The-homogeneous-solution-for-one-layer)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9289ab",
   "metadata": {},
   "source": [
    "Suppose we have $L$ atmospheric layers demarcated by $[0, \\tau_1], [\\tau_1, \\tau_2], [\\tau_2, \\tau_3], \\dots, [\\tau_{L-1}, \\tau_L]$ with $\\tau_L = \\tau_\\text{BoA}$ and homogeneous solutions $v_1, v_2, v_3, \\dots, v_L$. Denote $\\mathscr{E}_i = \\exp\\left(-\\mu_0^{-1} \\tau_i\\right)$. We have the BCs\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&v_1^-\\left(0\\right) = b^-_m - B_1^- - \\delta_{0m}\\mathscr{v}_s(0) &&v_1^+\\left(\\tau_1\\right) + B_1^+\\mathscr{E}_1 = v_2^+\\left(\\tau_1\\right) + B_2^+\\mathscr{E}_1 \\\\\n",
    "&v_2^-\\left(\\tau_1\\right) + B_2^-\\mathscr{E}_1 = v_1^-\\left(\\tau_1\\right) + B_1^-\\mathscr{E}_1 &&v_2^+\\left(\\tau_2\\right) + B_2^+\\mathscr{E}_2 = v_3^+\\left(\\tau_2\\right) + B_3^+\\mathscr{E}_2 \\\\\n",
    "&v_3^-\\left(\\tau_2\\right) + B_3^-\\mathscr{E}_2 = v_2^-\\left(\\tau_2\\right) + B_2^-\\mathscr{E}_2 &&v_3^+\\left(\\tau_3\\right) + B_3^+\\mathscr{E}_3 = v_4^+\\left(\\tau_3\\right) + B_4^+\\mathscr{E}_3 \\\\\n",
    "&\\quad \\vdots &&\\quad \\vdots\\\\\n",
    "&v_L^-\\left(\\tau_{L-1}\\right) + B_L^-\\mathscr{E}_{L-1} = v_{L-1}^-\\left(\\tau_{L-1}\\right) + B_{L-1}^-\\mathscr{E}_{L-1} && v^+_L\\left( \\tau_L \\right) - \\mathscr{D}Wv^-_L\\left( \\tau_L \\right) = b^+_m + \\left(\\mathscr{X} + \\mathscr{D}WB^- - B^+\\right)\\mathscr{E}_L\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Denote $E_j = E_l(\\tau_j)$, where $E_l(\\tau)$ is the multi-layer generalization of $E(\\tau)$ from section [3.6.2](#3.6.2-The-homogeneous-solution-for-a-single-layer-atmosphere) and contains the eigenvalues of the $l$th layer. We omit the index $l$ in $E_j$ because it will always match that of the accompanying $G_l$ eigenvector term. The BCs produce the system\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "G^-_1 & 0 & 0 & & 0 & 0 \\\\ \n",
    "G^+_1 E_1 & -G^+_2 E_1 & 0 & & 0 & 0 \\\\ \n",
    "-G^-_1 E_1 & G^-_2 E_1 & 0 & & 0 & 0 \\\\ \n",
    "0 & G^+_2 E_2 & -G^+_3 E_2 & & 0 & 0 \\\\ \n",
    "0 & -G^-_2 E_2 & G^-_3 E_2 & & 0 & 0 \\\\\n",
    "& & & \\ddots & & \\\\\n",
    "0 & 0 & 0 & & G^+_{L-1} E_{L-1} & -G^+_L E_{L-1} \\\\ \n",
    "0 & 0 & 0 & & -G^-_{L-1} E_{L-1} & G^-_L E_{L-1} \\\\\n",
    "0 & 0 & 0 & & 0 & \\left(G^+_L - \\mathscr{D}WG^-_L\\right) E_L\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix} \n",
    "\\xi_1 \\\\ \n",
    "\\xi_2 \\\\ \n",
    "\\xi_3 \\\\ \n",
    "\\vdots \\\\\n",
    "\\xi_{L-1} \\\\\n",
    "\\xi_L \n",
    "\\end{bmatrix} \n",
    "= \n",
    "\\begin{bmatrix} b^-_m - B_1^- - \\delta_{0m}\\mathscr{v}_s(0) \\\\ \n",
    "\\left(B^+_2 - B^+_1\\right)\\mathscr{E}_1 \\\\\n",
    "\\left(B^-_1 - B^-_2\\right)\\mathscr{E}_1 \\\\ \n",
    "\\left(B^+_3 - B^+_2\\right)\\mathscr{E}_2 \\\\\n",
    "\\left(B^-_2 - B^-_3\\right)\\mathscr{E}_2 \\\\ \n",
    "\\vdots \\\\\n",
    "\\left(B^+_L - B^+_{L-1}\\right)\\mathscr{E}_{L-1} \\\\ \n",
    "\\left(B^-_{L-1} - B^-_L\\right)\\mathscr{E}_{L-1} \\\\ \n",
    "b^+_m + \\left(\\mathscr{X} + \\mathscr{D}WB_L^- - B_L^+\\right)\\mathscr{E}_L\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd0b39e",
   "metadata": {},
   "source": [
    "Once again, we add the superscripts $+$ and $-$ that correspond to only positive or only negative eigenvalues respectively. The multi-layer Stamnes-Conklin's substitutions [[5]](#cite-SC1984) are \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\xi_1^- = C_1^- &&\\xi_1^+ = C_1^+ E^-_1 \\\\\n",
    "&\\xi_2^- = C_2^- E^+_1 &&\\xi_2^+ = C_2^+ E^-_2 \\\\\n",
    "&\\quad \\vdots &&\\quad \\vdots \\\\\n",
    "&\\xi_L^- = C_L^- E^+_{L-1} &&\\xi_L^+ = C_L^+ E^-_L\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c292eef",
   "metadata": {},
   "source": [
    "Recall that the eigenvalues are arranged negative then positive, e.g. $G^+_1 E_1 = \\begin{bmatrix} G^{+-}_1 E^-_1 & G^{++}_1 E^+_1 \\end{bmatrix}$. Denote $E_{ij} = E^-_iE^+_j$. Every entry of $E_{ij}$ will have a negative exponent if $i > j$. The post-substitution system is\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "G^{--}_1 & G^{-+}_1 E^-_1 & 0 & 0 & & 0 & 0 & 0 & 0 \\\\ \n",
    "G^{+-}_1 E^-_1 & G^{++}_1 & -G^{+-}_2 & -G^{++}_2 E_{21} & & 0 & 0 & 0 & 0 \\\\ \n",
    "-G^{--}_1 E^-_1 & -G^{-+}_1 & G^{--}_2 & G^{-+}_2 E_{21} & & 0 & 0 & 0 & 0 \\\\ \n",
    "& & & & \\ddots & & & & \\\\\n",
    "0 & 0 & 0 & 0 & & G^{+-}_{L-1} E^-_{L-1} & G^{++}_{L-1} & -G^{+-}_L & -G^{++}_L E_{L,\\,L-1} \\\\ \n",
    "0 & 0 & 0 & 0 & & -G^{--}_{L-1} E^-_{L-1} & -G^{-+}_{L-1} & G^{--}_L & G^{-+}_L E_{L,\\,L-1} \\\\\n",
    "0 & 0 & 0 & 0 & & 0 & 0 & \\left(G^+_L - \\mathscr{D}WG^-_L\\right)^- E_{L,\\,L-1} & \\left(G^+_L - \\mathscr{D}WG^-_L\\right)^+\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix} \n",
    "C^-_1 \\\\ \n",
    "C^+_1 \\\\ \n",
    "C^-_2 \\\\ \n",
    "C^+_2 \\\\ \n",
    "\\vdots \\\\\n",
    "C^-_{L-1} \\\\\n",
    "C^+_{L-1} \\\\\n",
    "C^-_L \\\\\n",
    "C^+_L \\\\\n",
    "\\end{bmatrix} \n",
    "= \\text{RHS}\n",
    "$$\n",
    "and it will always be well-conditioned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aeb413",
   "metadata": {},
   "source": [
    "# 6. Comparisons with Stamnes' DISORT -- TO REPLACE WITH PYTEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc435b35",
   "metadata": {},
   "source": [
    "`mu_arr` is arranged as it is for code efficiency and readability. For presentation purposes we re-arrange `mu_arr` from smallest to largest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735a52d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_arr, u_NT, flux_up, flux_down = PyDISORT.pydisort(\n",
    "    0, 0, \n",
    "    False, \n",
    "    NQuad, NLeg, NLoops, \n",
    "    Leg_coeffs_all, \n",
    "    tauL, omega, \n",
    "    mu0, phi0, I0, \n",
    "    f=f[0], NT_cor=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of phi grid points\n",
    "# This selection should ensure that the phi quadrature is at least as accurate as the mu quadrature\n",
    "Nphi = int((NQuad * pi) // 2) * 2 + 1  \n",
    "phi_arr, full_weights_phi = PyDISORT.subroutines.Clenshaw_Curtis_quad(Nphi)\n",
    "\n",
    "reorder_mu = np.argsort(mu_arr)\n",
    "\n",
    "full_weights_mu_RO = full_weights_mu[reorder_mu]\n",
    "mu_arr_RO = mu_arr[reorder_mu]\n",
    "\n",
    "MU_ARR, PHI_ARR = np.meshgrid(phi_arr, mu_arr_RO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f75ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntau = 5 # Number of tau test points\n",
    "tau_arr = np.random.random(Ntau) * tauL[0]\n",
    "#tau_arr = np.array([0, tauL[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61dda6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Problem 3:  Henyey-Greenstein Scattering, g = 0.9 (Compare To Ref. VH2, Table 37)\n",
    "nlyr = 1\n",
    "nmom = NLeg + 1 # The additional Legendre coefficient is used as f\n",
    "nstr = NQuad\n",
    "numu = NQuad\n",
    "nphi = Nphi\n",
    "ntau = Ntau\n",
    "#ntau = Ntau_deriv\n",
    "usrang = True\n",
    "usrtau = True\n",
    "ibcnd = 0\n",
    "onlyfl = False\n",
    "prnt = np.array([True, False, False, False, False])  # Prints to CMD instead of here\n",
    "plank = False\n",
    "lamber = True\n",
    "deltamplus = False\n",
    "do_pseudo_sphere = False\n",
    "dtauc = tauL\n",
    "ssalb = omega\n",
    "pmom = Leg_coeffs_all[:NLeg + 1]\n",
    "# pmom = np.append(Leg_coeffs_all, f) # Equivalent to above\n",
    "temper = np.zeros(nlyr + 1)\n",
    "wvnmlo = 0\n",
    "wvnmhi = 0\n",
    "utau = tau_arr\n",
    "#utau = tau_arr_deriv\n",
    "umu0 = mu0\n",
    "phi0 = phi0\n",
    "umu = mu_arr_RO\n",
    "phi = phi_arr\n",
    "fbeam = I0\n",
    "fisot = 0\n",
    "albedo = 0\n",
    "btemp = 0\n",
    "ttemp = 0\n",
    "temis = 0\n",
    "earth_radius = 6371\n",
    "h_lyr = np.zeros(nlyr + 1)\n",
    "rhoq = np.zeros((nstr // 2, nstr + 1, nstr))\n",
    "rhou = np.zeros((numu, nstr // 2 + 1, nstr))\n",
    "rho_accurate = np.zeros((numu, nphi))\n",
    "bemst = np.zeros(nstr // 2)\n",
    "emust = np.zeros(numu)\n",
    "accur = 0\n",
    "header = \"Test Problem 3:  Henyey-Greenstein Scattering, g = 0.9 (Compare To Ref. VH2, Table 37)\"\n",
    "rfldir = np.zeros(ntau)\n",
    "rfldn = np.zeros(ntau)\n",
    "flup = np.zeros(ntau)\n",
    "dfdt = np.zeros(ntau)\n",
    "uavg = np.zeros(ntau)\n",
    "uu = np.zeros((numu, ntau, nphi))\n",
    "albmed = np.zeros(numu)\n",
    "trnmed = np.zeros(numu)\n",
    "\n",
    "# More practically, if desired print the \"disort\" function's inputs and outputs\n",
    "# print(disort.disort.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1ccbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run disort, putting DFDT, UAVG, and UU in a, b, and c, respectively\n",
    "rfldir, rfldn, flup, dfdt, uavg, uu, albmed, trnmed = disort.disort(usrang, usrtau, ibcnd, onlyfl, prnt, plank, lamber, deltamplus, do_pseudo_sphere, dtauc, ssalb,\n",
    "                        pmom, temper, wvnmlo, wvnmhi, utau, umu0, phi0 * 180/pi, umu, phi * 180/pi, fbeam, fisot, albedo, btemp, ttemp,\n",
    "                        temis, earth_radius, h_lyr, rhoq, rhou, rho_accurate, bemst, emust, accur, header, rfldir,\n",
    "                        rfldn, flup, dfdt, uavg, uu, albmed, trnmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eca120",
   "metadata": {},
   "source": [
    "**Flux comparisons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2fe052",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference ratios\")\n",
    "print()\n",
    "print(\"Upward (diffuse) fluxes\")\n",
    "print(np.abs(flup - flux_up(tau_arr)) / np.clip(flup, a_min=1e-6, a_max=None))\n",
    "print()\n",
    "print(\"Downward (diffuse) fluxes\")\n",
    "print(np.abs(rfldn - flux_down(tau_arr)[0]) / np.clip(rfldn, a_min=1e-6, a_max=None))\n",
    "print()\n",
    "print(\"Direct (downward) fluxes\")\n",
    "print(np.abs(rfldir - flux_down(tau_arr)[1]) / np.clip(rfldir, a_min=1e-6, a_max=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6749fed",
   "metadata": {},
   "source": [
    "**Intensity comparisons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc6909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "index = 0\n",
    "tau_pt = tau_arr[index]\n",
    "\n",
    "plot = np.log10(\n",
    "    np.abs(uu[:, index, :] - u_NT(tau_pt, phi_arr)[reorder_mu])\n",
    "    / np.clip(np.abs(uu[:, index, :]), a_min=1e-3, a_max=None)\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(9, 6))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.contourf(MU_ARR, PHI_ARR, plot, 200)\n",
    "ax.scatter(\n",
    "    phi0,\n",
    "    -mu0,\n",
    "    np.linspace(np.min(plot), np.max(plot), 200) * 1.1,\n",
    "    marker=\".\",\n",
    "    color=\"red\",\n",
    "    label=\"Incident beam at $\\mu$ = \"\n",
    "    + str(-mu0)\n",
    "    + \", $\\phi$ = \"\n",
    "    + str(np.around(phi0, 3)),\n",
    ")\n",
    "ax.set_xlabel(r\"$\\phi$\")\n",
    "ax.set_ylabel(r\"$\\mu$\")\n",
    "ax.set_zlabel(\"Log10 of difference ratios\")\n",
    "plt.title(r\"Difference ratios of intensities at $\\tau =$\" + str(np.around(tau_pt, 3)))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2925992",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"At tau = \" + str(tau_pt))\n",
    "print(\n",
    "    \"Max pointwise difference = \",\n",
    "    np.max(np.abs(uu[:, index, :] - u_NT(tau_pt, phi_arr)[reorder_mu])),\n",
    ")\n",
    "print(\n",
    "    \"Max pointwise difference ratio =\",\n",
    "    np.max(\n",
    "        np.abs(uu[:, index, :] - u_NT(tau_pt, phi_arr)[reorder_mu])\n",
    "        / np.clip(np.abs(uu[:, index, :]), a_min=1e-6, a_max=None),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d61eee",
   "metadata": {},
   "source": [
    "**Timing Stamnes' DISORT (wrapped using F2PY)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949eff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Intensity\")\n",
    "%timeit disort.disort(usrang, usrtau, ibcnd, False, prnt, plank, lamber, deltamplus, do_pseudo_sphere, dtauc, ssalb, pmom, temper, wvnmlo, wvnmhi, utau, umu0, phi0, umu, phi, fbeam, fisot, albedo, btemp, ttemp, temis, earth_radius, h_lyr, rhoq, rhou, rho_accurate, bemst, emust, accur, header, rfldir, rfldn, flup, dfdt, uavg, uu, albmed, trnmed)\n",
    "print()\n",
    "\n",
    "print(\"Only fluxes\")\n",
    "%timeit disort.disort(usrang, usrtau, ibcnd, True, prnt, plank, lamber, deltamplus, do_pseudo_sphere, dtauc, ssalb, pmom, temper, wvnmlo, wvnmhi, utau, umu0, phi0, umu, phi, fbeam, fisot, albedo, btemp, ttemp, temis, earth_radius, h_lyr, rhoq, rhou, rho_accurate, bemst, emust, accur, header, rfldir, rfldn, flup, dfdt, uavg, uu, albmed, trnmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed4cb45",
   "metadata": {},
   "source": [
    "<!--bibtex\n",
    "\n",
    "@inbook{Tre1996,\n",
    "  author    = {Trefethen, L. N.},\n",
    "  title     = {Finite difference and spectral methods for ordinary and partial differential equations},\n",
    "  chapter   = {Chapter 8. Chebyshev spectral methods},\n",
    "  year      = {1996},\n",
    "  pages     = {260300},\n",
    "  url = {https://people.maths.ox.ac.uk/trefethen/pdetext.html}\n",
    "}\n",
    "\n",
    "@article{NT1988,\n",
    "title = {Algorithms for radiative intensity calculations in moderately thick atmospheres using a truncation approximation},\n",
    "journal = {Journal of Quantitative Spectroscopy and Radiative Transfer},\n",
    "volume = {40},\n",
    "number = {1},\n",
    "pages = {51-69},\n",
    "year = {1988},\n",
    "issn = {0022-4073},\n",
    "doi = {https://doi.org/10.1016/0022-4073(88)90031-3},\n",
    "url = {https://www.sciencedirect.com/science/article/pii/0022407388900313},\n",
    "author = {T. Nakajima and M. Tanaka},\n",
    "abstract = {The efficiency of numerical calculations is discussed for selected algorithms employing the discrete ordinate method and the truncation approximation for the solar radiative intensity in moderately thick, plane-parallel scattering atmospheres. It is found that truncation of the phase function causes a significant error in the computed intensity and the magnitude of this error depends significantly on how the intensity is retrieved from the truncated radiative transfer equation. A newly developed retrieval algorithm, the TMS- method, yields the intensity field with an error 1% when the number of discrete path is as small as 10 in the hemisphere for aerosol-laden atmospheres with optical thickness 1.}\n",
    "}\n",
    "\n",
    "@article{YTA1971,\n",
    "title = {Radiative heat transfer in water clouds by infrared radiation},\n",
    "journal = {Journal of Quantitative Spectroscopy and Radiative Transfer},\n",
    "volume = {11},\n",
    "number = {6},\n",
    "pages = {697-708},\n",
    "year = {1971},\n",
    "issn = {0022-4073},\n",
    "doi = {https://doi.org/10.1016/0022-4073(71)90048-3},\n",
    "url = {https://www.sciencedirect.com/science/article/pii/0022407371900483},\n",
    "author = {Giichi Yamamoto and Masayuki Tanaka and Shoji Asano},\n",
    "abstract = {Radiative heat transfer in water clouds is studied by the method of discrete ordinates, taking into account not only scattering, absorption and emission by cloud droplets but also absorption and emission by water vapor in the cloud. According to Semuelson the method of discrete ordinates is not very amenable to studies involving the intermediate optical thickness, because of instabilities that are inherent in the method for the intermediate optical thickness. A method of avoiding these instabilities is shown in this paper. Numerical calculation for the spectral region from 5 to 40  was carried out on the model altostratus clouds, and that only for the window region on the model stratocumulus and nimbostratus clouds. The radiative temperature change in a very thin cloud is everywhere cooling. With increasing cloud thickness, however, the upper parts of the cloud undergo cooling, while the lower parts undergo heating. The rate of both heating and cooling is largest near the surface. In a semi-infinitely thick cloud the cloud top undergoes cooling at a rate of about 30C/hr and effective cooling extends to about 100 m interior from the cloud boundary.}\n",
    "}\n",
    "\n",
    "@book{Cha1960, \n",
    "      author = \"S.  Chandrasekhar\",\n",
    "      title = \"Radiative Transfer\",\n",
    "      year = \"1960\",\n",
    "      publisher = \"Dover\",\n",
    "}\n",
    "\n",
    "@article{Wis1977,\n",
    "      author = \"W. J.  Wiscombe\",\n",
    "      title = \"The DeltaM Method: Rapid Yet Accurate Radiative Flux Calculations for Strongly Asymmetric Phase Functions\",\n",
    "      journal = \"Journal of Atmospheric Sciences\",\n",
    "      year = \"1977\",\n",
    "      publisher = \"American Meteorological Society\",\n",
    "      address = \"Boston MA, USA\",\n",
    "      volume = \"34\",\n",
    "      number = \"9\",\n",
    "      doi = \"10.1175/1520-0469(1977)034<1408:TDMRYA>2.0.CO;2\",\n",
    "      pages=      \"1408 - 1422\",\n",
    "      url = \"https://journals.ametsoc.org/view/journals/atsc/34/9/1520-0469_1977_034_1408_tdmrya_2_0_co_2.xml\"\n",
    "}\n",
    "\n",
    "@article{Syk1951,\n",
    "    author = {Sykes, J. B.},\n",
    "    title = \"{Approximate Integration of the Equation of Transfer}\",\n",
    "    journal = {Monthly Notices of the Royal Astronomical Society},\n",
    "    volume = {111},\n",
    "    number = {4},\n",
    "    pages = {377-386},\n",
    "    year = {1951},\n",
    "    month = {08},\n",
    "    abstract = \"{The value of numerical integration in obtaining approximate solutions of an equation of transfer, and the different methods at our disposal, are discussed. It is shown that although the Newton-Cotes method, used by Kourganoff, is better than the Gauss method, used by Chandrasekhar, both are inferior to a new method, the double-Gauss, discovered by the author. The errors in the approximate values of the source-function and the limb-darkening in all three methods are tabulated for various approximations, and illustrated by graphs.}\",\n",
    "    issn = {0035-8711},\n",
    "    doi = {10.1093/mnras/111.4.377},\n",
    "    url = {https://doi.org/10.1093/mnras/111.4.377},\n",
    "    eprint = {https://academic.oup.com/mnras/article-pdf/111/4/377/8077435/mnras111-0377.pdf},\n",
    "}\n",
    "\n",
    "\n",
    "@article{STWJ1988,\n",
    "author = {Knut Stamnes and S-Chee Tsay and Warren Wiscombe and Kolf Jayaweera},\n",
    "journal = {Appl. Opt.},\n",
    "keywords = {Electromagnetic radiation; Multiple scattering; Optical depth; Radiative transfer; Reflection; Thermal emission},\n",
    "number = {12},\n",
    "pages = {2502--2509},\n",
    "publisher = {Optica Publishing Group},\n",
    "title = {Numerically stable algorithm for discrete-ordinate-method radiative transfer in multiple scattering and emitting layered media},\n",
    "volume = {27},\n",
    "month = {Jun},\n",
    "year = {1988},\n",
    "url = {http://opg.optica.org/ao/abstract.cfm?URI=ao-27-12-2502},\n",
    "doi = {10.1364/AO.27.002502},\n",
    "abstract = {We summarize an advanced, thoroughly documented, and quite general purpose discrete ordinate algorithm for time-independent transfer calculations in vertically inhomogeneous, nonisothermal, plane-parallel media. Atmospheric applications ranging from the UV to the radar region of the electromagnetic spectrum are possible. The physical processes included are thermal emission, scattering, absorption, and bidirectional reflection and emissionat the lower boundary. The medium may be forced at the top boundary by parallel or diffuse radiation and by internal and boundary thermal sources as well. We provide a brief account of the theoretical basis as well as a discussion of the numerical implementation of the theory. The recent advances made by ourselves and our collaborators---advances in both formulation and numerical solution---are all incorporated in the algorithm. Prominent among these advances are the complete conquest of two ill-conditioning problems which afflicted all previous discrete ordinate implementations: (1) the computation of eigenvalues and eigenvectors and (2) the inversion of the matrix determining the constants of integration. Copies of the fortran program on microcomputer diskettes are available for interested users.},\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "@article{STWLE2000,\n",
    "author = {Stamnes, Knut and Tsay, Si-Chee and Wiscombe, Warren and Laszlo, Istvan and Einaudi, Franco},\n",
    "year = {2000},\n",
    "month = {02},\n",
    "pages = {},\n",
    "title = {General Purpose Fortran Program for Discrete-Ordinate-Method Radiative Transfer in Scattering and Emitting Layered Media: An Update of DISORT}\n",
    "}\n",
    "\n",
    "@article{SS1981,\n",
    "      author = \"Knut  Stamnes and Roy A.  Swanson\",\n",
    "      title = \"A New Look at the Discrete Ordinate Method for Radiative Transfer Calculations in Anisotropically Scattering Atmospheres\",\n",
    "      journal = \"Journal of Atmospheric Sciences\",\n",
    "      year = \"1981\",\n",
    "      publisher = \"American Meteorological Society\",\n",
    "      address = \"Boston MA, USA\",\n",
    "      volume = \"38\",\n",
    "      number = \"2\",\n",
    "      doi = \"10.1175/1520-0469(1981)038<0387:ANLATD>2.0.CO;2\",\n",
    "      pages=      \"387 - 399\",\n",
    "      url = \"https://journals.ametsoc.org/view/journals/atsc/38/2/1520-0469_1981_038_0387_anlatd_2_0_co_2.xml\"\n",
    "}\n",
    "\n",
    "@article{SC1984,\n",
    "title = {A new multi-layer discrete ordinate approach to radiative transfer in vertically inhomogeneous atmospheres},\n",
    "journal = {Journal of Quantitative Spectroscopy and Radiative Transfer},\n",
    "volume = {31},\n",
    "number = {3},\n",
    "pages = {273-282},\n",
    "year = {1984},\n",
    "issn = {0022-4073},\n",
    "doi = {https://doi.org/10.1016/0022-4073(84)90031-1},\n",
    "url = {https://www.sciencedirect.com/science/article/pii/0022407384900311},\n",
    "author = {Knut Stamnes and Paul Conklin},\n",
    "abstract = {A recently developed matrix formulation of the discrete ordinate method is extended for application to an inhomogeneous atmosphere. The solution yields fluxes, as well as the complete azimuthal dependence of the intensity at any level in the atmosphere. The numerical aspects of the solution are discussed and numerical verification is provided by comparing computed results with those obtained by other methods. In particular, it is shown that a simple scaling scheme, which removes the positive exponentials in the coefficient matrix when solving for the constants of integration, provides unconditionally stable solutions for arbitrary optical thicknesses. An assessment of the accuracy to be expected is also provided, and it is shown that low-order discrete ordinate approximations yield very accurate flux values.}\n",
    "}\n",
    "\n",
    "@article{MH2017,\n",
    "title = {A demonstration of adjoint methods for multi-dimensional remote sensing of the atmosphere and surface},\n",
    "journal = {Journal of Quantitative Spectroscopy and Radiative Transfer},\n",
    "volume = {204},\n",
    "pages = {215-231},\n",
    "year = {2018},\n",
    "issn = {0022-4073},\n",
    "doi = {https://doi.org/10.1016/j.jqsrt.2017.09.031},\n",
    "url = {https://www.sciencedirect.com/science/article/pii/S0022407317305198},\n",
    "author = {William G.K. Martin and Otto P. Hasekamp},\n",
    "keywords = {Adjoint methods, Three-dimensional vector radiative transfer, Linearization, Remote sensing, Parameter derivatives, Searchlight functions},\n",
    "abstract = {In previous work, we derived the adjoint method as a computationally efficient path to three-dimensional (3D) retrievals of clouds and aerosols. In this paper we will demonstrate the use of adjoint methods for retrieving two-dimensional (2D) fields of cloud extinction. The demonstration uses a new 2D radiative transfer solver (FSDOM). This radiation code was augmented with adjoint methods to allow efficient derivative calculations needed to retrieve cloud and surface properties from multi-angle reflectance measurements. The code was then used in three synthetic retrieval studies. Our retrieval algorithm adjusts the cloud extinction field and surface albedo to minimize the measurement misfit function with a gradient-based, quasi-Newton approach. At each step we compute the value of the misfit function and its gradient with two calls to the solver FSDOM. First we solve the forward radiative transfer equation to compute the residual misfit with measurements, and second we solve the adjoint radiative transfer equation to compute the gradient of the misfit function with respect to all unknowns. The synthetic retrieval studies verify that adjoint methods are scalable to retrieval problems with many measurements and unknowns. We can retrieve the vertically-integrated optical depth of moderately thick clouds as a function of the horizontal coordinate. It is also possible to retrieve the vertical profile of clouds that are separated by clear regions. The vertical profile retrievals improve for smaller cloud fractions. This leads to the conclusion that cloud edges actually increase the amount of information that is available for retrieving the vertical profile of clouds. However, to exploit this information one must retrieve the horizontally heterogeneous cloud properties with a 2D (or 3D) model. This prototype shows that adjoint methods can efficiently compute the gradient of the misfit function. This work paves the way for the application of similar methods to 3D remote sensing problems.}\n",
    "}\n",
    "\n",
    "@article{MCB2014,\n",
    "title = {Adjoint methods for adjusting three-dimensional atmosphere and surface properties to fit multi-angle/multi-pixel polarimetric measurements},\n",
    "journal = {Journal of Quantitative Spectroscopy and Radiative Transfer},\n",
    "volume = {144},\n",
    "pages = {68-85},\n",
    "year = {2014},\n",
    "issn = {0022-4073},\n",
    "doi = {https://doi.org/10.1016/j.jqsrt.2014.03.030},\n",
    "url = {https://www.sciencedirect.com/science/article/pii/S002240731400154X},\n",
    "author = {William Martin and Brian Cairns and Guillaume Bal},\n",
    "keywords = {Adjoint methods, Three-dimensional vector radiative transfer, Linearization, Remote sensing, Parameter derivatives},\n",
    "abstract = {This paper derives an efficient procedure for using the three-dimensional (3D) vector radiative transfer equation (VRTE) to adjust atmosphere and surface properties and improve their fit with multi-angle/multi-pixel radiometric and polarimetric measurements of scattered sunlight. The proposed adjoint method uses the 3D VRTE to compute the measurement misfit function and the adjoint 3D VRTE to compute its gradient with respect to all unknown parameters. In the remote sensing problems of interest, the scalar-valued misfit function quantifies agreement with data as a function of atmosphere and surface properties, and its gradient guides the search through this parameter space. Remote sensing of the atmosphere and surface in a three-dimensional region may require thousands of unknown parameters and millions of data points. Many approaches would require calls to the 3D VRTE solver in proportion to the number of unknown parameters or measurements. To avoid this issue of scale, we focus on computing the gradient of the misfit function as an alternative to the Jacobian of the measurement operator. The resulting adjoint method provides a way to adjust 3D atmosphere and surface properties with only two calls to the 3D VRTE solver for each spectral channel, regardless of the number of retrieval parameters, measurement view angles or pixels. This gives a procedure for adjusting atmosphere and surface parameters that will scale to the large problems of 3D remote sensing. For certain types of multi-angle/multi-pixel polarimetric measurements, this encourages the development of a new class of three-dimensional retrieval algorithms with more flexible parametrizations of spatial heterogeneity, less reliance on data screening procedures, and improved coverage in terms of the resolved physical processes in the Earths atmosphere.}\n",
    "}\n",
    "\n",
    "@article{LSJLTWS2015,\n",
    "title = {Improved discrete ordinate solutions in the presence of an anisotropically reflecting lower boundary: Upgrades of the DISORT computational tool},\n",
    "journal = {Journal of Quantitative Spectroscopy and Radiative Transfer},\n",
    "volume = {157},\n",
    "pages = {119-134},\n",
    "year = {2015},\n",
    "issn = {0022-4073},\n",
    "doi = {https://doi.org/10.1016/j.jqsrt.2015.02.014},\n",
    "url = {https://www.sciencedirect.com/science/article/pii/S0022407315000679},\n",
    "author = {Z. Lin and S. Stamnes and Z. Jin and I. Laszlo and S.-C. Tsay and W.J. Wiscombe and K. Stamnes},\n",
    "keywords = {Radiative transfer model, BRDF, CoxMunk, RossLi, RPV, Single scattering correction},\n",
    "abstract = {A successor version 3 of DISORT (DISORT3) is presented with important upgrades that improve the accuracy, efficiency, and stability of the algorithm. Compared with version 2 (DISORT2 released in 2000) these upgrades include (a) a redesigned BRDF computation that improves both speed and accuracy, (b) a revised treatment of the single scattering correction, and (c) additional efficiency and stability upgrades for beam sources. In DISORT3 the BRDF computation is improved in the following three ways: (i) the Fourier decomposition is prepared off-line, thus avoiding the repeated internal computations done in DISORT2; (ii) a large enough number of terms in the Fourier expansion of the BRDF is employed to guarantee accurate values of the expansion coefficients (default is 200 instead of 50 in DISORT2); (iii) in the post-processing step the reflection of the direct attenuated beam from the lower boundary is included resulting in a more accurate single scattering correction. These improvements in the treatment of the BRDF have led to improved accuracy and a several-fold increase in speed. In addition, the stability of beam sources has been improved by removing a singularity occurring when the cosine of the incident beam angle is too close to the reciprocal of any of the eigenvalues. The efficiency for beam sources has been further improved from reducing by a factor of 2 (compared to DISORT2) the dimension of the linear system of equations that must be solved to obtain the particular solutions, and by replacing the LINPAK routines used in DISORT2 by LAPACK 3.5 in DISORT3. These beam source stability and efficiency upgrades bring enhanced stability and an additional 57% improvement in speed. Numerical results are provided to demonstrate and quantify the improvements in accuracy and efficiency of DISORT3 compared to DISORT2.}\n",
    "}\n",
    "\n",
    "@article {JWW1976,\n",
    "      author = \"J. H.  Joseph and W. J.  Wiscombe and J. A.  Weinman\",\n",
    "      title = \"The Delta-Eddington Approximation for Radiative Flux Transfer\",\n",
    "      journal = \"Journal of Atmospheric Sciences\",\n",
    "      year = \"1976\",\n",
    "      publisher = \"American Meteorological Society\",\n",
    "      address = \"Boston MA, USA\",\n",
    "      volume = \"33\",\n",
    "      number = \"12\",\n",
    "      doi = \"10.1175/1520-0469(1976)033<2452:TDEAFR>2.0.CO;2\",\n",
    "      pages=      \"2452 - 2459\",\n",
    "      url = \"https://journals.ametsoc.org/view/journals/atsc/33/12/1520-0469_1976_033_2452_tdeafr_2_0_co_2.xml\"\n",
    "}\n",
    "\n",
    "@Article{HMMNPW2017,\n",
    "AUTHOR = {Hase, N. and Miller, S. M. and Maa{\\ss}, P. and Notholt, J. and Palm, M. and Warneke, T.},\n",
    "TITLE = {Atmospheric inverse modeling via sparse reconstruction},\n",
    "JOURNAL = {Geoscientific Model Development},\n",
    "VOLUME = {10},\n",
    "YEAR = {2017},\n",
    "NUMBER = {10},\n",
    "PAGES = {3695--3713},\n",
    "URL = {https://gmd.copernicus.org/articles/10/3695/2017/},\n",
    "DOI = {10.5194/gmd-10-3695-2017}\n",
    "}\n",
    "\n",
    "@article {FL1992,\n",
    "      author = \"Qiang  Fu and K. N.  Liou\",\n",
    "      title = \"On the Correlated k-Distribution Method for Radiative Transfer in Nonhomogeneous Atmospheres\",\n",
    "      journal = \"Journal of Atmospheric Sciences\",\n",
    "      year = \"1992\",\n",
    "      publisher = \"American Meteorological Society\",\n",
    "      address = \"Boston MA, USA\",\n",
    "      volume = \"49\",\n",
    "      number = \"22\",\n",
    "      doi = \"10.1175/1520-0469(1992)049<2139:OTCDMF>2.0.CO;2\",\n",
    "      pages=      \"2139 - 2156\",\n",
    "      url = \"https://journals.ametsoc.org/view/journals/atsc/49/22/1520-0469_1992_049_2139_otcdmf_2_0_co_2.xml\"\n",
    "}\n",
    "\n",
    "@inproceedings{FJ1999,\n",
    "  title={Computer-based underwater imaging analysis},\n",
    "  author={Georges R. Fournier and Miroslaw Jonasz},\n",
    "  booktitle={Optics \\& Photonics},\n",
    "  year={1999}\n",
    "}\n",
    "\n",
    "@article{DM2010,\n",
    "\tdoi = {10.1088/0034-4885/73/2/026801},\n",
    "\turl = {https://doi.org/10.1088/0034-4885/73/2/026801},\n",
    "\tyear = 2010,\n",
    "\tmonth = {jan},\n",
    "\tpublisher = {{IOP} Publishing},\n",
    "\tvolume = {73},\n",
    "\tnumber = {2},\n",
    "\tpages = {026801},\n",
    "\tauthor = {Anthony B Davis and Alexander Marshak},\n",
    "\ttitle = {Solar radiation transport in the cloudy atmosphere: a 3D perspective on observations and climate impacts},\n",
    "\tjournal = {Reports on Progress in Physics},\n",
    "\tabstract = {The interplay of sunlight with clouds is a ubiquitous and often pleasant visual experience, but it conjures up major challenges for weather, climate, environmental science and beyond. Those engaged in the characterization of clouds (and the clear air nearby) by remote sensing methods are even more confronted. The problem comes, on the one hand, from the spatial complexity of real clouds and, on the other hand, from the dominance of multiple scattering in the radiation transport. The former ingredient contrasts sharply with the still popular representation of clouds as homogeneous plane-parallel slabs for the purposes of radiative transfer computations. In typical cloud scenes the opposite asymptotic transport regimes of diffusion and ballistic propagation coexist. We survey the three-dimensional (3D) atmospheric radiative transfer literature over the past 50 years and identify three concurrent and intertwining thrusts: first, how to assess the damage (bias) caused by 3D effects in the operational 1D radiative transfer models? Second, how to mitigate this damage? Finally, can we exploit 3D radiative transfer phenomena to innovate observation methods and technologies? We quickly realize that the smallest scale resolved computationally or observationally may be artificial but is nonetheless a key quantity that separates the 3D radiative transfer solutions into two broad and complementary classes: stochastic and deterministic. Both approaches draw on classic and contemporary statistical, mathematical and computational physics.}\n",
    "}\n",
    "\n",
    "@article{DFDM2021,\n",
    "      author = \"Linda Forster and Anthony B. Davis and David J. Diner and Bernhard Mayer\",\n",
    "      title = \"Toward Cloud Tomography from Space Using MISR and MODIS: Locating the Veiled Core in Opaque Convective Clouds\",\n",
    "      journal = \"Journal of the Atmospheric Sciences\",\n",
    "      year = \"2021\",\n",
    "      publisher = \"American Meteorological Society\",\n",
    "      address = \"Boston MA, USA\",\n",
    "      volume = \"78\",\n",
    "      number = \"1\",\n",
    "      doi = \"10.1175/JAS-D-19-0262.1\",\n",
    "      pages=      \"155 - 166\",\n",
    "      url = \"https://journals.ametsoc.org/view/journals/atsc/78/1/jas-d-19-0262.1.xml\"\n",
    "}\n",
    "\n",
    "@article{DDET2022,\n",
    "title = {Cloud tomographic retrieval algorithms. I: Surrogate minimization method},\n",
    "journal = {Journal of Quantitative Spectroscopy and Radiative Transfer},\n",
    "volume = {277},\n",
    "pages = {107954},\n",
    "year = {2022},\n",
    "issn = {0022-4073},\n",
    "doi = {https://doi.org/10.1016/j.jqsrt.2021.107954},\n",
    "url = {https://www.sciencedirect.com/science/article/pii/S0022407321004465},\n",
    "author = {Adrian Doicu and Alexandru Doicu and Dmitry Efremenko and Thomas Trautmann},\n",
    "keywords = {Cloud tomographic retrieval, Multi-dimensional models},\n",
    "abstract = {A cloud tomographic retrieval algorithm relying on (i) the spherical harmonics discrete ordinate method for computing the radiative transfer and (ii) the surrogate minimization method for solving the inverse problem has been designed. The retrieval algorithm uses regularization, accelerated projected gradient methods, and two types of surrogate functions. The performances of the retrieval algorithm are analyzed on a few synthetic two- and three-dimensional problems.}\n",
    "}\n",
    "\n",
    "@misc{Sta1999, \n",
    "\ttitle={LLLab disort website}, \n",
    "\turl={http://www.rtatmocn.com/disort/}, \n",
    "\tjournal={Light and Life Lab (LLLab)}, \n",
    "\tauthor={Stamnes, S.}, \n",
    "\tyear={1999}\n",
    "} \n",
    "\n",
    "@INPROCEEDINGS{ALHSAV2020,\n",
    "  author={Aides, Amit and Levis, Aviad and Holodovsky, Vadim and Schechner, Yoav Y. and Althausen, Dietrich and Vainiger, Adi},\n",
    "  booktitle={2020 IEEE International Conference on Computational Photography (ICCP)}, \n",
    "  title={Distributed Sky Imaging Radiometry and Tomography}, \n",
    "  year={2020},\n",
    "  volume={},\n",
    "  number={},\n",
    "  pages={1-12},\n",
    "  doi={10.1109/ICCP48838.2020.9105241}}\n",
    "\n",
    "@article {MW1980,\n",
    "      author = \"W. E.  Meador and W. R.  Weaver\",\n",
    "      title = \"Two-Stream Approximations to Radiative Transfer in Planetary Atmospheres: A Unified Description of Existing Methods and a New Improvement\",\n",
    "      journal = \"Journal of Atmospheric Sciences\",\n",
    "      year = \"1980\",\n",
    "      publisher = \"American Meteorological Society\",\n",
    "      address = \"Boston MA, USA\",\n",
    "      volume = \"37\",\n",
    "      number = \"3\",\n",
    "      doi = \"10.1175/1520-0469(1980)037<0630:TSATRT>2.0.CO;2\",\n",
    "      pages=      \"630 - 643\",\n",
    "      url = \"https://journals.ametsoc.org/view/journals/atsc/37/3/1520-0469_1980_037_0630_tsatrt_2_0_co_2.xml\"\n",
    "}\n",
    "\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9195021",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1) S.  Chandrasekhar. 1960. _Radiative Transfer_.\n",
    "\n",
    "2) Knut Stamnes and S-Chee Tsay and Warren Wiscombe and Kolf Jayaweera. 1988. _Numerically stable algorithm for discrete-ordinate-method radiative transfer in multiple scattering and emitting layered media_. [URL](http://opg.optica.org/ao/abstract.cfm?URI=ao-27-12-2502)\n",
    "\n",
    "3) Stamnes, S.. 1999. _LLLab disort website_. [URL](http://www.rtatmocn.com/disort/)\n",
    "\n",
    "4) Knut Stamnes and Paul Conklin. 1984. _A new multi-layer discrete ordinate approach to radiative transfer in vertically inhomogeneous atmospheres_. [URL](https://www.sciencedirect.com/science/article/pii/0022407384900311)\n",
    "\n",
    "5) W. J.  Wiscombe. 1977. _The DeltaM Method: Rapid Yet Accurate Radiative Flux Calculations for Strongly Asymmetric Phase Functions_. [URL](https://journals.ametsoc.org/view/journals/atsc/34/9/1520-0469_1977_034_1408_tdmrya_2_0_co_2.xml)\n",
    "\n",
    "6) J. H.  Joseph and W. J.  Wiscombe and J. A.  Weinman. 1976. _The Delta-Eddington Approximation for Radiative Flux Transfer_. [URL](https://journals.ametsoc.org/view/journals/atsc/33/12/1520-0469_1976_033_2452_tdeafr_2_0_co_2.xml)\n",
    "\n",
    "7) Sykes, J. B.. 1951. _Approximate Integration of the Equation of Transfer_. [URL](https://doi.org/10.1093/mnras/111.4.377)\n",
    "\n",
    "8) Stamnes, Knut and Tsay, Si-Chee and Wiscombe, Warren and Laszlo, Istvan and Einaudi, Franco. 2000. _General Purpose Fortran Program for Discrete-Ordinate-Method Radiative Transfer in Scattering and Emitting Layered Media: An Update of DISORT_.\n",
    "\n",
    "9) Z. Lin and S. Stamnes and Z. Jin and I. Laszlo and S.-C. Tsay and W.J. Wiscombe and K. Stamnes. 2015. _Improved discrete ordinate solutions in the presence of an anisotropically reflecting lower boundary: Upgrades of the DISORT computational tool_. [URL](https://www.sciencedirect.com/science/article/pii/S0022407315000679)\n",
    "\n",
    "10) Trefethen, L. N.. 1996. _Finite difference and spectral methods for ordinary and partial differential equations_. [URL](https://people.maths.ox.ac.uk/trefethen/pdetext.html)\n",
    "\n",
    "11) T. Nakajima and M. Tanaka. 1988. _Algorithms for radiative intensity calculations in moderately thick atmospheres using a truncation approximation_. [URL](https://www.sciencedirect.com/science/article/pii/0022407388900313)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
